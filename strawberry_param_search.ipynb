{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import consts\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OVERALL LIKING</th>\n",
       "      <th>TEXTURE LIKING</th>\n",
       "      <th>SWEETNESS INTENSITY</th>\n",
       "      <th>SOURNESS INTENSITY</th>\n",
       "      <th>STRAWBERRY FLAVOR INTENSITY</th>\n",
       "      <th>6915-15-7</th>\n",
       "      <th>77-92-9</th>\n",
       "      <th>50-99-7</th>\n",
       "      <th>57-48-7</th>\n",
       "      <th>57-50-1</th>\n",
       "      <th>...</th>\n",
       "      <th>7786-58-5</th>\n",
       "      <th>15111-96-3</th>\n",
       "      <th>706-14-9</th>\n",
       "      <th>10522-34-6</th>\n",
       "      <th>5881-17-4</th>\n",
       "      <th>128-37-0</th>\n",
       "      <th>40716-66-3</th>\n",
       "      <th>4887-30-3</th>\n",
       "      <th>5454-09-1</th>\n",
       "      <th>2305-05-7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.307068</td>\n",
       "      <td>0.250174</td>\n",
       "      <td>0.276647</td>\n",
       "      <td>0.146214</td>\n",
       "      <td>0.305021</td>\n",
       "      <td>-2.120421</td>\n",
       "      <td>0.171179</td>\n",
       "      <td>0.759180</td>\n",
       "      <td>0.612070</td>\n",
       "      <td>0.314374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271610</td>\n",
       "      <td>-0.443110</td>\n",
       "      <td>-0.544167</td>\n",
       "      <td>-0.463164</td>\n",
       "      <td>1.289539</td>\n",
       "      <td>-0.945803</td>\n",
       "      <td>0.122098</td>\n",
       "      <td>-0.127048</td>\n",
       "      <td>0.804970</td>\n",
       "      <td>-0.354773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.307859</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.276101</td>\n",
       "      <td>0.147151</td>\n",
       "      <td>0.306364</td>\n",
       "      <td>-2.119978</td>\n",
       "      <td>0.171282</td>\n",
       "      <td>0.759195</td>\n",
       "      <td>0.612046</td>\n",
       "      <td>0.314378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>-0.334642</td>\n",
       "      <td>-0.545464</td>\n",
       "      <td>-0.568225</td>\n",
       "      <td>1.346308</td>\n",
       "      <td>-0.893509</td>\n",
       "      <td>0.117186</td>\n",
       "      <td>-0.129998</td>\n",
       "      <td>0.800679</td>\n",
       "      <td>-0.374048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.306348</td>\n",
       "      <td>0.248756</td>\n",
       "      <td>0.277115</td>\n",
       "      <td>0.146568</td>\n",
       "      <td>0.306719</td>\n",
       "      <td>-2.120063</td>\n",
       "      <td>0.171283</td>\n",
       "      <td>0.759176</td>\n",
       "      <td>0.612056</td>\n",
       "      <td>0.314430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275587</td>\n",
       "      <td>-0.241669</td>\n",
       "      <td>-0.544872</td>\n",
       "      <td>-0.614111</td>\n",
       "      <td>1.283168</td>\n",
       "      <td>-0.997929</td>\n",
       "      <td>0.118824</td>\n",
       "      <td>-0.128236</td>\n",
       "      <td>0.819822</td>\n",
       "      <td>-0.357370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.307694</td>\n",
       "      <td>0.248227</td>\n",
       "      <td>0.277607</td>\n",
       "      <td>0.147135</td>\n",
       "      <td>0.305276</td>\n",
       "      <td>-2.120058</td>\n",
       "      <td>0.171249</td>\n",
       "      <td>0.759163</td>\n",
       "      <td>0.612060</td>\n",
       "      <td>0.314365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273644</td>\n",
       "      <td>-0.312912</td>\n",
       "      <td>-0.545357</td>\n",
       "      <td>-0.562826</td>\n",
       "      <td>1.343249</td>\n",
       "      <td>-0.959232</td>\n",
       "      <td>0.118201</td>\n",
       "      <td>-0.126724</td>\n",
       "      <td>0.811123</td>\n",
       "      <td>-0.377366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.308055</td>\n",
       "      <td>0.249070</td>\n",
       "      <td>0.276638</td>\n",
       "      <td>0.145692</td>\n",
       "      <td>0.305707</td>\n",
       "      <td>-2.120678</td>\n",
       "      <td>0.171089</td>\n",
       "      <td>0.759154</td>\n",
       "      <td>0.612069</td>\n",
       "      <td>0.314420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273092</td>\n",
       "      <td>-0.211367</td>\n",
       "      <td>-0.545153</td>\n",
       "      <td>-0.589017</td>\n",
       "      <td>1.548846</td>\n",
       "      <td>-0.901830</td>\n",
       "      <td>0.120083</td>\n",
       "      <td>-0.126749</td>\n",
       "      <td>0.807910</td>\n",
       "      <td>-0.384669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53995</th>\n",
       "      <td>0.237395</td>\n",
       "      <td>0.249785</td>\n",
       "      <td>0.205025</td>\n",
       "      <td>0.225081</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>-0.222374</td>\n",
       "      <td>0.884429</td>\n",
       "      <td>-0.834332</td>\n",
       "      <td>-0.803436</td>\n",
       "      <td>-0.625512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368228</td>\n",
       "      <td>-0.326611</td>\n",
       "      <td>-0.146165</td>\n",
       "      <td>-0.098112</td>\n",
       "      <td>0.032811</td>\n",
       "      <td>-0.408101</td>\n",
       "      <td>0.055410</td>\n",
       "      <td>-0.453151</td>\n",
       "      <td>-0.452553</td>\n",
       "      <td>-0.445544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53996</th>\n",
       "      <td>0.240068</td>\n",
       "      <td>0.250256</td>\n",
       "      <td>0.204477</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>0.257519</td>\n",
       "      <td>-0.221809</td>\n",
       "      <td>0.884286</td>\n",
       "      <td>-0.834295</td>\n",
       "      <td>-0.803437</td>\n",
       "      <td>-0.625499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.394705</td>\n",
       "      <td>-0.280702</td>\n",
       "      <td>-0.155369</td>\n",
       "      <td>-0.551456</td>\n",
       "      <td>-0.262542</td>\n",
       "      <td>-0.313059</td>\n",
       "      <td>0.052135</td>\n",
       "      <td>-0.480896</td>\n",
       "      <td>-0.518157</td>\n",
       "      <td>-0.462789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53997</th>\n",
       "      <td>0.238440</td>\n",
       "      <td>0.249851</td>\n",
       "      <td>0.204574</td>\n",
       "      <td>0.224211</td>\n",
       "      <td>0.257632</td>\n",
       "      <td>-0.222039</td>\n",
       "      <td>0.884583</td>\n",
       "      <td>-0.834322</td>\n",
       "      <td>-0.803455</td>\n",
       "      <td>-0.625512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390017</td>\n",
       "      <td>-0.316657</td>\n",
       "      <td>-0.159865</td>\n",
       "      <td>-0.542431</td>\n",
       "      <td>-0.189396</td>\n",
       "      <td>-0.153318</td>\n",
       "      <td>0.055373</td>\n",
       "      <td>-0.470802</td>\n",
       "      <td>-0.472719</td>\n",
       "      <td>-0.478549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53998</th>\n",
       "      <td>0.238959</td>\n",
       "      <td>0.250376</td>\n",
       "      <td>0.202495</td>\n",
       "      <td>0.225202</td>\n",
       "      <td>0.258081</td>\n",
       "      <td>-0.222588</td>\n",
       "      <td>0.884425</td>\n",
       "      <td>-0.834312</td>\n",
       "      <td>-0.803431</td>\n",
       "      <td>-0.625546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.372647</td>\n",
       "      <td>-0.229130</td>\n",
       "      <td>-0.157428</td>\n",
       "      <td>-0.625398</td>\n",
       "      <td>-0.242219</td>\n",
       "      <td>-0.471044</td>\n",
       "      <td>0.053125</td>\n",
       "      <td>-0.445828</td>\n",
       "      <td>-0.497520</td>\n",
       "      <td>-0.467094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53999</th>\n",
       "      <td>0.237641</td>\n",
       "      <td>0.250909</td>\n",
       "      <td>0.204326</td>\n",
       "      <td>0.225894</td>\n",
       "      <td>0.258426</td>\n",
       "      <td>-0.222090</td>\n",
       "      <td>0.884445</td>\n",
       "      <td>-0.834318</td>\n",
       "      <td>-0.803431</td>\n",
       "      <td>-0.625532</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379062</td>\n",
       "      <td>-0.670598</td>\n",
       "      <td>-0.160341</td>\n",
       "      <td>-0.515137</td>\n",
       "      <td>-0.225687</td>\n",
       "      <td>-0.381578</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>-0.463344</td>\n",
       "      <td>-0.495692</td>\n",
       "      <td>-0.447090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54000 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OVERALL LIKING  TEXTURE LIKING  SWEETNESS INTENSITY  \\\n",
       "0            0.307068        0.250174             0.276647   \n",
       "1            0.307859        0.249023             0.276101   \n",
       "2            0.306348        0.248756             0.277115   \n",
       "3            0.307694        0.248227             0.277607   \n",
       "4            0.308055        0.249070             0.276638   \n",
       "...               ...             ...                  ...   \n",
       "53995        0.237395        0.249785             0.205025   \n",
       "53996        0.240068        0.250256             0.204477   \n",
       "53997        0.238440        0.249851             0.204574   \n",
       "53998        0.238959        0.250376             0.202495   \n",
       "53999        0.237641        0.250909             0.204326   \n",
       "\n",
       "       SOURNESS INTENSITY  STRAWBERRY FLAVOR INTENSITY  6915-15-7   77-92-9  \\\n",
       "0                0.146214                     0.305021  -2.120421  0.171179   \n",
       "1                0.147151                     0.306364  -2.119978  0.171282   \n",
       "2                0.146568                     0.306719  -2.120063  0.171283   \n",
       "3                0.147135                     0.305276  -2.120058  0.171249   \n",
       "4                0.145692                     0.305707  -2.120678  0.171089   \n",
       "...                   ...                          ...        ...       ...   \n",
       "53995            0.225081                     0.258800  -0.222374  0.884429   \n",
       "53996            0.226804                     0.257519  -0.221809  0.884286   \n",
       "53997            0.224211                     0.257632  -0.222039  0.884583   \n",
       "53998            0.225202                     0.258081  -0.222588  0.884425   \n",
       "53999            0.225894                     0.258426  -0.222090  0.884445   \n",
       "\n",
       "        50-99-7   57-48-7   57-50-1  ...  7786-58-5   15111-96-3   706-14-9   \\\n",
       "0      0.759180  0.612070  0.314374  ...   -0.271610    -0.443110  -0.544167   \n",
       "1      0.759195  0.612046  0.314378  ...   -0.276743    -0.334642  -0.545464   \n",
       "2      0.759176  0.612056  0.314430  ...   -0.275587    -0.241669  -0.544872   \n",
       "3      0.759163  0.612060  0.314365  ...   -0.273644    -0.312912  -0.545357   \n",
       "4      0.759154  0.612069  0.314420  ...   -0.273092    -0.211367  -0.545153   \n",
       "...         ...       ...       ...  ...         ...          ...        ...   \n",
       "53995 -0.834332 -0.803436 -0.625512  ...   -0.368228    -0.326611  -0.146165   \n",
       "53996 -0.834295 -0.803437 -0.625499  ...   -0.394705    -0.280702  -0.155369   \n",
       "53997 -0.834322 -0.803455 -0.625512  ...   -0.390017    -0.316657  -0.159865   \n",
       "53998 -0.834312 -0.803431 -0.625546  ...   -0.372647    -0.229130  -0.157428   \n",
       "53999 -0.834318 -0.803431 -0.625532  ...   -0.379062    -0.670598  -0.160341   \n",
       "\n",
       "       10522-34-6   5881-17-4   128-37-0  40716-66-3   4887-30-3   5454-09-1   \\\n",
       "0        -0.463164    1.289539 -0.945803     0.122098   -0.127048    0.804970   \n",
       "1        -0.568225    1.346308 -0.893509     0.117186   -0.129998    0.800679   \n",
       "2        -0.614111    1.283168 -0.997929     0.118824   -0.128236    0.819822   \n",
       "3        -0.562826    1.343249 -0.959232     0.118201   -0.126724    0.811123   \n",
       "4        -0.589017    1.548846 -0.901830     0.120083   -0.126749    0.807910   \n",
       "...            ...         ...       ...          ...         ...         ...   \n",
       "53995    -0.098112    0.032811 -0.408101     0.055410   -0.453151   -0.452553   \n",
       "53996    -0.551456   -0.262542 -0.313059     0.052135   -0.480896   -0.518157   \n",
       "53997    -0.542431   -0.189396 -0.153318     0.055373   -0.470802   -0.472719   \n",
       "53998    -0.625398   -0.242219 -0.471044     0.053125   -0.445828   -0.497520   \n",
       "53999    -0.515137   -0.225687 -0.381578     0.054690   -0.463344   -0.495692   \n",
       "\n",
       "       2305-05-7  \n",
       "0      -0.354773  \n",
       "1      -0.374048  \n",
       "2      -0.357370  \n",
       "3      -0.377366  \n",
       "4      -0.384669  \n",
       "...          ...  \n",
       "53995  -0.445544  \n",
       "53996  -0.462789  \n",
       "53997  -0.478549  \n",
       "53998  -0.467094  \n",
       "53999  -0.447090  \n",
       "\n",
       "[54000 rows x 94 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/strawberry_samples_big.csv', index_col=0)\n",
    "val_data = pd.read_csv('data/strawberry_val_dataset.csv', index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OVERALL LIKING', 'TEXTURE LIKING', 'SWEETNESS INTENSITY',\n",
       "       'SOURNESS INTENSITY', 'STRAWBERRY FLAVOR INTENSITY'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cols = data.columns[:5]\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['6915-15-7', '77-92-9', '50-99-7', '57-48-7', '57-50-1', 'SSC', 'pH',\n",
       "       'TA', '75-85-4 ', '616-25-1 ', '1629-58-9 ', '96-22-0 ', '110-62-3 ',\n",
       "       '1534-08-3 ', '105-37-3', '109-60-4 ', '623-42-7 ', '591-78-6 ',\n",
       "       '108-10-1 ', '1576-87-0 ', '1576-86-9 ', '623-43-8 ', '71-41-0',\n",
       "       '1576-95-0 ', '556-24-1 ', '589-38-8 ', '105-54-4 ', '66-25-1 ',\n",
       "       '123-86-4 ', '624-24-8 ', '29674-47-3 ', '96-04-8 ', '638-11-9 ',\n",
       "       '116-53-0 ', '7452-79-1 ', '6728-26-3 ', '928-95-0 ', '111-27-3 ',\n",
       "       '123-92-2 ', '624-41-9 ', '110-43-0', '2432-51-1 ', '105-66-8 ',\n",
       "       '539-82-2 ', '111-71-7 ', '628-63-7 ', '1191-16-8 ', '106-70-7 ',\n",
       "       '55514-48-2 ', '110-93-0 ', '109-21-7 ', '123-66-0 ', '124-13-0 ',\n",
       "       '142-92-7 ', '2497-18-9 ', '60415-61-4', '104-76-7 ', ' 2311-46-8 ',\n",
       "       '109-19-3 ', '2548-87-0 ', '540-18-1 ', '4077-47-8 ', '20664-46-4',\n",
       "       '821-55-6 ', '5989-33-3 ', '78-70-6 ', '124-19-6 ', '103-09-3',\n",
       "       '140-11-4 ', '2639-63-6 ', '53398-83-7 ', '106-32-1 ', '112-14-1 ',\n",
       "       '564-94-3 ', '3913-81-3 ', '134-20-3 ', '110-39-4 ', '110-38-3 ',\n",
       "       '29811-50-5 ', '7786-58-5 ', '15111-96-3 ', '706-14-9 ', '10522-34-6 ',\n",
       "       '5881-17-4 ', '128-37-0', '40716-66-3 ', '4887-30-3 ', '5454-09-1 ',\n",
       "       '2305-05-7'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = data.columns[5:]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 89)\n",
      "(54000, 5)\n"
     ]
    }
   ],
   "source": [
    "features = data[feature_cols].values\n",
    "labels = data[target_cols].values\n",
    "val_features = val_data[feature_cols].values\n",
    "val_labels = val_data[target_cols].values\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38000, 89), (38000, 5), (16, 89), (16, 5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLE_TYPES = 54\n",
    "NUM_SAMPLES_PER_TYPE = len(data) // NUM_SAMPLE_TYPES\n",
    "TEST_SIZE = 0.3\n",
    "train_X = features[int(NUM_SAMPLE_TYPES*TEST_SIZE)*NUM_SAMPLES_PER_TYPE:]\n",
    "train_y = labels[int(NUM_SAMPLE_TYPES*TEST_SIZE)*NUM_SAMPLES_PER_TYPE:]\n",
    "val_X = val_features[:int(NUM_SAMPLE_TYPES*TEST_SIZE)]\n",
    "val_y = val_labels[:int(NUM_SAMPLE_TYPES*TEST_SIZE)]\n",
    "\n",
    "train_X.shape, train_y.shape, val_X.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Hyperbolic </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from hypll import nn as hnn\n",
    "from hypll.tensors import TangentTensor\n",
    "\n",
    "# Define your MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers, manifold):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.manual_seed(consts.TORCH_MANUAL_SEED)\n",
    "        self.fc_in = hnn.HLinear(input_size, layer_size, manifold=manifold)\n",
    "        self.relu = hnn.HReLU(manifold=manifold)\n",
    "        self.hidden_fcs = nn.ModuleList([hnn.HLinear(layer_size, layer_size, manifold=manifold) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = hnn.HLinear(layer_size, output_size, manifold=manifold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define custom PyTorch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Define training function\n",
    "def train_model(model, train_loader, criterion, optimizer, manifold, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tangents = TangentTensor(data=inputs, man_dim=-1, manifold=manifold)\n",
    "        manifold_inputs = manifold.expmap(tangents)\n",
    "\n",
    "        outputs = model(manifold_inputs)\n",
    "\n",
    "        loss = criterion(outputs.tensor, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def do_prediction(model, inputs, manifold, device):\n",
    "    inputs = inputs.float().to(device)\n",
    "    tangents = TangentTensor(data=inputs.to(device), man_dim=-1, manifold=manifold)\n",
    "    manifold_inputs = manifold.expmap(tangents)\n",
    "\n",
    "    outputs = model(manifold_inputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'num_hidden_layers': [16,20,24,28],\n",
    "              'layer_size': [96,128,160],\n",
    "            #   'lr': [0.0008,0.001,0.003],\n",
    "              'lr': [0.001],\n",
    "            #   'weight_decay': [0.001,0.005,0.01],\n",
    "              'weight_decay': [0.005],\n",
    "              'batch_size': [64],\n",
    "              'epochs': [5],\n",
    "              'curvature': [-1]}\n",
    "\n",
    "np.prod([len(p) for p in param_grid.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "param_combinations = list(itertools.product(*param_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Combination 0 -----\n",
      "('num_hidden_layers', 16) ('layer_size', 96) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.039753463, 0.034016572, 0.033266805, 0.035706356, 0.038671445]\n",
      "----- Combination 1 -----\n",
      "('num_hidden_layers', 16) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.038096897, 0.036231786, 0.035506245, 0.037174985, 0.038930852]\n",
      "----- Combination 2 -----\n",
      "('num_hidden_layers', 16) ('layer_size', 160) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.040986847, 0.03903661, 0.038483553, 0.039921723, 0.040040977]\n",
      "----- Combination 3 -----\n",
      "('num_hidden_layers', 20) ('layer_size', 96) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.03582763, 0.03295012, 0.03438847, 0.03888855, 0.038800538]\n",
      "----- Combination 4 -----\n",
      "('num_hidden_layers', 20) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.035465766, 0.034658235, 0.038445994, 0.038267326, 0.038869806]\n",
      "----- Combination 5 -----\n",
      "('num_hidden_layers', 20) ('layer_size', 160) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.0399306, 0.036463127, 0.037539497, 0.03998844, 0.039980646]\n",
      "----- Combination 6 -----\n",
      "('num_hidden_layers', 24) ('layer_size', 96) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.035906404, 0.03449464, 0.039053537, 0.03845703, 0.038390223]\n",
      "----- Combination 7 -----\n",
      "('num_hidden_layers', 24) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.034512985, 0.03530819, 0.03973347, 0.038127225, 0.038651414]\n",
      "----- Combination 8 -----\n",
      "('num_hidden_layers', 24) ('layer_size', 160) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.038288474, 0.03657856, 0.03908568, 0.03992459, 0.03989709]\n",
      "----- Combination 9 -----\n",
      "('num_hidden_layers', 28) ('layer_size', 96) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.03321109, 0.036283955, 0.039134014, 0.03850893, 0.038402956]\n",
      "----- Combination 10 -----\n",
      "('num_hidden_layers', 28) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.035675116, 0.03912691, 0.03969203, 0.038122263, 0.038602732]\n",
      "----- Combination 11 -----\n",
      "('num_hidden_layers', 28) ('layer_size', 160) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
      "[0.03831195, 0.03907163, 0.03894907, 0.039776064, 0.039700024]\n"
     ]
    }
   ],
   "source": [
    "from hypll.optim import RiemannianAdam\n",
    "from hypll.manifolds.poincare_ball import Curvature, PoincareBall\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_eval_stats = []\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f'----- Combination {i} -----')\n",
    "    print(*zip(param_grid.keys(), params))\n",
    "    num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs, curvature = params\n",
    "\n",
    "    manifold = PoincareBall(c=Curvature(curvature))\n",
    "\n",
    "    # Create DataLoader for training and validation\n",
    "    train_dataset = CustomDataset(train_X, train_y)\n",
    "    val_dataset = CustomDataset(val_X, val_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = MLP(input_size=len(feature_cols), output_size=len(target_cols), layer_size=layer_size, num_hidden_layers=num_hidden_layers, manifold=manifold).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = RiemannianAdam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        eval_stats['loss']['train'].append(train_model(model, train_loader, criterion, optimizer, manifold, device))\n",
    "        eval_stats['loss']['val'].append(util.h_evaluate_loss(model, val_loader, criterion, manifold, device))\n",
    "\n",
    "        eval_stats['mae']['train'].append(util.h_evaluate_mae(model, train_loader, manifold, device))\n",
    "        eval_stats['mae']['val'].append(util.h_evaluate_mae(model, val_loader, manifold, device))\n",
    "\n",
    "        # print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {eval_stats['loss']['train'][-1]:.4f}, Val Loss: {eval_stats['loss']['val'][-1]:.4f}\")\n",
    "    print(eval_stats['mae']['val'])\n",
    "    param_eval_stats.append(eval_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('num_hidden_layers', 20) ('layer_size', 128) ('lr', 0.005) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
    "\n",
    "[0.038952585, 0.040071815, 0.034801837, 0.037369102, 0.037260015]\n",
    "\n",
    "\n",
    "('num_hidden_layers', 20) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
    "\n",
    "[0.035465766, 0.034658235, 0.038445994, 0.038267326, 0.038869806]\n",
    "\n",
    "\n",
    "('num_hidden_layers', 28) ('layer_size', 96) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 5) ('curvature', -1)\n",
    "\n",
    "[0.03321109, 0.036283955, 0.039134014, 0.03850893, 0.038402956]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Traditional </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define your MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.manual_seed(consts.TORCH_MANUAL_SEED)\n",
    "        self.fc_in = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_fcs = nn.ModuleList([nn.Linear(layer_size, layer_size) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define custom PyTorch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Define training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def do_prediction(model, inputs, device):\n",
    "    inputs = inputs.float().to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'num_hidden_layers': [0,1,2,4,8],\n",
    "              'layer_size': [64,128,256],\n",
    "              'lr': [0.001],\n",
    "              'weight_decay': [0.005],\n",
    "              'batch_size': [64],\n",
    "              'epochs': [10]}\n",
    "\n",
    "np.prod([len(p) for p in param_grid.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "param_combinations = list(itertools.product(*param_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Combination 0 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 64) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.15532045, 0.10479045, 0.07160214, 0.064478405, 0.061701585, 0.062163077, 0.061328154, 0.061621614, 0.06215279, 0.061490215]\n",
      "----- Combination 1 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.15178974, 0.09704803, 0.06883284, 0.064692736, 0.06090314, 0.06277335, 0.06281453, 0.063628055, 0.06348946, 0.06398847]\n",
      "----- Combination 2 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 256) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.1424336, 0.08878113, 0.067971535, 0.062521756, 0.06254799, 0.06248487, 0.06393636, 0.06154679, 0.06254211, 0.06275411]\n",
      "----- Combination 3 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 64) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.06498646, 0.05064618, 0.04684478, 0.044228904, 0.04275305, 0.04331131, 0.043975733, 0.04494964, 0.04313879, 0.043811586]\n",
      "----- Combination 4 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.061192643, 0.04834754, 0.043614976, 0.043773822, 0.044946916, 0.04393547, 0.044169653, 0.043685716, 0.043375835, 0.044443317]\n",
      "----- Combination 5 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 256) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.055384964, 0.04468766, 0.043333873, 0.044100832, 0.0455338, 0.044565015, 0.044251908, 0.044931896, 0.044842143, 0.045603525]\n",
      "----- Combination 6 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 64) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.04235778, 0.038977817, 0.038357027, 0.03815228, 0.038362578, 0.038210694, 0.03799292, 0.03748234, 0.038416404, 0.038109966]\n",
      "----- Combination 7 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.04244005, 0.039581347, 0.039160706, 0.037028592, 0.037599258, 0.037541658, 0.03768769, 0.03837457, 0.037800785, 0.037633583]\n",
      "----- Combination 8 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 256) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.041322745, 0.038460217, 0.038535766, 0.03700017, 0.037181117, 0.037191298, 0.03834337, 0.038020793, 0.037870966, 0.03743192]\n",
      "----- Combination 9 -----\n",
      "('num_hidden_layers', 4) ('layer_size', 64) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.038847085, 0.03905403, 0.037521534, 0.037483096, 0.03744069, 0.038638763, 0.03847439, 0.037896033, 0.03834484, 0.038296267]\n",
      "----- Combination 10 -----\n",
      "('num_hidden_layers', 4) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.04040382, 0.039590742, 0.03790839, 0.0377963, 0.03788594, 0.036986552, 0.03724333, 0.038333874, 0.038134657, 0.03877967]\n",
      "----- Combination 11 -----\n",
      "('num_hidden_layers', 4) ('layer_size', 256) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.03755466, 0.038191877, 0.03828069, 0.038948577, 0.03804489, 0.03853116, 0.037906848, 0.038686737, 0.03785177, 0.03752563]\n",
      "----- Combination 12 -----\n",
      "('num_hidden_layers', 8) ('layer_size', 64) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.03851342, 0.03736866, 0.037852846, 0.03783962, 0.03750629, 0.038112678, 0.037765004, 0.03706355, 0.037600715, 0.038538583]\n",
      "----- Combination 13 -----\n",
      "('num_hidden_layers', 8) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.040197745, 0.038643114, 0.037972488, 0.03818772, 0.038038515, 0.037484206, 0.038676597, 0.038489513, 0.037618056, 0.038609035]\n",
      "----- Combination 14 -----\n",
      "('num_hidden_layers', 8) ('layer_size', 256) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.039353155, 0.03834812, 0.037989225, 0.03743767, 0.038092375, 0.03803058, 0.037311748, 0.037371624, 0.037900027, 0.038351536]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_eval_stats = []\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f'----- Combination {i} -----')\n",
    "    print(*zip(param_grid.keys(), params))\n",
    "    num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs = params\n",
    "\n",
    "    manifold = PoincareBall(c=Curvature(curvature))\n",
    "\n",
    "    # Create DataLoader for training and validation\n",
    "    train_dataset = CustomDataset(train_X, train_y)\n",
    "    val_dataset = CustomDataset(val_X, val_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = MLP(input_size=len(feature_cols), output_size=len(target_cols), layer_size=layer_size, num_hidden_layers=num_hidden_layers).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        eval_stats['loss']['train'].append(train_model(model, train_loader, criterion, optimizer, device))\n",
    "        eval_stats['loss']['val'].append(util.evaluate_loss(model, val_loader, criterion, device))\n",
    "\n",
    "        eval_stats['mae']['train'].append(util.evaluate_mae(model, train_loader, device))\n",
    "        eval_stats['mae']['val'].append(util.evaluate_mae(model, val_loader, device))\n",
    "\n",
    "        # print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {eval_stats['loss']['train'][-1]:.4f}, Val Loss: {eval_stats['loss']['val'][-1]:.4f}\")\n",
    "    print(eval_stats['mae']['val'])\n",
    "    param_eval_stats.append(eval_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('num_hidden_layers', 4) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
    "\n",
    "[0.04040382, 0.039590742, 0.03790839, 0.0377963, 0.03788594, 0.036986552, 0.03724333, 0.038333874, 0.038134657, 0.03877967]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
