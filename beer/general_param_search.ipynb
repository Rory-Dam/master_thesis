{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from hypll import nn as hnn\n",
    "from hypll.tensors import TangentTensor\n",
    "from hypll.optim import RiemannianAdam\n",
    "from hypll.manifolds.poincare_ball import Curvature, PoincareBall\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 231), (17500, 50), (175, 231), (175, 50), (75, 231), (75, 50))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_X = pd.read_csv('../data/beer_features_train_samples.csv', index_col=0)\n",
    "df_train_y = pd.read_csv('../data/beer_labels_panel_train_samples.csv', index_col=0)\n",
    "df_val_X = pd.read_csv('../data/beer_features_train.csv', index_col=0)\n",
    "df_val_y = pd.read_csv('../data/beer_labels_panel_train.csv', index_col=0)\n",
    "df_test_X = pd.read_csv('../data/beer_features_test.csv', index_col=0)\n",
    "df_test_y = pd.read_csv('../data/beer_labels_panel_test.csv', index_col=0)\n",
    "\n",
    "train_X = df_train_X.values\n",
    "train_y = df_train_y.values\n",
    "val_X = df_val_X.values\n",
    "val_y = df_val_y.values\n",
    "test_X = df_test_X.values\n",
    "test_y = df_test_y.values\n",
    "\n",
    "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([    0,     1,     2, ..., 17497, 17498, 17499]),\n",
       " array([    0,     1,     2, ..., 17497, 17498, 17499]),\n",
       " array([  100,   101,   102, ..., 17497, 17498, 17499]),\n",
       " array([    0,     1,     2, ..., 17397, 17398, 17399]),\n",
       " array([    0,     1,     2, ..., 17497, 17498, 17499])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "NUM_SAMPLE_TYPES = len(val_X)\n",
    "NUM_SAMPLES_PER_TYPE = len(train_X) // NUM_SAMPLE_TYPES\n",
    "\n",
    "fold_nums = list(range(FOLDS))\n",
    "[num*NUM_SAMPLE_TYPES for num in fold_nums]\n",
    "[(num+1)*NUM_SAMPLE_TYPES for num in fold_nums]\n",
    "\n",
    "# FOLD_INDICES = util.get_fold_indices(NUM_SAMPLE_TYPES, FOLDS)\n",
    "\n",
    "# def get_fold_indices(num_types, num_per_type, k, seed=42):\n",
    "#     def get_val_start_ends(size, k):\n",
    "#         fold_size = size // k\n",
    "#         rest = size % k\n",
    "\n",
    "#         fold_sizes = [fold_size] * k\n",
    "\n",
    "#         for i in range(rest):\n",
    "#             fold_sizes[i] += 1\n",
    "\n",
    "#         indices = np.cumsum([fold_sizes])\n",
    "\n",
    "#         return list(zip(indices-np.array(fold_sizes), indices))\n",
    "\n",
    "\n",
    "#     np.random.seed(seed)\n",
    "#     indices = np.random.random(num_types).argsort()\n",
    "\n",
    "#     val_start_ends = get_val_start_ends(num_types, k)\n",
    "#     val_indices = [indices[start:end] for start, end in val_start_ends]\n",
    "\n",
    "#     train_indices = [list(set(range(num_types)) - set(val_is)) for val_is in val_indices]\n",
    "#     exp_train_indices = [[list(range(val_i*num_per_type,(val_i+1)*num_per_type)) for val_i in val_is] for val_is in train_indices]\n",
    "\n",
    "#     return np.array(val_indices), [np.array(exp_is).flatten() for exp_is in exp_train_indices]\n",
    "\n",
    "val_indices, train_indices = util.get_fold_indices_rand(NUM_SAMPLE_TYPES, NUM_SAMPLES_PER_TYPE, FOLDS)\n",
    "train_indices\n",
    "# print(FOLD_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom PyTorch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Hyperbolic </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP model\n",
    "class HYP_MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers, manifold):\n",
    "        super(HYP_MLP, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.fc_in = hnn.HLinear(input_size, layer_size, manifold=manifold)\n",
    "        self.relu = hnn.HReLU(manifold=manifold)\n",
    "        self.hidden_fcs = nn.ModuleList([hnn.HLinear(layer_size, layer_size, manifold=manifold) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = hnn.HLinear(layer_size, output_size, manifold=manifold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define training function\n",
    "def hyp_train_model(model, train_loader, criterion, optimizer, manifold, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tangents = TangentTensor(data=inputs, man_dim=-1, manifold=manifold)\n",
    "        manifold_inputs = manifold.expmap(tangents)\n",
    "\n",
    "        outputs = model(manifold_inputs)\n",
    "\n",
    "        loss = criterion(outputs.tensor, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> EUCLIDEAN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP model\n",
    "class EUC_MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers):\n",
    "        super(EUC_MLP, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.fc_in = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_fcs = nn.ModuleList([nn.Linear(layer_size, layer_size) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define training function\n",
    "def euc_train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# param_grid = {\n",
    "#     'model_type': ['hyp', 'euc'],\n",
    "#     'num_hidden_layers': [0,1,2,4,8],\n",
    "#     'layer_size': [64,128,256],\n",
    "#     'lr': [0.001,0.003,0.01],\n",
    "#     'weight_decay': [0.001],\n",
    "#     'batch_size': [1024],\n",
    "#     'epochs': [10],\n",
    "#     'curvature': [-1]\n",
    "# }\n",
    "param_grid = {\n",
    "    'model_type': ['hyp', 'euc'],\n",
    "    'num_hidden_layers': [4],\n",
    "    'layer_size': [256],\n",
    "    'lr': [0.003],\n",
    "    'weight_decay': [0.001],\n",
    "    'batch_size': [1024],\n",
    "    'epochs': [100],\n",
    "    'curvature': [-1]\n",
    "}\n",
    "\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "len(param_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Combination 0 -----\n",
      "('model_type', 'hyp') ('num_hidden_layers', 4) ('layer_size', 256) ('lr', 0.003) ('weight_decay', 0.001) ('batch_size', 1024) ('epochs', 100) ('curvature', -1)\n",
      "Fold 0\n",
      "[0.07909973876278877, 0.09367001774728743, 0.1057633270929675, 0.10883881356667777, 0.11011095407897134, 0.10615182046488941, 0.10707008587318052, 0.10252012303043737, 0.10410803814158989, 0.10081582998329697, 0.1021307730856285, 0.1028657346119807, 0.10086331124899509, 0.10220556593923696, 0.10039777667436547, 0.1014937632953576, 0.1015297854949658, 0.09920677300680567, 0.09949343591291085, 0.10002138462193176, 0.09958962044134449, 0.10193769598774667, 0.1002850832200885, 0.0985802823729688, 0.09729863739946917, 0.09656472105301632, 0.098171122798986, 0.09593640826352184, 0.09518050992453445, 0.09555145575607846, 0.098393040867355, 0.0971348926249365, 0.09736277617212981, 0.09782338203514891, 0.09733486242702252, 0.09480762527727682, 0.0974133520254803, 0.09591586890544523, 0.09567762933955598, 0.09665812396472631, 0.09604428474413623, 0.09707101899869908, 0.09715901122687853, 0.09643176608320969, 0.09502066769829604, 0.0968867257101673, 0.09579621140790445, 0.09745372253147877, 0.09599215652837188, 0.0960637083960948, 0.09823631143753833, 0.09697589771127381, 0.09757094140646295, 0.0964905370383499, 0.09636630945679393, 0.09508730495402049, 0.09558849575831509, 0.0949006746628662, 0.09827472667630274, 0.09347510069479213, 0.09534670403690727, 0.09800735167343756, 0.09355066763429055, 0.09402018573968007, 0.09625035178383344, 0.0954303943663232, 0.09592800434060755, 0.09424281012407851, 0.09709326150711078, 0.09543919776935317, 0.09562227921009053, 0.09603118534804066, 0.0941308513494854, 0.09669941069171543, 0.09483217541542892, 0.09591746655853313, 0.09650081379404565, 0.09691661391588059, 0.09778809663402538, 0.09660080401429307, 0.09415694933676018, 0.09361359376892249, 0.09641330975034838, 0.09561423083873002, 0.09525325319197776, 0.09460395142535795, 0.0949983409189909, 0.09549411244778191, 0.0961952162772746, 0.0964340868349425, 0.09730960694894535, 0.09492005947179714, 0.0960705101838229, 0.09685976866928092, 0.09708944350457013, 0.09482779154048215, 0.09506589972616462, 0.09541131711375973, 0.09530095908007014, 0.09756733176831311]\n",
      "Fold 1\n",
      "[0.10867697601925531, 0.12861346343309812, 0.13216944691345528, 0.1331776972669393, 0.1349976965296748, 0.13489417658959496, 0.13298298637489034, 0.13188617369818093, 0.1323101868858269, 0.13209981919615804, 0.13298533909512344, 0.13350546648944328, 0.13246027890174733, 0.13144201097367403, 0.13465154214077052, 0.13245332594777015, 0.13166506944926226, 0.132772560445966, 0.1328367093580088, 0.13312498137526224, 0.13373128899888131, 0.1339877121117299, 0.13348501894276915, 0.1346763889376027, 0.13627132541195508, 0.13454680296603588, 0.1376275792069335, 0.13624693336658025, 0.1385961081703302, 0.1383854177245389, 0.1383690347492513, 0.14067069203273638, 0.13885363139075885, 0.1391503926948875, 0.14004848391969194, 0.1395501521677083, 0.13950733000383914, 0.13912112903693313, 0.1402819084144232, 0.1390552005020448, 0.14052944460842806, 0.1411144525295921, 0.14137926727824174, 0.1423471249508596, 0.1419251743107788, 0.1414261852086585, 0.14140629368112015, 0.14124240971921503, 0.14177212415705187, 0.14290450014266262, 0.14238556715996603, 0.14237525210499483, 0.14154317868541041, 0.1426204102610733, 0.14193788781506106, 0.1427229228982352, 0.1435706324539254, 0.14326934167536787, 0.14233078790040488, 0.1430949671235593, 0.14302435971436622, 0.14255252114589578, 0.14105821751329026, 0.1417712934858012, 0.14250804090962554, 0.14149332967109626, 0.14271738347229002, 0.14191496185748056, 0.14120296396173962, 0.1409860887627719, 0.1403517069282431, 0.14232980744462037, 0.14190391722193055, 0.1424315048210318, 0.14113848030441895, 0.14202755697898023, 0.13975327098553492, 0.14218516284377397, 0.14204536796110998, 0.14226991925567808, 0.1435387910490992, 0.1419097752331579, 0.14232531177537447, 0.14141512601632436, 0.14274561501839422, 0.14282219093087428, 0.14152291791930074, 0.1421606610250173, 0.1406343463270716, 0.14250864961980347, 0.14217618204081658, 0.142070926594856, 0.14013688475905126, 0.1417933411278005, 0.1404655688759805, 0.1413402487880252, 0.1414880205345727, 0.14137593491345382, 0.1425909789736927, 0.14410162554654862]\n",
      "Fold 2\n",
      "[0.07888923936887043, 0.0997035389691568, 0.10610755271030277, 0.10407998258020833, 0.09987890775392907, 0.09642375759819798, 0.0895895327973248, 0.08599745847247398, 0.08195745261098875, 0.0806825271316864, 0.07890062469415554, 0.07855234967043905, 0.0776819819834318, 0.07419752829711294, 0.07183209804624352, 0.06967644819428646, 0.07041589906878548, 0.07035839402109413, 0.07112554679107465, 0.06959796450710548, 0.06904294417457091, 0.06994768383234906, 0.07285038786230894, 0.07129958937826421, 0.07142075715496504, 0.07062538242076973, 0.07145644437732744, 0.07176182047098938, 0.07339006091486745, 0.07126432908351699, 0.07344471760364794, 0.07207656940724429, 0.07628359549413499, 0.07480622348322002, 0.07418954391029804, 0.0746633085423977, 0.0748853845429912, 0.07584120037466774, 0.07585913339244277, 0.07569526051343611, 0.07763781494081531, 0.07712894369610131, 0.07693633254024303, 0.07729039110459487, 0.07861916808510222, 0.07853625436138255, 0.07687121440340945, 0.07775586610310156, 0.07523130723616964, 0.07724891124110708, 0.07627821591209616, 0.07953301550199747, 0.07722170342641264, 0.07757727370599725, 0.07716740818018801, 0.07862904067644055, 0.07797163086555402, 0.07937093541583895, 0.07895900080361448, 0.07782152701863396, 0.07995869401983507, 0.0802207980525337, 0.07923825410603122, 0.07927024541461729, 0.0775148620737252, 0.07891120118591219, 0.07958886735621885, 0.07884120796034744, 0.07946372658595158, 0.08028638337552961, 0.08086725949370459, 0.07974954394292659, 0.07944092263188748, 0.07854947735090743, 0.07953640844555995, 0.08097381179764969, 0.08054233518022429, 0.08148634849072049, 0.07873449848572145, 0.0782451510829696, 0.0785080637231835, 0.07918911963340185, 0.07979276479430686, 0.0805048631552475, 0.07841050006890406, 0.08201280772745377, 0.07930161352503798, 0.08056833419623917, 0.08119909740348703, 0.08196533127625144, 0.07883515560635967, 0.07985679904992962, 0.08146048701882323, 0.07927171471339997, 0.08054305075342168, 0.07953000063533675, 0.07993647554978069, 0.08023039267129137, 0.08032319843154159, 0.08215778068598453]\n",
      "Fold 3\n",
      "[0.06372710507893672, 0.08105350760113339, 0.07845504774029961, 0.07655834828714866, 0.07580030411851381, 0.08050799798212761, 0.08086845785492996, 0.07873520343666328, 0.0749994756782145, 0.07582992121683846, 0.07168251343886042, 0.07255517305797152, 0.06898384224252085, 0.07143823786148296, 0.06816440411692049, 0.06835434255644682, 0.06790598347531328, 0.06789830558050733, 0.06517940636612614, 0.06615852483537843, 0.06529994787291776, 0.06494385657555907, 0.06473788282350192, 0.06233371626488944, 0.06062771046821121, 0.06604156427054092, 0.0650819713855205, 0.06436039878002876, 0.06446322599012268, 0.06442274279947517, 0.06475706504213993, 0.06381813309997193, 0.06525844612885665, 0.06200823892555876, 0.06525158698408023, 0.06195499102625218, 0.06348219523956966, 0.0638859282079006, 0.06399651315440386, 0.06440019921661314, 0.06531013484984242, 0.0652928156089108, 0.06333143150042145, 0.06515388939248241, 0.06553557231295751, 0.06358236336253995, 0.06670358648684532, 0.063941834076204, 0.06479026077669653, 0.06523894967247591, 0.06384272200570378, 0.06643573275534628, 0.06470194498279033, 0.06395579969319815, 0.06548031595576424, 0.06553592350853188, 0.06575882141151863, 0.06630994790862006, 0.06518247079240913, 0.06556112644022097, 0.06407442329380525, 0.06679554978898436, 0.06708160239727112, 0.06704250969521279, 0.06761679441514763, 0.0635234903101328, 0.0654747414919657, 0.06541720633945115, 0.06673736349644316, 0.06641335339320534, 0.06799028451316784, 0.06506127384069686, 0.06916229084716981, 0.06766068521137185, 0.06740218035558533, 0.06583526116778599, 0.06502334927037658, 0.0674113750025405, 0.0647422312339256, 0.06883067795825804, 0.06659130268361015, 0.06568694767328248, 0.06652429534317603, 0.0663828512654039, 0.0681839094506437, 0.06775861922507359, 0.0685633483290415, 0.06503776219871216, 0.07007211417012997, 0.06613485169510065, 0.06680799132664586, 0.06647122131371828, 0.06671077094107486, 0.06852200360906048, 0.0674955353686726, 0.06709864871248264, 0.06639117585021148, 0.06702164679044056, 0.06738099852726735, 0.06420380051908163]\n",
      "Fold 4\n",
      "[0.07242007521111367, 0.09017505507828988, 0.09533771971389307, 0.09108198349522453, 0.08946604300050179, 0.08991010994056399, 0.08783629248364795, 0.08751683739154005, 0.08369166606684, 0.08348101499310495, 0.08313774351011821, 0.08290027921994847, 0.08441126483760311, 0.08583003304487487, 0.08606263229798657, 0.0866302104989698, 0.08699886071106083, 0.08684310491364804, 0.08688641223669827, 0.08925201630707624, 0.0873498773008318, 0.08878919484892106, 0.08969744685893762, 0.08941063176790966, 0.08953341391405623, 0.08840375626069122, 0.09126914031868923, 0.08829894256788855, 0.0877566326616048, 0.08707055367600881, 0.08954380325173329, 0.08932160631372461, 0.08938250399350361, 0.08930299793123626, 0.08762312240787075, 0.08824266692180545, 0.08916400776538631, 0.09031641456278133, 0.08953437081454724, 0.0895010653815347, 0.09018035391864375, 0.09019518190887883, 0.08656931465553973, 0.09235729003615845, 0.09053467589564797, 0.08998504145316787, 0.09139866117549214, 0.09036400215013839, 0.09134499027795485, 0.09013139946741155, 0.09172598381837725, 0.09159979181223614, 0.08919013506195476, 0.09023662401514133, 0.09172845353570441, 0.09244191508892725, 0.0923826563977737, 0.0919341301548411, 0.09141733196884744, 0.09119434683819377, 0.09355292819885896, 0.0904744295162502, 0.09016369238235816, 0.09327982321404787, 0.0939961646498, 0.09551658459604799, 0.09294055889581801, 0.09183060948857033, 0.09270633177702284, 0.0910445975837643, 0.0913382084321469, 0.09064998884333222, 0.0932938049017967, 0.09346901217100174, 0.09225051621551397, 0.0939245311007445, 0.09059540197605422, 0.09120635727602748, 0.09179813334268225, 0.09217075382546497, 0.09040667655847198, 0.09304337518902152, 0.0925507926697758, 0.09233123311225289, 0.09118983496117186, 0.0941215595358172, 0.09342888600393799, 0.09158297286637089, 0.09288527851726533, 0.09098259986978195, 0.0927142718244853, 0.0915257568255765, 0.09360304804944551, 0.0922437312343524, 0.09278049593868598, 0.09198004626484782, 0.09302387391964881, 0.08990661924917703, 0.09259822337208316, 0.09110660261148987]\n",
      "----- Combination 1 -----\n",
      "('model_type', 'euc') ('num_hidden_layers', 4) ('layer_size', 256) ('lr', 0.003) ('weight_decay', 0.001) ('batch_size', 1024) ('epochs', 100) ('curvature', -1)\n",
      "Fold 0\n",
      "[0.0597327889296524, 0.06598733638294556, 0.03570309931416589, 0.07082774528966794, 0.029142810689423242, 0.01657619344022997, -0.06794553029604755, -0.011616908855822143, -0.014159684889728687, -0.030407411174621774, 0.0007257645708274651, -0.009529621967916376, -0.007633940816561302, -0.02448385977254319, -0.012377031146540492, -0.04686085193021333, -0.015443015176134946, -8.910890500514857e-05, -0.023202583768244345, -0.013120627939752726, -0.028854835172757262, -0.028294891807229088, -0.024476275952776537, -0.010250022394117193, -0.009359306497441724, -0.042658219149687454, -0.011541416684410966, -0.0023870344122434517, 0.006620828609540588, 0.008981494367466957, 0.0037369296416766896, -0.010497040988712403, -0.01073286375353264, -0.004966624321862701, -0.014580908671680968, 0.02110576911371913, 0.010321512028132173, -0.008455081523227532, -0.009119348854304545, -0.015186009133036698, 0.013401209270468561, 0.002617299103767372, -0.00942587550249217, -0.014151275804890152, -0.008358136398479342, -0.010920733725052952, -0.01736266692539537, -0.013950611060543959, -0.0035329671865904745, 0.015247558837682187, -0.011961105952388302, -0.012494944533539214, -0.02387849889466157, -0.0012519510418388903, 0.0028848749178479395, -0.0031563343673141554, -0.032038601986175246, -0.031764388913336515, -0.012984001644214104, -0.019544711859955276, -0.017154482169374027, -0.012155435529855187, -0.025981570786814723, -0.019886655084804538, 0.00545897655595416, -0.005190050334428434, -0.014525718337981943, -0.04030822417424158, -0.013331998574478247, -0.02833659782692379, -0.026474358767678233, -0.010618613431212176, -0.015217925189747234, -0.005869305680071362, -0.019143110941333887, -0.025321984000114085, -0.027047852235280053, -0.0051504955680654165, -0.016747646220019875, -0.015155805089063563, -0.010604910880397514, -0.01680032955385722, -0.036612218277076905, -0.01627453107006796, -0.009754781833439339, -0.015189030909909623, -0.017807911009254423, -0.029297548362325877, -0.029110523637638063, -0.025657903155721626, -0.007819820011445924, -0.013262857250060245, -0.018676013006198686, -0.010231268447212359, -0.017935301697603098, -0.024091376818578217, -0.004582500055168543, -0.009793745297434249, -0.033210089439428735, -0.037388652009759084]\n",
      "Fold 1\n",
      "[0.1364716053103292, 0.17540466600156573, 0.20370239650588393, 0.17836717497641275, 0.19079997211946187, 0.2266124885823058, 0.22192051878173022, 0.16966166304499716, 0.16476826940551853, 0.17289844137069835, 0.1744513676772509, 0.1667973380593305, 0.17189425423075988, 0.17970516800007105, 0.1554882036674395, 0.1595716483928985, 0.15821147511039294, 0.17018932021165717, 0.16995186268108672, 0.17099841361583773, 0.15217388413321703, 0.1444873693722986, 0.1484234266482462, 0.15874662635163833, 0.16389145176425582, 0.16216411348511212, 0.15944534843222707, 0.13804245757363068, 0.1491980018036127, 0.1562856153816022, 0.1613674742934721, 0.1619941306715301, 0.14787579105100743, 0.1547546339688455, 0.16346782359472173, 0.16530165093690616, 0.15909975806120308, 0.15222958915037127, 0.16254580601438565, 0.1560252006665731, 0.1499448603086059, 0.16930958837540974, 0.1668473865925861, 0.1598511882041436, 0.15937326446280248, 0.15391449578641075, 0.15682818266630988, 0.16412317254575506, 0.16045249359887656, 0.16183443400447892, 0.15709150081045023, 0.1616986808268332, 0.1661063027165817, 0.16834119725347618, 0.1551307243225272, 0.15912924579940158, 0.15267838108239298, 0.16293204804496064, 0.15874120361763966, 0.16460445460018497, 0.157954268190609, 0.16100776985501997, 0.15083821420466553, 0.1592019996152374, 0.16868296084853238, 0.1503042321462331, 0.17595013013491503, 0.15951200493149198, 0.1653582098211567, 0.16922492911725917, 0.16097261714707517, 0.1660245389089664, 0.16426498639970266, 0.15913923264501784, 0.1596951108245764, 0.16624069082470272, 0.1572549411431087, 0.15606858461792872, 0.16447876046483972, 0.17272248893516243, 0.17249376810880793, 0.17514338219968795, 0.1454607021438244, 0.1570352311614741, 0.16550354598538347, 0.1680263916467402, 0.16557349003390576, 0.16436702171061746, 0.1661591116157873, 0.16272432301156484, 0.1531839929761778, 0.1657129770421944, 0.16234036927768597, 0.169579661066268, 0.16345404426714155, 0.14768658232005694, 0.1630321930914327, 0.17217889501249048, 0.16017207382436285, 0.16240317936767376]\n",
      "Fold 2\n",
      "[0.03125876033666994, 0.06845236781831252, 0.058788941582080013, 0.07167406520170869, 0.018751147154245546, 0.06482999261655793, 0.027497100101718665, 0.038619217451554284, 0.02678470568999879, 0.028758385059549717, 0.033491016578740256, 0.02256782843953125, 0.012320507722374092, 0.01902464440262054, 0.003964502609967149, 0.032192331548999656, 0.008972678647640016, 0.023000681442135164, 0.021729704809341978, 0.01741950537117931, 0.01515998785702967, 0.0031928475394048706, 0.04114292756323604, 0.011251054925237338, 0.019094765624609898, 0.03563042723087022, 0.013791352181965855, -0.0024093581746812663, 0.0194423772803497, 0.02656653224053496, 0.006472808883803958, 0.02840543823254679, 0.019236529739226297, 0.012837099913393622, 0.012670039073176329, 0.01976201106601857, 0.01454566544206731, 0.007407050304088749, 0.030020312915204923, 0.008304270491882235, 0.0013573800928655944, 0.030273690838160835, 0.01197300554640575, -0.0008373364001223526, 0.01767200040943409, 0.019352376436333824, -0.0066310440369365085, 0.010770601922693053, 0.018676192558655734, 0.0293950152430924, 0.0062926007541751995, 0.01921126097217207, 0.008501409693605005, 0.009182148906319097, 0.007857816534108093, 0.014221464088996844, 0.008471885713584338, 0.019971971691993003, 0.021485310075945483, 0.010841200974326228, -0.008605529417526424, 0.013274575813507399, 0.011160825810561069, 0.03553016905537585, 0.018722759432331225, 0.02899484183676855, 0.016698419268533063, 0.0191484911891318, 0.027285202808026047, 0.021961352906419526, 0.016823072148360617, 0.029439720865968667, 0.024185906851044957, 0.009374032967194957, 0.014182120086505323, 0.016046237066514302, 0.011835985236142727, 0.004332920718986693, 0.01954762014796936, 0.030564346194513668, 0.031281942834789786, 0.027620973803351746, 0.0241547613381009, 0.017260538756394914, 0.014432895005943434, 0.027857433011820705, 0.03312926112222529, 0.03827335393662413, 0.029422578536744844, 0.01232225903759221, 0.005832867843537546, -0.00269736734068335, 0.01506460804121533, 0.019843288729600297, 0.021771365388984942, 0.027801276730270903, 0.02393055011519022, 0.03912473042808713, 0.03309032667972951, 0.021726608310234673]\n",
      "Fold 3\n",
      "[0.017681105608470218, 0.04743116314627275, 0.032455973012392204, 0.03592156425833877, 0.02377763173104554, 0.10037009582058991, 0.07158422143559638, 0.038999581853512114, 0.03547043584288361, 0.017114609684721702, -0.013592095910225194, -0.03464736598073151, -0.03854136601529656, -0.04595249933959831, -0.05347526049145881, -0.07123057219488499, -0.057527818533404294, -0.05703335443038218, -0.06913977870373507, -0.07139746374484406, -0.060883621796557146, -0.04919236523155851, -0.04224531040997794, -0.05900033166289947, -0.03660418481276289, -0.03936033038783513, -0.041763354968633946, -0.04082352201668538, -0.042817520386021435, -0.04053451122311465, -0.039641115566565834, -0.044681990570577125, -0.03630077226748836, -0.036968073148767766, -0.03982412728578362, -0.04627084871555537, -0.050169095509791435, -0.028162450473975906, -0.04466299562962716, -0.022758702524341926, -0.027872488968550973, -0.03347582817018031, -0.04257576475765408, -0.02342864856321358, -0.04813587543217621, -0.010396541905738807, -0.030374122986575602, -0.0362720576077048, -0.04516485555509473, -0.03196839129895132, -0.026412397851737778, -0.018861577949803986, -0.01930311744458216, -0.019821926059403326, -0.025564569475710116, -0.03367088728216676, -0.03628209969762596, -0.031848051193931026, -0.03021908401358529, -0.03646233483834571, -0.02026765553441488, -0.017845287167740868, -0.026416414502627196, -0.046222047063912045, -0.03139590156376492, -0.038238199375585265, -0.0013735603811301832, -0.04359028871582949, -0.02046156593585864, -0.03034566787046815, -0.03207490538604544, -0.03484343706439031, -0.01531436927563834, -0.022661501467996836, -0.019913088366102383, -0.032791509332241585, -0.03067112796077013, -0.0351999780553357, -0.012153244281891296, -0.023593232281861658, -0.020497964870279863, -0.02495901999337662, -0.017890165903035517, -0.025573796018510578, -0.027675266339468586, -0.024561997739777963, -0.029765434958935143, -0.022158261158757478, -0.010084106014637804, -0.04007150553537001, -0.012648918082467408, -0.015142462736474707, -0.03217122614700297, -0.017593675222796524, -0.018855358188441, -0.0003977035885651592, -0.012516773445114837, -0.03818263844082906, -0.04326059999647414, 0.009835194639215386]\n",
      "Fold 4\n",
      "[0.05128740781107057, 0.06077865873467623, 0.05779562477066043, 0.0017960675186529107, 0.033992676543193386, 0.01158628273330771, 0.0036475376808284277, 0.018794367538003707, -0.01800538316363254, -0.016080748255431182, 0.007556095520875237, -0.035570872326208605, -0.01815416462779251, -0.044509882701049205, -0.03290925225854912, -0.02589184834328937, -0.022816109016835298, -0.03569626341270153, -0.03356122205980571, -0.023799250371923087, -0.023646672602142708, -0.040121571427207015, -0.03474013970958038, -0.033639809749049364, -0.0330699025081981, -0.0223058393056145, -0.017446000000863337, -0.03963443643526349, -0.0572824963719585, -0.022394424156650362, -0.04488990913735976, -0.024814032981725295, -0.040765801228699966, -0.0330312005251595, -0.04216814426262582, -0.0422465026129111, -0.034561585580135404, -0.018236545802851743, -0.022583498382113484, -0.01540065171863303, -0.013635062679147771, -0.02746416338020232, -0.03955384745403624, -0.037863534522771254, -0.023176770800302428, -0.03389831264287339, -0.022351048120086682, -0.04553504073909183, -0.026283415482227786, -0.01681560945073894, -0.037790121046255204, -0.03257208514632033, -0.03227494659748137, -0.03824374742122656, -0.023561533670601705, -0.025259450220872462, -0.043030083372746615, -0.033850897035518954, -0.02568656600348296, -0.033470240059413126, -0.031768173666323486, -0.035887430633163235, -0.02249562773169729, -0.03013745139098099, -0.02671265516386507, -0.021687472152972894, -0.02768542557723283, -0.016019501496108552, -0.026073927509606413, -0.020783808539452754, -0.02688075177715258, -0.023973616902616336, -0.03440717848572703, -0.0160198197662012, -0.024510558961339548, -0.029455665103816597, -0.02790588566559143, -0.023355784780709037, -0.032334133019717846, -0.0296464487066347, -0.036776988099629176, -0.030186469079037073, -0.040621399013500835, -0.018251012745693473, -0.023451844108077945, -0.024157522815276184, -0.024427178046383377, -0.042991668840401814, -0.013308686513675142, -0.02141582787665043, -0.029256096558657588, -0.028983379672780734, -0.040090112069321275, -0.023433702548625743, -0.009511837376506507, -0.02753094520035079, -0.029455260547573175, -0.02649927697073828, -0.023608990736775165, -0.015188196929666315]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_eval_stats = []\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f'----- Combination {i} -----')\n",
    "    print(*zip(param_grid.keys(), params))\n",
    "    model_type, num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs, curvature = params\n",
    "\n",
    "    for fold, (fold_train_indices, fold_val_indices) in enumerate(zip(train_indices, val_indices)):\n",
    "        print(f'Fold {fold}')\n",
    "\n",
    "        fold_train_X = train_X[fold_train_indices]\n",
    "        fold_train_y = train_y[fold_train_indices]\n",
    "        fold_val_X   = val_X[fold_val_indices]\n",
    "        fold_val_y   = val_y[fold_val_indices]\n",
    "        # fold_val_X   = test_X\n",
    "        # fold_val_y   = test_y\n",
    "\n",
    "        train_dataset = CustomDataset(fold_train_X, fold_train_y)\n",
    "        val_dataset = CustomDataset(fold_val_X, fold_val_y)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            manifold = PoincareBall(c=Curvature(curvature))\n",
    "        elif model_type == 'euc':\n",
    "            manifold = None\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            model = HYP_MLP(input_size=train_X.shape[1],\n",
    "                            output_size=train_y.shape[1],\n",
    "                            layer_size=layer_size,\n",
    "                            num_hidden_layers=num_hidden_layers,\n",
    "                            manifold=manifold).to(device)\n",
    "        elif model_type == 'euc':\n",
    "            model = EUC_MLP(input_size=train_X.shape[1],\n",
    "                            output_size=train_y.shape[1],\n",
    "                            layer_size=layer_size,\n",
    "                            num_hidden_layers=num_hidden_layers).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            optimizer = RiemannianAdam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif model_type == 'euc':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if model_type == 'hyp':\n",
    "                eval_stats['loss']['train'].append(hyp_train_model(model, train_loader, criterion, optimizer, manifold, device))\n",
    "                eval_stats['loss']['val'].append(util.h_evaluate_loss(model, val_loader, criterion, manifold, device))\n",
    "\n",
    "                eval_stats['mae']['train'].append(util.h_evaluate_r2(model, train_loader, manifold, device))\n",
    "                eval_stats['mae']['val'].append(util.h_evaluate_r2(model, val_loader, manifold, device))\n",
    "            elif model_type == 'euc':\n",
    "                eval_stats['loss']['train'].append(euc_train_model(model, train_loader, criterion, optimizer, device))\n",
    "                eval_stats['loss']['val'].append(util.evaluate_loss(model, val_loader, criterion, device))\n",
    "\n",
    "                eval_stats['mae']['train'].append(util.evaluate_r2(model, train_loader, device))\n",
    "                eval_stats['mae']['val'].append(util.evaluate_r2(model, val_loader, device))\n",
    "\n",
    "        print(eval_stats['mae']['val'])\n",
    "        param_eval_stats.append(eval_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- Combination 0 -----<br>\n",
    "('model_type', 'hyp') ('num_hidden_layers', 4) ('layer_size', 256) ('lr', 0.003) ('weight_decay', 0.001) ('batch_size', 1024) ('epochs', 10) ('curvature', -1)<br>\n",
    "[0.07909973876278877,..., 0.10081582998329697]<br>\n",
    "[0.10867697601925531,..., 0.13209981919615804]<br>\n",
    "[0.07888923936887043,..., 0.0806825271316864]<br>\n",
    "[0.06372710507893672,..., 0.07582992121683846]<br>\n",
    "[0.07242007521111367,..., 0.08348101499310495]\n",
    "----- Combination 1 -----<br>\n",
    "('model_type', 'euc') ('num_hidden_layers', 4) ('layer_size', 256) ('lr', 0.003) ('weight_decay', 0.001) ('batch_size', 1024) ('epochs', 10) ('curvature', -1)<br>\n",
    "[0.0597327889296524,..., -0.030407411174621774]<br>\n",
    "[0.1364716053103292,..., 0.17289844137069835]<br>\n",
    "[0.03125876033666994,..., 0.028758385059549717]<br>\n",
    "[0.017681105608470218,..., 0.017114609684721702]<br>\n",
    "[0.05128740781107057,..., -0.016080748255431182]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- Combination 0 -----<br>\n",
    "('model_type', 'hyp') ('num_hidden_layers', 4) ('layer_size', 256) ('lr', 0.003) ('weight_decay', 0.001) ('batch_size', 1024) ('epochs', 10) ('curvature', -1)<br>\n",
    "[0.10685445129188582,..., 0.12495217899578208]<br>\n",
    "[0.0962506210960994,..., 0.11440528334098365]<br>\n",
    "[0.10296769072663453,..., 0.1108899194806743]<br>\n",
    "[0.10252289118510735,..., 0.11087943753314528]<br>\n",
    "[0.10206298167066763,..., 0.12226956711673814]\n",
    "----- Combination 1 -----<br>\n",
    "('model_type', 'euc') ('num_hidden_layers', 4) ('layer_size', 256) ('lr', 0.003) ('weight_decay', 0.001) ('batch_size', 1024) ('epochs', 10) ('curvature', -1)<br>\n",
    "[0.10988954513029288,..., 0.08912999329518442]<br>\n",
    "[0.14445988839066382,..., 0.10542541591240182]<br>\n",
    "[0.11891582177369578,..., 0.07819352731161501]<br>\n",
    "[0.1130920031919792,..., 0.09652397295541668]<br>\n",
    "[0.12616840387768863,..., 0.12226560128698781]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>COMBINATION MODEL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BEST MODEL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'model_type': 'hyp',\n",
    "    'num_hidden_layers': 4,\n",
    "    'layer_size': 256,\n",
    "    'lr': 0.003,\n",
    "    'weight_decay': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 10,\n",
    "    'curvature': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[0.10765948756282011, 0.12308321606784109, 0.12454766437259072, 0.12698461687460463, 0.12526742669738572, 0.12486854942081814, 0.12370138416228064, 0.12335186350024209, 0.12311975745553928, 0.12481188844227081]\n",
      "Fold 1\n",
      "[0.10694572659119995, 0.12300988018483655, 0.12596251597252395, 0.12713697133334384, 0.12319071662434075, 0.1237505160267185, 0.12400483580433477, 0.12384196969943467, 0.1234383582915921, 0.12249493166258585]\n",
      "Fold 2\n",
      "[0.10687856100840314, 0.12332382928659225, 0.12391855430801797, 0.12579272453833407, 0.12444610385627473, 0.12367803361043006, 0.12372331089353018, 0.1233711942192049, 0.1228020182704326, 0.12271869834607192]\n",
      "Fold 3\n",
      "[0.10750576340391302, 0.12294992224508638, 0.12448672795407086, 0.12575785160685407, 0.12344712346292308, 0.12408783516163593, 0.12411291205089521, 0.12396073754233594, 0.12467454068302844, 0.1239832776480632]\n",
      "Fold 4\n",
      "[0.106911679887455, 0.12443725528270613, 0.1259176711359262, 0.12665070801147957, 0.12356156026727742, 0.12352775207013689, 0.12360777193862876, 0.12350345822525063, 0.12307396887222734, 0.12265171284640589]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_type, num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs, curvature = best_params.values()\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_X, train_y)):\n",
    "    print(f'Fold {fold}')\n",
    "    # fold_train_X, fold_val_X = train_X[train_idx], train_X[val_idx]\n",
    "    # fold_train_y, fold_val_y = train_y[train_idx], train_y[val_idx]\n",
    "    fold_train_X, fold_val_X = train_X[train_idx], test_X\n",
    "    fold_train_y, fold_val_y = train_y[train_idx], test_y\n",
    "    # fold_train_X, fold_val_X = train_X, test_X\n",
    "    # fold_train_y, fold_val_y = train_y, test_y\n",
    "\n",
    "    train_dataset = CustomDataset(fold_train_X, fold_train_y)\n",
    "    val_dataset = CustomDataset(fold_val_X, fold_val_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    if model_type == 'hyp':\n",
    "        manifold = PoincareBall(c=Curvature(curvature))\n",
    "    elif model_type == 'euc':\n",
    "        manifold = None\n",
    "\n",
    "    if model_type == 'hyp':\n",
    "        model = HYP_MLP(input_size=train_X.shape[1],\n",
    "                        output_size=train_y.shape[1],\n",
    "                        layer_size=layer_size,\n",
    "                        num_hidden_layers=num_hidden_layers,\n",
    "                        manifold=manifold).to(device)\n",
    "    elif model_type == 'euc':\n",
    "        model = EUC_MLP(input_size=train_X.shape[1],\n",
    "                        output_size=train_y.shape[1],\n",
    "                        layer_size=layer_size,\n",
    "                        num_hidden_layers=num_hidden_layers).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if model_type == 'hyp':\n",
    "        optimizer = RiemannianAdam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif model_type == 'euc':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if model_type == 'hyp':\n",
    "            eval_stats['loss']['train'].append(hyp_train_model(model, train_loader, criterion, optimizer, manifold, device))\n",
    "            eval_stats['loss']['val'].append(util.h_evaluate_loss(model, val_loader, criterion, manifold, device))\n",
    "\n",
    "            eval_stats['mae']['train'].append(util.h_evaluate_r2(model, train_loader, manifold, device))\n",
    "            eval_stats['mae']['val'].append(util.h_evaluate_r2(model, val_loader, manifold, device))\n",
    "        elif model_type == 'euc':\n",
    "            eval_stats['loss']['train'].append(euc_train_model(model, train_loader, criterion, optimizer, device))\n",
    "            eval_stats['loss']['val'].append(util.evaluate_loss(model, val_loader, criterion, device))\n",
    "\n",
    "            eval_stats['mae']['train'].append(util.evaluate_r2(model, train_loader, device))\n",
    "            eval_stats['mae']['val'].append(util.evaluate_r2(model, val_loader, device))\n",
    "\n",
    "    print(eval_stats['mae']['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000\n",
    "[0.09094981766612582, 0.09608602443769439, 0.10213084733073284, 0.10297098476158782, 0.1039252772204806, 0.10574258559121522, 0.10593447081943014, 0.1103528270397624, 0.10905810868205902, 0.11674655082317638]\n",
    "[0.09124620536624219, 0.09588467278224783, 0.1109519321222205, 0.10871296762480671, 0.11322182908185724, 0.10604926039894355, 0.11641335699831064, 0.11669891168893905, 0.1163291967061507, 0.11563811270315874]\n",
    "[0.08261279344412803, 0.0949209955128152, 0.10394174902203669, 0.11074634671414854, 0.10959598998285282, 0.10492545479477039, 0.10590842284461503, 0.10538532105083259, 0.10268829907521582, 0.10696282556494001]\n",
    "[0.09143749197812866, 0.10686854162056632, 0.10184344301563328, 0.10983260881492879, 0.11453995365690633, 0.10643641196149398, 0.10807662045371369, 0.11264002132608508, 0.11569218070241385, 0.11172063540914148]\n",
    "[0.08755047684265042, 0.09826624201788955, 0.10408250848514021, 0.10678387971099373, 0.10168236861206509, 0.09372892354517087, 0.10054121141766975, 0.11307510662137904, 0.10818904212215745, 0.11473416400049585]\n",
    "\n",
    "100\n",
    "[0.10712686537621345, 0.12349026400269329, 0.1246606174341069, 0.12693350344457163, 0.12540473804914373, 0.12392351307841984, 0.12310184680085018, 0.1235122503852268, 0.12269077053490243, 0.12184334164640719]\n",
    "[0.10825157818713378, 0.12289771182725055, 0.12439158651151372, 0.1261395813343429, 0.1253862140092597, 0.12366027630753869, 0.12406626861964988, 0.1242455269123071, 0.12363573750877084, 0.12229140361532519]\n",
    "[0.10685337929854738, 0.12412607759674942, 0.12566753200057623, 0.12656694272727384, 0.1249030203813584, 0.12408679288839279, 0.12404666634192532, 0.12445838543532435, 0.12457812238461687, 0.12304167921955317]\n",
    "[0.10745761389567116, 0.12397573098456129, 0.12521314400651082, 0.1276813410290952, 0.1252595522840151, 0.12429336231380055, 0.12474528515336958, 0.1244248228708727, 0.12511605799175451, 0.12342070994183856]\n",
    "[0.10759642672103734, 0.12380378206783893, 0.12494015763741242, 0.12673535494694677, 0.12506894028324306, 0.12376698712522265, 0.12254735888244327, 0.12283803721108438, 0.12404691001407472, 0.12351575220797617]\n",
    "\n",
    "50\n",
    "[0.07677710553661399, 0.10584953169202371, 0.11966844890984371, 0.12411849676234876, 0.12520028446868986, 0.12418479259928074, 0.12732371128638775, 0.126271569310595, 0.12580454850610243, 0.12429738209045665]\n",
    "[0.07721830195010819, 0.10745842540642501, 0.12112070383855837, 0.12386530838495947, 0.125923156127822, 0.1258847214082912, 0.12830232653117135, 0.12669207756182055, 0.12631119756542175, 0.12450957025993643]\n",
    "[0.07632709753632146, 0.10763185461397703, 0.1186022848413865, 0.12323234538214417, 0.12484191533525628, 0.12444830403759084, 0.12718327433134718, 0.12617910882856184, 0.12485011626791608, 0.1229076001872507]\n",
    "[0.0767599902123989, 0.1089768569863075, 0.120941318864213, 0.12583457699099382, 0.1255480130415434, 0.12394682506457874, 0.12700223229913882, 0.12678108092511747, 0.12549870300322666, 0.12525940753014855]\n",
    "[0.07607748019287443, 0.10721069870104394, 0.12062286165389509, 0.12380926861086566, 0.1258001163362215, 0.12521044913472032, 0.1273641772466992, 0.1263506895116415, 0.12492659096620544, 0.12464252907020315]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
