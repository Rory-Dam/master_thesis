{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from hypll import nn as hnn\n",
    "from hypll.tensors import TangentTensor\n",
    "from hypll.optim import RiemannianAdam\n",
    "from hypll.manifolds.poincare_ball import Curvature, PoincareBall\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AROMA = ['A_malt_all', 'A_malt_grain', 'A_malt_bread', 'A_malt_cara',\n",
    "'A_malt_burn', 'A_hops_all', 'A_hops_citrus', 'A_hops_tropical',\n",
    "'A_hops_noble', 'A_hops_woody', 'A_esters_all', 'A_esters_ethac',\n",
    "'A_esters_isoaa', 'A_esters_flower', 'A_esters_fruity']\n",
    "\n",
    "FLAVOUR = ['F_malt_all', 'F_malt_grain', 'F_malt_bread', 'F_malt_cara', \n",
    "'F_malt_burn', 'F_hops_all', 'F_hops_citrus', 'F_hops_tropical', \n",
    "'F_hops_noble', 'F_hops_woody', 'F_esters_all', 'F_esters_ethac',\n",
    "'F_esters_isoaa', 'F_esters_flower', 'F_esters_fruity']\n",
    "\n",
    "BOTH = ['A_malt_all', 'A_malt_grain', 'A_malt_bread', 'A_malt_cara',\n",
    "'A_malt_burn', 'A_hops_all', 'A_hops_citrus', 'A_hops_tropical',\n",
    "'A_hops_noble', 'A_hops_woody', 'A_esters_all', 'A_esters_ethac',\n",
    "'A_esters_isoaa', 'A_esters_flower', 'A_esters_fruity', 'F_malt_all',\n",
    "'F_malt_grain', 'F_malt_bread', 'F_malt_cara', 'F_malt_burn',\n",
    "'F_hops_all', 'F_hops_citrus', 'F_hops_tropical', 'F_hops_noble',\n",
    "'F_hops_woody', 'F_esters_all', 'F_esters_ethac', 'F_esters_isoaa',\n",
    "'F_esters_flower', 'F_esters_fruity']\n",
    "\n",
    "REST = ['acidity', 'bitternes','sweetness', 'X4vg', 'diacetyl', 'dms',\n",
    "'metallic', 'stale_hops', 't2n', 'orange', 'coriander', 'clove', 'lactic',\n",
    "'acetic', 'barnyard', 'alcohol', 'aftertaste', 'body', 'co2', 'overall']\n",
    "\n",
    "ALL = ['A_malt_all', 'A_malt_grain', 'A_malt_bread', 'A_malt_cara',\n",
    "'A_malt_burn', 'A_hops_all', 'A_hops_citrus', 'A_hops_tropical',\n",
    "'A_hops_noble', 'A_hops_woody', 'A_esters_all', 'A_esters_ethac',\n",
    "'A_esters_isoaa', 'A_esters_flower', 'A_esters_fruity', 'F_malt_all',\n",
    "'F_malt_grain', 'F_malt_bread', 'F_malt_cara', 'F_malt_burn',\n",
    "'F_hops_all', 'F_hops_citrus', 'F_hops_tropical', 'F_hops_noble',\n",
    "'F_hops_woody', 'F_esters_all', 'F_esters_ethac', 'F_esters_isoaa',\n",
    "'F_esters_flower', 'F_esters_fruity', 'acidity', 'bitternes',\n",
    "'sweetness', 'X4vg', 'diacetyl', 'dms', 'metallic', 'stale_hops', 't2n',\n",
    "'orange', 'coriander', 'clove', 'lactic', 'acetic', 'barnyard',\n",
    "'alcohol', 'aftertaste', 'body', 'co2', 'overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLS = BOTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1750, 231), (1750, 30), (175, 231), (175, 30), (75, 231), (75, 30))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_X = pd.read_csv('../data/beer_features_train_samples_small.csv', index_col=0)\n",
    "df_train_y = pd.read_csv('../data/beer_labels_panel_train_samples_small.csv', index_col=0)[LABEL_COLS]\n",
    "df_val_X = pd.read_csv('../data/beer_features_train.csv', index_col=0)\n",
    "df_val_y = pd.read_csv('../data/beer_labels_panel_train.csv', index_col=0)[LABEL_COLS]\n",
    "df_test_X = pd.read_csv('../data/beer_features_test.csv', index_col=0)\n",
    "df_test_y = pd.read_csv('../data/beer_labels_panel_test.csv', index_col=0)[LABEL_COLS]\n",
    "\n",
    "train_X = df_train_X.values\n",
    "train_y = df_train_y.values\n",
    "val_X = df_val_X.values\n",
    "val_y = df_val_y.values\n",
    "test_X = df_test_X.values\n",
    "test_y = df_test_y.values\n",
    "\n",
    "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A_malt_all', 'A_malt_grain', 'A_malt_bread', 'A_malt_cara',\n",
       "       'A_malt_burn', 'A_hops_all', 'A_hops_citrus', 'A_hops_tropical',\n",
       "       'A_hops_noble', 'A_hops_woody', 'A_esters_all', 'A_esters_ethac',\n",
       "       'A_esters_isoaa', 'A_esters_flower', 'A_esters_fruity', 'F_malt_all',\n",
       "       'F_malt_grain', 'F_malt_bread', 'F_malt_cara', 'F_malt_burn',\n",
       "       'F_hops_all', 'F_hops_citrus', 'F_hops_tropical', 'F_hops_noble',\n",
       "       'F_hops_woody', 'F_esters_all', 'F_esters_ethac', 'F_esters_isoaa',\n",
       "       'F_esters_flower', 'F_esters_fruity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_malt_all</th>\n",
       "      <th>A_malt_grain</th>\n",
       "      <th>A_malt_bread</th>\n",
       "      <th>A_malt_cara</th>\n",
       "      <th>A_malt_burn</th>\n",
       "      <th>A_hops_all</th>\n",
       "      <th>A_hops_citrus</th>\n",
       "      <th>A_hops_tropical</th>\n",
       "      <th>A_hops_noble</th>\n",
       "      <th>A_hops_woody</th>\n",
       "      <th>...</th>\n",
       "      <th>F_hops_all</th>\n",
       "      <th>F_hops_citrus</th>\n",
       "      <th>F_hops_tropical</th>\n",
       "      <th>F_hops_noble</th>\n",
       "      <th>F_hops_woody</th>\n",
       "      <th>F_esters_all</th>\n",
       "      <th>F_esters_ethac</th>\n",
       "      <th>F_esters_isoaa</th>\n",
       "      <th>F_esters_flower</th>\n",
       "      <th>F_esters_fruity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.941996</td>\n",
       "      <td>-1.075726</td>\n",
       "      <td>-0.285841</td>\n",
       "      <td>-0.178741</td>\n",
       "      <td>-0.604294</td>\n",
       "      <td>-0.656378</td>\n",
       "      <td>0.099024</td>\n",
       "      <td>0.241602</td>\n",
       "      <td>-1.268708</td>\n",
       "      <td>-0.956593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579178</td>\n",
       "      <td>0.049896</td>\n",
       "      <td>0.394143</td>\n",
       "      <td>-0.699003</td>\n",
       "      <td>-0.489993</td>\n",
       "      <td>0.269429</td>\n",
       "      <td>0.256784</td>\n",
       "      <td>0.265336</td>\n",
       "      <td>0.252748</td>\n",
       "      <td>0.884750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.893153</td>\n",
       "      <td>-0.977069</td>\n",
       "      <td>-0.195571</td>\n",
       "      <td>-0.179653</td>\n",
       "      <td>-0.543785</td>\n",
       "      <td>-0.639002</td>\n",
       "      <td>0.101024</td>\n",
       "      <td>0.256538</td>\n",
       "      <td>-1.409950</td>\n",
       "      <td>-0.906288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.587823</td>\n",
       "      <td>0.267661</td>\n",
       "      <td>0.272445</td>\n",
       "      <td>-0.709525</td>\n",
       "      <td>-0.588568</td>\n",
       "      <td>0.408465</td>\n",
       "      <td>0.330896</td>\n",
       "      <td>0.257570</td>\n",
       "      <td>0.160407</td>\n",
       "      <td>0.917691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.880786</td>\n",
       "      <td>-1.137411</td>\n",
       "      <td>-0.336594</td>\n",
       "      <td>-0.230738</td>\n",
       "      <td>-0.601020</td>\n",
       "      <td>-0.634813</td>\n",
       "      <td>0.015769</td>\n",
       "      <td>0.156042</td>\n",
       "      <td>-1.255992</td>\n",
       "      <td>-0.990153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.532024</td>\n",
       "      <td>0.222864</td>\n",
       "      <td>0.307719</td>\n",
       "      <td>-0.707494</td>\n",
       "      <td>-0.559350</td>\n",
       "      <td>0.364755</td>\n",
       "      <td>0.260127</td>\n",
       "      <td>0.259842</td>\n",
       "      <td>0.104715</td>\n",
       "      <td>0.975348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.879230</td>\n",
       "      <td>-1.071754</td>\n",
       "      <td>-0.219354</td>\n",
       "      <td>-0.193606</td>\n",
       "      <td>-0.624828</td>\n",
       "      <td>-0.643238</td>\n",
       "      <td>0.045454</td>\n",
       "      <td>0.138254</td>\n",
       "      <td>-1.286502</td>\n",
       "      <td>-0.981138</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658764</td>\n",
       "      <td>0.168499</td>\n",
       "      <td>0.340776</td>\n",
       "      <td>-0.747876</td>\n",
       "      <td>-0.543543</td>\n",
       "      <td>0.417304</td>\n",
       "      <td>0.290938</td>\n",
       "      <td>0.220279</td>\n",
       "      <td>0.156029</td>\n",
       "      <td>0.831098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.875527</td>\n",
       "      <td>-1.063935</td>\n",
       "      <td>-0.205753</td>\n",
       "      <td>-0.175031</td>\n",
       "      <td>-0.584146</td>\n",
       "      <td>-0.621780</td>\n",
       "      <td>0.057810</td>\n",
       "      <td>0.077312</td>\n",
       "      <td>-1.368296</td>\n",
       "      <td>-0.878475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.462325</td>\n",
       "      <td>0.225635</td>\n",
       "      <td>0.333147</td>\n",
       "      <td>-0.852606</td>\n",
       "      <td>-0.565921</td>\n",
       "      <td>0.253423</td>\n",
       "      <td>0.269553</td>\n",
       "      <td>0.359259</td>\n",
       "      <td>0.203860</td>\n",
       "      <td>0.835554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>-1.233758</td>\n",
       "      <td>-0.868750</td>\n",
       "      <td>-0.638104</td>\n",
       "      <td>-0.875494</td>\n",
       "      <td>-0.278827</td>\n",
       "      <td>0.173468</td>\n",
       "      <td>1.801565</td>\n",
       "      <td>0.154312</td>\n",
       "      <td>-0.052745</td>\n",
       "      <td>-0.926968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.199043</td>\n",
       "      <td>0.393259</td>\n",
       "      <td>-0.062328</td>\n",
       "      <td>-0.198177</td>\n",
       "      <td>-0.873516</td>\n",
       "      <td>-0.439124</td>\n",
       "      <td>-0.571566</td>\n",
       "      <td>-0.152859</td>\n",
       "      <td>0.955879</td>\n",
       "      <td>-0.472166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>-1.063261</td>\n",
       "      <td>-0.751914</td>\n",
       "      <td>-0.690999</td>\n",
       "      <td>-0.924281</td>\n",
       "      <td>-0.199706</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>1.762356</td>\n",
       "      <td>0.196654</td>\n",
       "      <td>-0.057584</td>\n",
       "      <td>-0.970705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.241538</td>\n",
       "      <td>0.380833</td>\n",
       "      <td>-0.016418</td>\n",
       "      <td>-0.088493</td>\n",
       "      <td>-0.886290</td>\n",
       "      <td>-0.455828</td>\n",
       "      <td>-0.626556</td>\n",
       "      <td>-0.211931</td>\n",
       "      <td>0.980663</td>\n",
       "      <td>-0.437777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>-1.153341</td>\n",
       "      <td>-0.802079</td>\n",
       "      <td>-0.600642</td>\n",
       "      <td>-0.885723</td>\n",
       "      <td>-0.224749</td>\n",
       "      <td>0.186860</td>\n",
       "      <td>1.835002</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>-0.184398</td>\n",
       "      <td>-0.911978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289067</td>\n",
       "      <td>0.469711</td>\n",
       "      <td>-0.103032</td>\n",
       "      <td>-0.149013</td>\n",
       "      <td>-0.920699</td>\n",
       "      <td>-0.412000</td>\n",
       "      <td>-0.697202</td>\n",
       "      <td>-0.144763</td>\n",
       "      <td>0.922279</td>\n",
       "      <td>-0.484735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>-1.203214</td>\n",
       "      <td>-0.841736</td>\n",
       "      <td>-0.702683</td>\n",
       "      <td>-0.924173</td>\n",
       "      <td>-0.245043</td>\n",
       "      <td>0.189425</td>\n",
       "      <td>1.795088</td>\n",
       "      <td>0.202355</td>\n",
       "      <td>-0.090596</td>\n",
       "      <td>-0.851933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229928</td>\n",
       "      <td>0.327766</td>\n",
       "      <td>-0.113218</td>\n",
       "      <td>-0.158142</td>\n",
       "      <td>-0.973524</td>\n",
       "      <td>-0.497657</td>\n",
       "      <td>-0.649775</td>\n",
       "      <td>-0.144909</td>\n",
       "      <td>1.004456</td>\n",
       "      <td>-0.433727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>-1.195146</td>\n",
       "      <td>-0.865512</td>\n",
       "      <td>-0.613141</td>\n",
       "      <td>-0.913100</td>\n",
       "      <td>-0.202405</td>\n",
       "      <td>0.153794</td>\n",
       "      <td>1.711885</td>\n",
       "      <td>0.199160</td>\n",
       "      <td>-0.156965</td>\n",
       "      <td>-0.914155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268506</td>\n",
       "      <td>0.370085</td>\n",
       "      <td>-0.075180</td>\n",
       "      <td>-0.122509</td>\n",
       "      <td>-0.919742</td>\n",
       "      <td>-0.458488</td>\n",
       "      <td>-0.629269</td>\n",
       "      <td>-0.017913</td>\n",
       "      <td>0.959702</td>\n",
       "      <td>-0.474090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1750 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A_malt_all  A_malt_grain  A_malt_bread  A_malt_cara  A_malt_burn  \\\n",
       "0      -0.941996     -1.075726     -0.285841    -0.178741    -0.604294   \n",
       "1      -0.893153     -0.977069     -0.195571    -0.179653    -0.543785   \n",
       "2      -0.880786     -1.137411     -0.336594    -0.230738    -0.601020   \n",
       "3      -0.879230     -1.071754     -0.219354    -0.193606    -0.624828   \n",
       "4      -0.875527     -1.063935     -0.205753    -0.175031    -0.584146   \n",
       "...          ...           ...           ...          ...          ...   \n",
       "1745   -1.233758     -0.868750     -0.638104    -0.875494    -0.278827   \n",
       "1746   -1.063261     -0.751914     -0.690999    -0.924281    -0.199706   \n",
       "1747   -1.153341     -0.802079     -0.600642    -0.885723    -0.224749   \n",
       "1748   -1.203214     -0.841736     -0.702683    -0.924173    -0.245043   \n",
       "1749   -1.195146     -0.865512     -0.613141    -0.913100    -0.202405   \n",
       "\n",
       "      A_hops_all  A_hops_citrus  A_hops_tropical  A_hops_noble  A_hops_woody  \\\n",
       "0      -0.656378       0.099024         0.241602     -1.268708     -0.956593   \n",
       "1      -0.639002       0.101024         0.256538     -1.409950     -0.906288   \n",
       "2      -0.634813       0.015769         0.156042     -1.255992     -0.990153   \n",
       "3      -0.643238       0.045454         0.138254     -1.286502     -0.981138   \n",
       "4      -0.621780       0.057810         0.077312     -1.368296     -0.878475   \n",
       "...          ...            ...              ...           ...           ...   \n",
       "1745    0.173468       1.801565         0.154312     -0.052745     -0.926968   \n",
       "1746    0.088800       1.762356         0.196654     -0.057584     -0.970705   \n",
       "1747    0.186860       1.835002         0.190688     -0.184398     -0.911978   \n",
       "1748    0.189425       1.795088         0.202355     -0.090596     -0.851933   \n",
       "1749    0.153794       1.711885         0.199160     -0.156965     -0.914155   \n",
       "\n",
       "      ...  F_hops_all  F_hops_citrus  F_hops_tropical  F_hops_noble  \\\n",
       "0     ...   -0.579178       0.049896         0.394143     -0.699003   \n",
       "1     ...   -0.587823       0.267661         0.272445     -0.709525   \n",
       "2     ...   -0.532024       0.222864         0.307719     -0.707494   \n",
       "3     ...   -0.658764       0.168499         0.340776     -0.747876   \n",
       "4     ...   -0.462325       0.225635         0.333147     -0.852606   \n",
       "...   ...         ...            ...              ...           ...   \n",
       "1745  ...   -0.199043       0.393259        -0.062328     -0.198177   \n",
       "1746  ...   -0.241538       0.380833        -0.016418     -0.088493   \n",
       "1747  ...   -0.289067       0.469711        -0.103032     -0.149013   \n",
       "1748  ...   -0.229928       0.327766        -0.113218     -0.158142   \n",
       "1749  ...   -0.268506       0.370085        -0.075180     -0.122509   \n",
       "\n",
       "      F_hops_woody  F_esters_all  F_esters_ethac  F_esters_isoaa  \\\n",
       "0        -0.489993      0.269429        0.256784        0.265336   \n",
       "1        -0.588568      0.408465        0.330896        0.257570   \n",
       "2        -0.559350      0.364755        0.260127        0.259842   \n",
       "3        -0.543543      0.417304        0.290938        0.220279   \n",
       "4        -0.565921      0.253423        0.269553        0.359259   \n",
       "...            ...           ...             ...             ...   \n",
       "1745     -0.873516     -0.439124       -0.571566       -0.152859   \n",
       "1746     -0.886290     -0.455828       -0.626556       -0.211931   \n",
       "1747     -0.920699     -0.412000       -0.697202       -0.144763   \n",
       "1748     -0.973524     -0.497657       -0.649775       -0.144909   \n",
       "1749     -0.919742     -0.458488       -0.629269       -0.017913   \n",
       "\n",
       "      F_esters_flower  F_esters_fruity  \n",
       "0            0.252748         0.884750  \n",
       "1            0.160407         0.917691  \n",
       "2            0.104715         0.975348  \n",
       "3            0.156029         0.831098  \n",
       "4            0.203860         0.835554  \n",
       "...               ...              ...  \n",
       "1745         0.955879        -0.472166  \n",
       "1746         0.980663        -0.437777  \n",
       "1747         0.922279        -0.484735  \n",
       "1748         1.004456        -0.433727  \n",
       "1749         0.959702        -0.474090  \n",
       "\n",
       "[1750 rows x 30 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   0,    1,    2, ..., 1747, 1748, 1749]),\n",
       " array([   0,    1,    2, ..., 1747, 1748, 1749]),\n",
       " array([  10,   11,   12, ..., 1747, 1748, 1749]),\n",
       " array([   0,    1,    2, ..., 1737, 1738, 1739]),\n",
       " array([   0,    1,    2, ..., 1747, 1748, 1749])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "NUM_SAMPLE_TYPES = len(val_X)\n",
    "NUM_SAMPLES_PER_TYPE = len(train_X) // NUM_SAMPLE_TYPES\n",
    "\n",
    "fold_nums = list(range(FOLDS))\n",
    "[num*NUM_SAMPLE_TYPES for num in fold_nums]\n",
    "[(num+1)*NUM_SAMPLE_TYPES for num in fold_nums]\n",
    "\n",
    "val_indices, train_indices = util.get_fold_indices_rand(NUM_SAMPLE_TYPES, NUM_SAMPLES_PER_TYPE, FOLDS)\n",
    "train_indices\n",
    "# print(FOLD_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom PyTorch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Hyperbolic </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP model\n",
    "class HYP_MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers, manifold):\n",
    "        super(HYP_MLP, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.fc_in = hnn.HLinear(input_size, layer_size, manifold=manifold)\n",
    "        self.relu = hnn.HReLU(manifold=manifold)\n",
    "        self.hidden_fcs = nn.ModuleList([hnn.HLinear(layer_size, layer_size, manifold=manifold) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = hnn.HLinear(layer_size, output_size, manifold=manifold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define training function\n",
    "def hyp_train_model(model, train_loader, criterion, optimizer, manifold, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tangents = TangentTensor(data=inputs, man_dim=-1, manifold=manifold)\n",
    "        manifold_inputs = manifold.expmap(tangents)\n",
    "\n",
    "        outputs = model(manifold_inputs)\n",
    "\n",
    "        loss = criterion(outputs.tensor, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> EUCLIDEAN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP model\n",
    "class EUC_MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers):\n",
    "        super(EUC_MLP, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.fc_in = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_fcs = nn.ModuleList([nn.Linear(layer_size, layer_size) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define training function\n",
    "def euc_train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# param_grid = {\n",
    "#     'model_type': ['hyp', 'euc'],\n",
    "#     'num_hidden_layers': [0,1,2,4,8],\n",
    "#     'layer_size': [2,4,8,16,32,64,128],\n",
    "#     'lr': [0.005,0.01,0.02],\n",
    "#     'weight_decay': [0.005,0.01,0.02],\n",
    "#     'batch_size': [1024],\n",
    "#     'epochs': [40],\n",
    "#     'curvature': [-1]\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'model_type': ['hyp', 'euc'],\n",
    "    'num_hidden_layers': [0,1,2,4,8],\n",
    "    'layer_size': [2,4,8,16,32,64,128,256,512,1024,1536,2048],\n",
    "    'lr': [0.005,0.01,0.02,0.03,0.04],\n",
    "    'weight_decay': [0.005,0.01,0.02,0.03,0.04],\n",
    "    'batch_size': [1024],\n",
    "    'epochs': [40],\n",
    "    'curvature': [-1]\n",
    "}\n",
    "\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "len(param_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Combination 0 -----\n",
      "('model_type', 'hyp') ('num_hidden_layers', 0) ('layer_size', 2) ('lr', 0.005) ('weight_decay', 0.005) ('batch_size', 1024) ('epochs', 40) ('curvature', -1)\n",
      "Fold 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_eval_stats = []\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f'----- Combination {i} -----')\n",
    "    print(*zip(param_grid.keys(), params))\n",
    "    model_type, num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs, curvature = params\n",
    "\n",
    "    for fold, (fold_train_indices, fold_val_indices) in enumerate(zip(train_indices, val_indices)):\n",
    "        print(f'Fold {fold}')\n",
    "\n",
    "        fold_train_X = train_X[fold_train_indices]\n",
    "        fold_train_y = train_y[fold_train_indices]\n",
    "        fold_val_X   = val_X[fold_val_indices]\n",
    "        fold_val_y   = val_y[fold_val_indices]\n",
    "        # fold_val_X   = test_X\n",
    "        # fold_val_y   = test_y\n",
    "\n",
    "        train_dataset = CustomDataset(fold_train_X, fold_train_y)\n",
    "        val_dataset = CustomDataset(fold_val_X, fold_val_y)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            manifold = PoincareBall(c=Curvature(curvature))\n",
    "        elif model_type == 'euc':\n",
    "            manifold = None\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            model = HYP_MLP(input_size=train_X.shape[1],\n",
    "                            output_size=train_y.shape[1],\n",
    "                            layer_size=layer_size,\n",
    "                            num_hidden_layers=num_hidden_layers,\n",
    "                            manifold=manifold).to(device)\n",
    "        elif model_type == 'euc':\n",
    "            model = EUC_MLP(input_size=train_X.shape[1],\n",
    "                            output_size=train_y.shape[1],\n",
    "                            layer_size=layer_size,\n",
    "                            num_hidden_layers=num_hidden_layers).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            optimizer = RiemannianAdam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif model_type == 'euc':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if model_type == 'hyp':\n",
    "                eval_stats['loss']['train'].append(hyp_train_model(model, train_loader, criterion, optimizer, manifold, device))\n",
    "                eval_stats['loss']['val'].append(util.h_evaluate_loss(model, val_loader, criterion, manifold, device))\n",
    "\n",
    "                eval_stats['mae']['train'].append(util.h_evaluate_r2(model, train_loader, manifold, device))\n",
    "                eval_stats['mae']['val'].append(util.h_evaluate_r2(model, val_loader, manifold, device))\n",
    "            elif model_type == 'euc':\n",
    "                eval_stats['loss']['train'].append(euc_train_model(model, train_loader, criterion, optimizer, device))\n",
    "                eval_stats['loss']['val'].append(util.evaluate_loss(model, val_loader, criterion, device))\n",
    "\n",
    "                eval_stats['mae']['train'].append(util.evaluate_r2(model, train_loader, device))\n",
    "                eval_stats['mae']['val'].append(util.evaluate_r2(model, val_loader, device))\n",
    "\n",
    "        print(eval_stats['mae']['val'])\n",
    "        param_eval_stats.append(eval_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BEST MODEL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'model_type': 'hyp',\n",
    "    'num_hidden_layers': 4,\n",
    "    'layer_size': 256,\n",
    "    'lr': 0.003,\n",
    "    'weight_decay': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 10,\n",
    "    'curvature': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[0.015503318091556252, 0.046809887485494105, 0.06806168484875695, 0.08266557620391923, 0.09373429069746794, 0.10036593844714145, 0.10486519088700184, 0.1087505170615147, 0.11201089459243274, 0.11489738420749455]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_eval_stats = []\n",
    "\n",
    "model_type, num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs, curvature = best_params.values()\n",
    "\n",
    "for fold in [0]:\n",
    "    print(f'Fold {fold}')\n",
    "\n",
    "    fold_train_X = train_X\n",
    "    fold_train_y = train_y\n",
    "    fold_val_X   = test_X\n",
    "    fold_val_y   = test_y\n",
    "\n",
    "    train_dataset = CustomDataset(fold_train_X, fold_train_y)\n",
    "    val_dataset = CustomDataset(fold_val_X, fold_val_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    if model_type == 'hyp':\n",
    "        manifold = PoincareBall(c=Curvature(curvature))\n",
    "    elif model_type == 'euc':\n",
    "        manifold = None\n",
    "\n",
    "    if model_type == 'hyp':\n",
    "        model = HYP_MLP(input_size=train_X.shape[1],\n",
    "                        output_size=train_y.shape[1],\n",
    "                        layer_size=layer_size,\n",
    "                        num_hidden_layers=num_hidden_layers,\n",
    "                        manifold=manifold).to(device)\n",
    "    elif model_type == 'euc':\n",
    "        model = EUC_MLP(input_size=train_X.shape[1],\n",
    "                        output_size=train_y.shape[1],\n",
    "                        layer_size=layer_size,\n",
    "                        num_hidden_layers=num_hidden_layers).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if model_type == 'hyp':\n",
    "        optimizer = RiemannianAdam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif model_type == 'euc':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if model_type == 'hyp':\n",
    "            eval_stats['loss']['train'].append(hyp_train_model(model, train_loader, criterion, optimizer, manifold, device))\n",
    "            eval_stats['loss']['val'].append(util.h_evaluate_loss(model, val_loader, criterion, manifold, device))\n",
    "\n",
    "            eval_stats['mae']['train'].append(util.h_evaluate_r2(model, train_loader, manifold, device))\n",
    "            eval_stats['mae']['val'].append(util.h_evaluate_r2(model, val_loader, manifold, device))\n",
    "        elif model_type == 'euc':\n",
    "            eval_stats['loss']['train'].append(euc_train_model(model, train_loader, criterion, optimizer, device))\n",
    "            eval_stats['loss']['val'].append(util.evaluate_loss(model, val_loader, criterion, device))\n",
    "\n",
    "            eval_stats['mae']['train'].append(util.evaluate_r2(model, train_loader, device))\n",
    "            eval_stats['mae']['val'].append(util.evaluate_r2(model, val_loader, device))\n",
    "\n",
    "    print(eval_stats['mae']['val'])\n",
    "    param_eval_stats.append(eval_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
