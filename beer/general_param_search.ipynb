{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from hypll import nn as hnn\n",
    "from hypll.tensors import TangentTensor\n",
    "from hypll.optim import RiemannianAdam\n",
    "from hypll.manifolds.poincare_ball import Curvature, PoincareBall\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1750, 231), (1750, 50), (175, 231), (175, 50), (75, 231), (75, 50))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_X = pd.read_csv('../data/beer_features_train_samples_small.csv', index_col=0)\n",
    "df_train_y = pd.read_csv('../data/beer_labels_panel_train_samples_small.csv', index_col=0)\n",
    "df_val_X = pd.read_csv('../data/beer_features_train.csv', index_col=0)\n",
    "df_val_y = pd.read_csv('../data/beer_labels_panel_train.csv', index_col=0)\n",
    "df_test_X = pd.read_csv('../data/beer_features_test.csv', index_col=0)\n",
    "df_test_y = pd.read_csv('../data/beer_labels_panel_test.csv', index_col=0)\n",
    "\n",
    "train_X = df_train_X.values\n",
    "train_y = df_train_y.values\n",
    "val_X = df_val_X.values\n",
    "val_y = df_val_y.values\n",
    "test_X = df_test_X.values\n",
    "test_y = df_test_y.values\n",
    "\n",
    "train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_malt_all</th>\n",
       "      <th>A_malt_grain</th>\n",
       "      <th>A_malt_bread</th>\n",
       "      <th>A_malt_cara</th>\n",
       "      <th>A_malt_burn</th>\n",
       "      <th>A_hops_all</th>\n",
       "      <th>A_hops_citrus</th>\n",
       "      <th>A_hops_tropical</th>\n",
       "      <th>A_hops_noble</th>\n",
       "      <th>A_hops_woody</th>\n",
       "      <th>...</th>\n",
       "      <th>coriander</th>\n",
       "      <th>clove</th>\n",
       "      <th>lactic</th>\n",
       "      <th>acetic</th>\n",
       "      <th>barnyard</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>aftertaste</th>\n",
       "      <th>body</th>\n",
       "      <th>co2</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.941996</td>\n",
       "      <td>-1.075726</td>\n",
       "      <td>-0.285841</td>\n",
       "      <td>-0.178741</td>\n",
       "      <td>-0.604294</td>\n",
       "      <td>-0.656378</td>\n",
       "      <td>0.099024</td>\n",
       "      <td>0.241602</td>\n",
       "      <td>-1.268708</td>\n",
       "      <td>-0.956593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348461</td>\n",
       "      <td>-0.286272</td>\n",
       "      <td>-0.443933</td>\n",
       "      <td>-0.369483</td>\n",
       "      <td>-0.281909</td>\n",
       "      <td>1.091877</td>\n",
       "      <td>0.162093</td>\n",
       "      <td>0.425791</td>\n",
       "      <td>0.425166</td>\n",
       "      <td>0.710623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.893153</td>\n",
       "      <td>-0.977069</td>\n",
       "      <td>-0.195571</td>\n",
       "      <td>-0.179653</td>\n",
       "      <td>-0.543785</td>\n",
       "      <td>-0.639002</td>\n",
       "      <td>0.101024</td>\n",
       "      <td>0.256538</td>\n",
       "      <td>-1.409950</td>\n",
       "      <td>-0.906288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.321335</td>\n",
       "      <td>-0.321205</td>\n",
       "      <td>-0.399579</td>\n",
       "      <td>-0.355341</td>\n",
       "      <td>-0.291434</td>\n",
       "      <td>1.023505</td>\n",
       "      <td>0.163216</td>\n",
       "      <td>0.361180</td>\n",
       "      <td>0.390275</td>\n",
       "      <td>0.744412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.880786</td>\n",
       "      <td>-1.137411</td>\n",
       "      <td>-0.336594</td>\n",
       "      <td>-0.230738</td>\n",
       "      <td>-0.601020</td>\n",
       "      <td>-0.634813</td>\n",
       "      <td>0.015769</td>\n",
       "      <td>0.156042</td>\n",
       "      <td>-1.255992</td>\n",
       "      <td>-0.990153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.310829</td>\n",
       "      <td>-0.351813</td>\n",
       "      <td>-0.351749</td>\n",
       "      <td>-0.403402</td>\n",
       "      <td>-0.345381</td>\n",
       "      <td>1.085524</td>\n",
       "      <td>-0.024607</td>\n",
       "      <td>0.490808</td>\n",
       "      <td>0.432825</td>\n",
       "      <td>0.715099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.879230</td>\n",
       "      <td>-1.071754</td>\n",
       "      <td>-0.219354</td>\n",
       "      <td>-0.193606</td>\n",
       "      <td>-0.624828</td>\n",
       "      <td>-0.643238</td>\n",
       "      <td>0.045454</td>\n",
       "      <td>0.138254</td>\n",
       "      <td>-1.286502</td>\n",
       "      <td>-0.981138</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393938</td>\n",
       "      <td>-0.283200</td>\n",
       "      <td>-0.350663</td>\n",
       "      <td>-0.354406</td>\n",
       "      <td>-0.254126</td>\n",
       "      <td>0.973770</td>\n",
       "      <td>-0.046796</td>\n",
       "      <td>0.374827</td>\n",
       "      <td>0.449950</td>\n",
       "      <td>0.686550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.875527</td>\n",
       "      <td>-1.063935</td>\n",
       "      <td>-0.205753</td>\n",
       "      <td>-0.175031</td>\n",
       "      <td>-0.584146</td>\n",
       "      <td>-0.621780</td>\n",
       "      <td>0.057810</td>\n",
       "      <td>0.077312</td>\n",
       "      <td>-1.368296</td>\n",
       "      <td>-0.878475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348944</td>\n",
       "      <td>-0.363667</td>\n",
       "      <td>-0.395459</td>\n",
       "      <td>-0.270739</td>\n",
       "      <td>-0.279900</td>\n",
       "      <td>0.943100</td>\n",
       "      <td>-0.024269</td>\n",
       "      <td>0.404060</td>\n",
       "      <td>0.364624</td>\n",
       "      <td>0.793018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>-1.233758</td>\n",
       "      <td>-0.868750</td>\n",
       "      <td>-0.638104</td>\n",
       "      <td>-0.875494</td>\n",
       "      <td>-0.278827</td>\n",
       "      <td>0.173468</td>\n",
       "      <td>1.801565</td>\n",
       "      <td>0.154312</td>\n",
       "      <td>-0.052745</td>\n",
       "      <td>-0.926968</td>\n",
       "      <td>...</td>\n",
       "      <td>1.335555</td>\n",
       "      <td>0.253309</td>\n",
       "      <td>-0.447637</td>\n",
       "      <td>-0.364690</td>\n",
       "      <td>-0.689424</td>\n",
       "      <td>-0.543003</td>\n",
       "      <td>-1.372243</td>\n",
       "      <td>-1.187967</td>\n",
       "      <td>0.816348</td>\n",
       "      <td>0.139063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>-1.063261</td>\n",
       "      <td>-0.751914</td>\n",
       "      <td>-0.690999</td>\n",
       "      <td>-0.924281</td>\n",
       "      <td>-0.199706</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>1.762356</td>\n",
       "      <td>0.196654</td>\n",
       "      <td>-0.057584</td>\n",
       "      <td>-0.970705</td>\n",
       "      <td>...</td>\n",
       "      <td>1.251194</td>\n",
       "      <td>0.250426</td>\n",
       "      <td>-0.419021</td>\n",
       "      <td>-0.379580</td>\n",
       "      <td>-0.608952</td>\n",
       "      <td>-0.622449</td>\n",
       "      <td>-1.400643</td>\n",
       "      <td>-1.032351</td>\n",
       "      <td>0.763097</td>\n",
       "      <td>0.176882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>-1.153341</td>\n",
       "      <td>-0.802079</td>\n",
       "      <td>-0.600642</td>\n",
       "      <td>-0.885723</td>\n",
       "      <td>-0.224749</td>\n",
       "      <td>0.186860</td>\n",
       "      <td>1.835002</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>-0.184398</td>\n",
       "      <td>-0.911978</td>\n",
       "      <td>...</td>\n",
       "      <td>1.219650</td>\n",
       "      <td>0.254567</td>\n",
       "      <td>-0.426003</td>\n",
       "      <td>-0.321299</td>\n",
       "      <td>-0.669511</td>\n",
       "      <td>-0.423319</td>\n",
       "      <td>-1.300411</td>\n",
       "      <td>-0.964460</td>\n",
       "      <td>0.593767</td>\n",
       "      <td>0.137067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>-1.203214</td>\n",
       "      <td>-0.841736</td>\n",
       "      <td>-0.702683</td>\n",
       "      <td>-0.924173</td>\n",
       "      <td>-0.245043</td>\n",
       "      <td>0.189425</td>\n",
       "      <td>1.795088</td>\n",
       "      <td>0.202355</td>\n",
       "      <td>-0.090596</td>\n",
       "      <td>-0.851933</td>\n",
       "      <td>...</td>\n",
       "      <td>1.240388</td>\n",
       "      <td>0.291119</td>\n",
       "      <td>-0.420381</td>\n",
       "      <td>-0.401480</td>\n",
       "      <td>-0.575356</td>\n",
       "      <td>-0.562966</td>\n",
       "      <td>-1.437035</td>\n",
       "      <td>-1.153961</td>\n",
       "      <td>0.783051</td>\n",
       "      <td>0.162953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>-1.195146</td>\n",
       "      <td>-0.865512</td>\n",
       "      <td>-0.613141</td>\n",
       "      <td>-0.913100</td>\n",
       "      <td>-0.202405</td>\n",
       "      <td>0.153794</td>\n",
       "      <td>1.711885</td>\n",
       "      <td>0.199160</td>\n",
       "      <td>-0.156965</td>\n",
       "      <td>-0.914155</td>\n",
       "      <td>...</td>\n",
       "      <td>1.214910</td>\n",
       "      <td>0.230568</td>\n",
       "      <td>-0.434051</td>\n",
       "      <td>-0.320287</td>\n",
       "      <td>-0.678902</td>\n",
       "      <td>-0.582467</td>\n",
       "      <td>-1.391285</td>\n",
       "      <td>-1.001023</td>\n",
       "      <td>0.748031</td>\n",
       "      <td>0.176554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1750 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A_malt_all  A_malt_grain  A_malt_bread  A_malt_cara  A_malt_burn  \\\n",
       "0      -0.941996     -1.075726     -0.285841    -0.178741    -0.604294   \n",
       "1      -0.893153     -0.977069     -0.195571    -0.179653    -0.543785   \n",
       "2      -0.880786     -1.137411     -0.336594    -0.230738    -0.601020   \n",
       "3      -0.879230     -1.071754     -0.219354    -0.193606    -0.624828   \n",
       "4      -0.875527     -1.063935     -0.205753    -0.175031    -0.584146   \n",
       "...          ...           ...           ...          ...          ...   \n",
       "1745   -1.233758     -0.868750     -0.638104    -0.875494    -0.278827   \n",
       "1746   -1.063261     -0.751914     -0.690999    -0.924281    -0.199706   \n",
       "1747   -1.153341     -0.802079     -0.600642    -0.885723    -0.224749   \n",
       "1748   -1.203214     -0.841736     -0.702683    -0.924173    -0.245043   \n",
       "1749   -1.195146     -0.865512     -0.613141    -0.913100    -0.202405   \n",
       "\n",
       "      A_hops_all  A_hops_citrus  A_hops_tropical  A_hops_noble  A_hops_woody  \\\n",
       "0      -0.656378       0.099024         0.241602     -1.268708     -0.956593   \n",
       "1      -0.639002       0.101024         0.256538     -1.409950     -0.906288   \n",
       "2      -0.634813       0.015769         0.156042     -1.255992     -0.990153   \n",
       "3      -0.643238       0.045454         0.138254     -1.286502     -0.981138   \n",
       "4      -0.621780       0.057810         0.077312     -1.368296     -0.878475   \n",
       "...          ...            ...              ...           ...           ...   \n",
       "1745    0.173468       1.801565         0.154312     -0.052745     -0.926968   \n",
       "1746    0.088800       1.762356         0.196654     -0.057584     -0.970705   \n",
       "1747    0.186860       1.835002         0.190688     -0.184398     -0.911978   \n",
       "1748    0.189425       1.795088         0.202355     -0.090596     -0.851933   \n",
       "1749    0.153794       1.711885         0.199160     -0.156965     -0.914155   \n",
       "\n",
       "      ...  coriander     clove    lactic    acetic  barnyard   alcohol  \\\n",
       "0     ...  -0.348461 -0.286272 -0.443933 -0.369483 -0.281909  1.091877   \n",
       "1     ...  -0.321335 -0.321205 -0.399579 -0.355341 -0.291434  1.023505   \n",
       "2     ...  -0.310829 -0.351813 -0.351749 -0.403402 -0.345381  1.085524   \n",
       "3     ...  -0.393938 -0.283200 -0.350663 -0.354406 -0.254126  0.973770   \n",
       "4     ...  -0.348944 -0.363667 -0.395459 -0.270739 -0.279900  0.943100   \n",
       "...   ...        ...       ...       ...       ...       ...       ...   \n",
       "1745  ...   1.335555  0.253309 -0.447637 -0.364690 -0.689424 -0.543003   \n",
       "1746  ...   1.251194  0.250426 -0.419021 -0.379580 -0.608952 -0.622449   \n",
       "1747  ...   1.219650  0.254567 -0.426003 -0.321299 -0.669511 -0.423319   \n",
       "1748  ...   1.240388  0.291119 -0.420381 -0.401480 -0.575356 -0.562966   \n",
       "1749  ...   1.214910  0.230568 -0.434051 -0.320287 -0.678902 -0.582467   \n",
       "\n",
       "      aftertaste      body       co2   overall  \n",
       "0       0.162093  0.425791  0.425166  0.710623  \n",
       "1       0.163216  0.361180  0.390275  0.744412  \n",
       "2      -0.024607  0.490808  0.432825  0.715099  \n",
       "3      -0.046796  0.374827  0.449950  0.686550  \n",
       "4      -0.024269  0.404060  0.364624  0.793018  \n",
       "...          ...       ...       ...       ...  \n",
       "1745   -1.372243 -1.187967  0.816348  0.139063  \n",
       "1746   -1.400643 -1.032351  0.763097  0.176882  \n",
       "1747   -1.300411 -0.964460  0.593767  0.137067  \n",
       "1748   -1.437035 -1.153961  0.783051  0.162953  \n",
       "1749   -1.391285 -1.001023  0.748031  0.176554  \n",
       "\n",
       "[1750 rows x 50 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   0,    1,    2, ..., 1747, 1748, 1749]),\n",
       " array([   0,    1,    2, ..., 1747, 1748, 1749]),\n",
       " array([  10,   11,   12, ..., 1747, 1748, 1749]),\n",
       " array([   0,    1,    2, ..., 1737, 1738, 1739]),\n",
       " array([   0,    1,    2, ..., 1747, 1748, 1749])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "NUM_SAMPLE_TYPES = len(val_X)\n",
    "NUM_SAMPLES_PER_TYPE = len(train_X) // NUM_SAMPLE_TYPES\n",
    "\n",
    "fold_nums = list(range(FOLDS))\n",
    "[num*NUM_SAMPLE_TYPES for num in fold_nums]\n",
    "[(num+1)*NUM_SAMPLE_TYPES for num in fold_nums]\n",
    "\n",
    "# FOLD_INDICES = util.get_fold_indices(NUM_SAMPLE_TYPES, FOLDS)\n",
    "\n",
    "# def get_fold_indices(num_types, num_per_type, k, seed=42):\n",
    "#     def get_val_start_ends(size, k):\n",
    "#         fold_size = size // k\n",
    "#         rest = size % k\n",
    "\n",
    "#         fold_sizes = [fold_size] * k\n",
    "\n",
    "#         for i in range(rest):\n",
    "#             fold_sizes[i] += 1\n",
    "\n",
    "#         indices = np.cumsum([fold_sizes])\n",
    "\n",
    "#         return list(zip(indices-np.array(fold_sizes), indices))\n",
    "\n",
    "\n",
    "#     np.random.seed(seed)\n",
    "#     indices = np.random.random(num_types).argsort()\n",
    "\n",
    "#     val_start_ends = get_val_start_ends(num_types, k)\n",
    "#     val_indices = [indices[start:end] for start, end in val_start_ends]\n",
    "\n",
    "#     train_indices = [list(set(range(num_types)) - set(val_is)) for val_is in val_indices]\n",
    "#     exp_train_indices = [[list(range(val_i*num_per_type,(val_i+1)*num_per_type)) for val_i in val_is] for val_is in train_indices]\n",
    "\n",
    "#     return np.array(val_indices), [np.array(exp_is).flatten() for exp_is in exp_train_indices]\n",
    "\n",
    "val_indices, train_indices = util.get_fold_indices_rand(NUM_SAMPLE_TYPES, NUM_SAMPLES_PER_TYPE, FOLDS)\n",
    "train_indices\n",
    "# print(FOLD_INDICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom PyTorch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Hyperbolic </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP model\n",
    "class HYP_MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers, manifold):\n",
    "        super(HYP_MLP, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.fc_in = hnn.HLinear(input_size, layer_size, manifold=manifold)\n",
    "        self.relu = hnn.HReLU(manifold=manifold)\n",
    "        self.hidden_fcs = nn.ModuleList([hnn.HLinear(layer_size, layer_size, manifold=manifold) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = hnn.HLinear(layer_size, output_size, manifold=manifold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define training function\n",
    "def hyp_train_model(model, train_loader, criterion, optimizer, manifold, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tangents = TangentTensor(data=inputs, man_dim=-1, manifold=manifold)\n",
    "        manifold_inputs = manifold.expmap(tangents)\n",
    "\n",
    "        outputs = model(manifold_inputs)\n",
    "\n",
    "        loss = criterion(outputs.tensor, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> EUCLIDEAN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP model\n",
    "class EUC_MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers):\n",
    "        super(EUC_MLP, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.fc_in = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_fcs = nn.ModuleList([nn.Linear(layer_size, layer_size) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define training function\n",
    "def euc_train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'model_type': ['hyp', 'euc'],\n",
    "    'num_hidden_layers': [0,1,2,4,8],\n",
    "    'layer_size': [32,64,128,256],\n",
    "    'lr': [0.02],\n",
    "    'weight_decay': [0.005],\n",
    "    'batch_size': [1024],\n",
    "    'epochs': [100],\n",
    "    'curvature': [-1]\n",
    "}\n",
    "\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "len(param_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Combination 0 -----\n",
      "('model_type', 'hyp') ('num_hidden_layers', 0) ('layer_size', 32) ('lr', 0.02) ('weight_decay', 0.005) ('batch_size', 1024) ('epochs', 100) ('curvature', -1)\n",
      "Fold 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 61\u001b[0m         eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mhyp_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanifold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     62\u001b[0m         eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(util\u001b[38;5;241m.\u001b[39mh_evaluate_loss(model, val_loader, criterion, manifold, device))\n\u001b[0;32m     64\u001b[0m         eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(util\u001b[38;5;241m.\u001b[39mh_evaluate_r2(model, train_loader, manifold, device))\n",
      "Cell \u001b[1;32mIn[97], line 26\u001b[0m, in \u001b[0;36mhyp_train_model\u001b[1;34m(model, train_loader, criterion, optimizer, manifold, device)\u001b[0m\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     25\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roryd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\roryd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\roryd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[96], line 10\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_eval_stats = []\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f'----- Combination {i} -----')\n",
    "    print(*zip(param_grid.keys(), params))\n",
    "    model_type, num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs, curvature = params\n",
    "\n",
    "    # for fold, (fold_train_indices, fold_val_indices) in enumerate(zip(train_indices, val_indices)):\n",
    "    #     print(f'Fold {fold}')\n",
    "\n",
    "    #     fold_train_X = train_X[fold_train_indices]\n",
    "    #     fold_train_y = train_y[fold_train_indices]\n",
    "    #     fold_val_X   = val_X[fold_val_indices]\n",
    "    #     fold_val_y   = val_y[fold_val_indices]\n",
    "    #     # fold_val_X   = test_X\n",
    "    #     # fold_val_y   = test_y\n",
    "        \n",
    "    for fold in [0]:\n",
    "        print(f'Fold {fold}')\n",
    "\n",
    "        fold_train_X = train_X\n",
    "        fold_train_y = train_y\n",
    "        fold_val_X   = test_X\n",
    "        fold_val_y   = test_y\n",
    "\n",
    "        train_dataset = CustomDataset(fold_train_X, fold_train_y)\n",
    "        val_dataset = CustomDataset(fold_val_X, fold_val_y)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            manifold = PoincareBall(c=Curvature(curvature))\n",
    "        elif model_type == 'euc':\n",
    "            manifold = None\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            model = HYP_MLP(input_size=train_X.shape[1],\n",
    "                            output_size=train_y.shape[1],\n",
    "                            layer_size=layer_size,\n",
    "                            num_hidden_layers=num_hidden_layers,\n",
    "                            manifold=manifold).to(device)\n",
    "        elif model_type == 'euc':\n",
    "            model = EUC_MLP(input_size=train_X.shape[1],\n",
    "                            output_size=train_y.shape[1],\n",
    "                            layer_size=layer_size,\n",
    "                            num_hidden_layers=num_hidden_layers).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            optimizer = RiemannianAdam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif model_type == 'euc':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if model_type == 'hyp':\n",
    "                eval_stats['loss']['train'].append(hyp_train_model(model, train_loader, criterion, optimizer, manifold, device))\n",
    "                eval_stats['loss']['val'].append(util.h_evaluate_loss(model, val_loader, criterion, manifold, device))\n",
    "\n",
    "                eval_stats['mae']['train'].append(util.h_evaluate_r2(model, train_loader, manifold, device))\n",
    "                eval_stats['mae']['val'].append(util.h_evaluate_r2(model, val_loader, manifold, device))\n",
    "            elif model_type == 'euc':\n",
    "                eval_stats['loss']['train'].append(euc_train_model(model, train_loader, criterion, optimizer, device))\n",
    "                eval_stats['loss']['val'].append(util.evaluate_loss(model, val_loader, criterion, device))\n",
    "\n",
    "                eval_stats['mae']['train'].append(util.evaluate_r2(model, train_loader, device))\n",
    "                eval_stats['mae']['val'].append(util.evaluate_r2(model, val_loader, device))\n",
    "\n",
    "        print(eval_stats['mae']['val'])\n",
    "        param_eval_stats.append(eval_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BEST MODEL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'model_type': 'hyp',\n",
    "    'num_hidden_layers': 4,\n",
    "    'layer_size': 256,\n",
    "    'lr': 0.003,\n",
    "    'weight_decay': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 10,\n",
    "    'curvature': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[0.10765948756282011, 0.12308321606784109, 0.12454766437259072, 0.12698461687460463, 0.12526742669738572, 0.12486854942081814, 0.12370138416228064, 0.12335186350024209, 0.12311975745553928, 0.12481188844227081]\n",
      "Fold 1\n",
      "[0.10694572659119995, 0.12300988018483655, 0.12596251597252395, 0.12713697133334384, 0.12319071662434075, 0.1237505160267185, 0.12400483580433477, 0.12384196969943467, 0.1234383582915921, 0.12249493166258585]\n",
      "Fold 2\n",
      "[0.10687856100840314, 0.12332382928659225, 0.12391855430801797, 0.12579272453833407, 0.12444610385627473, 0.12367803361043006, 0.12372331089353018, 0.1233711942192049, 0.1228020182704326, 0.12271869834607192]\n",
      "Fold 3\n",
      "[0.10750576340391302, 0.12294992224508638, 0.12448672795407086, 0.12575785160685407, 0.12344712346292308, 0.12408783516163593, 0.12411291205089521, 0.12396073754233594, 0.12467454068302844, 0.1239832776480632]\n",
      "Fold 4\n",
      "[0.106911679887455, 0.12443725528270613, 0.1259176711359262, 0.12665070801147957, 0.12356156026727742, 0.12352775207013689, 0.12360777193862876, 0.12350345822525063, 0.12307396887222734, 0.12265171284640589]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_type, num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs, curvature = best_params.values()\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_X, train_y)):\n",
    "    print(f'Fold {fold}')\n",
    "    # fold_train_X, fold_val_X = train_X[train_idx], train_X[val_idx]\n",
    "    # fold_train_y, fold_val_y = train_y[train_idx], train_y[val_idx]\n",
    "    fold_train_X, fold_val_X = train_X[train_idx], test_X\n",
    "    fold_train_y, fold_val_y = train_y[train_idx], test_y\n",
    "    # fold_train_X, fold_val_X = train_X, test_X\n",
    "    # fold_train_y, fold_val_y = train_y, test_y\n",
    "\n",
    "    train_dataset = CustomDataset(fold_train_X, fold_train_y)\n",
    "    val_dataset = CustomDataset(fold_val_X, fold_val_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    if model_type == 'hyp':\n",
    "        manifold = PoincareBall(c=Curvature(curvature))\n",
    "    elif model_type == 'euc':\n",
    "        manifold = None\n",
    "\n",
    "    if model_type == 'hyp':\n",
    "        model = HYP_MLP(input_size=train_X.shape[1],\n",
    "                        output_size=train_y.shape[1],\n",
    "                        layer_size=layer_size,\n",
    "                        num_hidden_layers=num_hidden_layers,\n",
    "                        manifold=manifold).to(device)\n",
    "    elif model_type == 'euc':\n",
    "        model = EUC_MLP(input_size=train_X.shape[1],\n",
    "                        output_size=train_y.shape[1],\n",
    "                        layer_size=layer_size,\n",
    "                        num_hidden_layers=num_hidden_layers).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if model_type == 'hyp':\n",
    "        optimizer = RiemannianAdam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif model_type == 'euc':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if model_type == 'hyp':\n",
    "            eval_stats['loss']['train'].append(hyp_train_model(model, train_loader, criterion, optimizer, manifold, device))\n",
    "            eval_stats['loss']['val'].append(util.h_evaluate_loss(model, val_loader, criterion, manifold, device))\n",
    "\n",
    "            eval_stats['mae']['train'].append(util.h_evaluate_r2(model, train_loader, manifold, device))\n",
    "            eval_stats['mae']['val'].append(util.h_evaluate_r2(model, val_loader, manifold, device))\n",
    "        elif model_type == 'euc':\n",
    "            eval_stats['loss']['train'].append(euc_train_model(model, train_loader, criterion, optimizer, device))\n",
    "            eval_stats['loss']['val'].append(util.evaluate_loss(model, val_loader, criterion, device))\n",
    "\n",
    "            eval_stats['mae']['train'].append(util.evaluate_r2(model, train_loader, device))\n",
    "            eval_stats['mae']['val'].append(util.evaluate_r2(model, val_loader, device))\n",
    "\n",
    "    print(eval_stats['mae']['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000\n",
    "[0.09094981766612582, 0.09608602443769439, 0.10213084733073284, 0.10297098476158782, 0.1039252772204806, 0.10574258559121522, 0.10593447081943014, 0.1103528270397624, 0.10905810868205902, 0.11674655082317638]\n",
    "[0.09124620536624219, 0.09588467278224783, 0.1109519321222205, 0.10871296762480671, 0.11322182908185724, 0.10604926039894355, 0.11641335699831064, 0.11669891168893905, 0.1163291967061507, 0.11563811270315874]\n",
    "[0.08261279344412803, 0.0949209955128152, 0.10394174902203669, 0.11074634671414854, 0.10959598998285282, 0.10492545479477039, 0.10590842284461503, 0.10538532105083259, 0.10268829907521582, 0.10696282556494001]\n",
    "[0.09143749197812866, 0.10686854162056632, 0.10184344301563328, 0.10983260881492879, 0.11453995365690633, 0.10643641196149398, 0.10807662045371369, 0.11264002132608508, 0.11569218070241385, 0.11172063540914148]\n",
    "[0.08755047684265042, 0.09826624201788955, 0.10408250848514021, 0.10678387971099373, 0.10168236861206509, 0.09372892354517087, 0.10054121141766975, 0.11307510662137904, 0.10818904212215745, 0.11473416400049585]\n",
    "\n",
    "100\n",
    "[0.10712686537621345, 0.12349026400269329, 0.1246606174341069, 0.12693350344457163, 0.12540473804914373, 0.12392351307841984, 0.12310184680085018, 0.1235122503852268, 0.12269077053490243, 0.12184334164640719]\n",
    "[0.10825157818713378, 0.12289771182725055, 0.12439158651151372, 0.1261395813343429, 0.1253862140092597, 0.12366027630753869, 0.12406626861964988, 0.1242455269123071, 0.12363573750877084, 0.12229140361532519]\n",
    "[0.10685337929854738, 0.12412607759674942, 0.12566753200057623, 0.12656694272727384, 0.1249030203813584, 0.12408679288839279, 0.12404666634192532, 0.12445838543532435, 0.12457812238461687, 0.12304167921955317]\n",
    "[0.10745761389567116, 0.12397573098456129, 0.12521314400651082, 0.1276813410290952, 0.1252595522840151, 0.12429336231380055, 0.12474528515336958, 0.1244248228708727, 0.12511605799175451, 0.12342070994183856]\n",
    "[0.10759642672103734, 0.12380378206783893, 0.12494015763741242, 0.12673535494694677, 0.12506894028324306, 0.12376698712522265, 0.12254735888244327, 0.12283803721108438, 0.12404691001407472, 0.12351575220797617]\n",
    "\n",
    "50\n",
    "[0.07677710553661399, 0.10584953169202371, 0.11966844890984371, 0.12411849676234876, 0.12520028446868986, 0.12418479259928074, 0.12732371128638775, 0.126271569310595, 0.12580454850610243, 0.12429738209045665]\n",
    "[0.07721830195010819, 0.10745842540642501, 0.12112070383855837, 0.12386530838495947, 0.125923156127822, 0.1258847214082912, 0.12830232653117135, 0.12669207756182055, 0.12631119756542175, 0.12450957025993643]\n",
    "[0.07632709753632146, 0.10763185461397703, 0.1186022848413865, 0.12323234538214417, 0.12484191533525628, 0.12444830403759084, 0.12718327433134718, 0.12617910882856184, 0.12485011626791608, 0.1229076001872507]\n",
    "[0.0767599902123989, 0.1089768569863075, 0.120941318864213, 0.12583457699099382, 0.1255480130415434, 0.12394682506457874, 0.12700223229913882, 0.12678108092511747, 0.12549870300322666, 0.12525940753014855]\n",
    "[0.07607748019287443, 0.10721069870104394, 0.12062286165389509, 0.12380926861086566, 0.1258001163362215, 0.12521044913472032, 0.1273641772466992, 0.1263506895116415, 0.12492659096620544, 0.12464252907020315]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
