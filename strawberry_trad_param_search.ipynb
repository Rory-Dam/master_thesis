{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import consts\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OVERALL LIKING</th>\n",
       "      <th>TEXTURE LIKING</th>\n",
       "      <th>SWEETNESS INTENSITY</th>\n",
       "      <th>SOURNESS INTENSITY</th>\n",
       "      <th>STRAWBERRY FLAVOR INTENSITY</th>\n",
       "      <th>6915-15-7</th>\n",
       "      <th>77-92-9</th>\n",
       "      <th>50-99-7</th>\n",
       "      <th>57-48-7</th>\n",
       "      <th>57-50-1</th>\n",
       "      <th>...</th>\n",
       "      <th>7786-58-5</th>\n",
       "      <th>15111-96-3</th>\n",
       "      <th>706-14-9</th>\n",
       "      <th>10522-34-6</th>\n",
       "      <th>5881-17-4</th>\n",
       "      <th>128-37-0</th>\n",
       "      <th>40716-66-3</th>\n",
       "      <th>4887-30-3</th>\n",
       "      <th>5454-09-1</th>\n",
       "      <th>2305-05-7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.307068</td>\n",
       "      <td>0.250174</td>\n",
       "      <td>0.276647</td>\n",
       "      <td>0.146214</td>\n",
       "      <td>0.305021</td>\n",
       "      <td>-2.120421</td>\n",
       "      <td>0.171179</td>\n",
       "      <td>0.759180</td>\n",
       "      <td>0.612070</td>\n",
       "      <td>0.314374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271610</td>\n",
       "      <td>-0.443110</td>\n",
       "      <td>-0.544167</td>\n",
       "      <td>-0.463164</td>\n",
       "      <td>1.289539</td>\n",
       "      <td>-0.945803</td>\n",
       "      <td>0.122098</td>\n",
       "      <td>-0.127048</td>\n",
       "      <td>0.804970</td>\n",
       "      <td>-0.354773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.307859</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.276101</td>\n",
       "      <td>0.147151</td>\n",
       "      <td>0.306364</td>\n",
       "      <td>-2.119978</td>\n",
       "      <td>0.171282</td>\n",
       "      <td>0.759195</td>\n",
       "      <td>0.612046</td>\n",
       "      <td>0.314378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>-0.334642</td>\n",
       "      <td>-0.545464</td>\n",
       "      <td>-0.568225</td>\n",
       "      <td>1.346308</td>\n",
       "      <td>-0.893509</td>\n",
       "      <td>0.117186</td>\n",
       "      <td>-0.129998</td>\n",
       "      <td>0.800679</td>\n",
       "      <td>-0.374048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.306348</td>\n",
       "      <td>0.248756</td>\n",
       "      <td>0.277115</td>\n",
       "      <td>0.146568</td>\n",
       "      <td>0.306719</td>\n",
       "      <td>-2.120063</td>\n",
       "      <td>0.171283</td>\n",
       "      <td>0.759176</td>\n",
       "      <td>0.612056</td>\n",
       "      <td>0.314430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275587</td>\n",
       "      <td>-0.241669</td>\n",
       "      <td>-0.544872</td>\n",
       "      <td>-0.614111</td>\n",
       "      <td>1.283168</td>\n",
       "      <td>-0.997929</td>\n",
       "      <td>0.118824</td>\n",
       "      <td>-0.128236</td>\n",
       "      <td>0.819822</td>\n",
       "      <td>-0.357370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.307694</td>\n",
       "      <td>0.248227</td>\n",
       "      <td>0.277607</td>\n",
       "      <td>0.147135</td>\n",
       "      <td>0.305276</td>\n",
       "      <td>-2.120058</td>\n",
       "      <td>0.171249</td>\n",
       "      <td>0.759163</td>\n",
       "      <td>0.612060</td>\n",
       "      <td>0.314365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273644</td>\n",
       "      <td>-0.312912</td>\n",
       "      <td>-0.545357</td>\n",
       "      <td>-0.562826</td>\n",
       "      <td>1.343249</td>\n",
       "      <td>-0.959232</td>\n",
       "      <td>0.118201</td>\n",
       "      <td>-0.126724</td>\n",
       "      <td>0.811123</td>\n",
       "      <td>-0.377366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.308055</td>\n",
       "      <td>0.249070</td>\n",
       "      <td>0.276638</td>\n",
       "      <td>0.145692</td>\n",
       "      <td>0.305707</td>\n",
       "      <td>-2.120678</td>\n",
       "      <td>0.171089</td>\n",
       "      <td>0.759154</td>\n",
       "      <td>0.612069</td>\n",
       "      <td>0.314420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273092</td>\n",
       "      <td>-0.211367</td>\n",
       "      <td>-0.545153</td>\n",
       "      <td>-0.589017</td>\n",
       "      <td>1.548846</td>\n",
       "      <td>-0.901830</td>\n",
       "      <td>0.120083</td>\n",
       "      <td>-0.126749</td>\n",
       "      <td>0.807910</td>\n",
       "      <td>-0.384669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53995</th>\n",
       "      <td>0.237395</td>\n",
       "      <td>0.249785</td>\n",
       "      <td>0.205025</td>\n",
       "      <td>0.225081</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>-0.222374</td>\n",
       "      <td>0.884429</td>\n",
       "      <td>-0.834332</td>\n",
       "      <td>-0.803436</td>\n",
       "      <td>-0.625512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368228</td>\n",
       "      <td>-0.326611</td>\n",
       "      <td>-0.146165</td>\n",
       "      <td>-0.098112</td>\n",
       "      <td>0.032811</td>\n",
       "      <td>-0.408101</td>\n",
       "      <td>0.055410</td>\n",
       "      <td>-0.453151</td>\n",
       "      <td>-0.452553</td>\n",
       "      <td>-0.445544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53996</th>\n",
       "      <td>0.240068</td>\n",
       "      <td>0.250256</td>\n",
       "      <td>0.204477</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>0.257519</td>\n",
       "      <td>-0.221809</td>\n",
       "      <td>0.884286</td>\n",
       "      <td>-0.834295</td>\n",
       "      <td>-0.803437</td>\n",
       "      <td>-0.625499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.394705</td>\n",
       "      <td>-0.280702</td>\n",
       "      <td>-0.155369</td>\n",
       "      <td>-0.551456</td>\n",
       "      <td>-0.262542</td>\n",
       "      <td>-0.313059</td>\n",
       "      <td>0.052135</td>\n",
       "      <td>-0.480896</td>\n",
       "      <td>-0.518157</td>\n",
       "      <td>-0.462789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53997</th>\n",
       "      <td>0.238440</td>\n",
       "      <td>0.249851</td>\n",
       "      <td>0.204574</td>\n",
       "      <td>0.224211</td>\n",
       "      <td>0.257632</td>\n",
       "      <td>-0.222039</td>\n",
       "      <td>0.884583</td>\n",
       "      <td>-0.834322</td>\n",
       "      <td>-0.803455</td>\n",
       "      <td>-0.625512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390017</td>\n",
       "      <td>-0.316657</td>\n",
       "      <td>-0.159865</td>\n",
       "      <td>-0.542431</td>\n",
       "      <td>-0.189396</td>\n",
       "      <td>-0.153318</td>\n",
       "      <td>0.055373</td>\n",
       "      <td>-0.470802</td>\n",
       "      <td>-0.472719</td>\n",
       "      <td>-0.478549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53998</th>\n",
       "      <td>0.238959</td>\n",
       "      <td>0.250376</td>\n",
       "      <td>0.202495</td>\n",
       "      <td>0.225202</td>\n",
       "      <td>0.258081</td>\n",
       "      <td>-0.222588</td>\n",
       "      <td>0.884425</td>\n",
       "      <td>-0.834312</td>\n",
       "      <td>-0.803431</td>\n",
       "      <td>-0.625546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.372647</td>\n",
       "      <td>-0.229130</td>\n",
       "      <td>-0.157428</td>\n",
       "      <td>-0.625398</td>\n",
       "      <td>-0.242219</td>\n",
       "      <td>-0.471044</td>\n",
       "      <td>0.053125</td>\n",
       "      <td>-0.445828</td>\n",
       "      <td>-0.497520</td>\n",
       "      <td>-0.467094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53999</th>\n",
       "      <td>0.237641</td>\n",
       "      <td>0.250909</td>\n",
       "      <td>0.204326</td>\n",
       "      <td>0.225894</td>\n",
       "      <td>0.258426</td>\n",
       "      <td>-0.222090</td>\n",
       "      <td>0.884445</td>\n",
       "      <td>-0.834318</td>\n",
       "      <td>-0.803431</td>\n",
       "      <td>-0.625532</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379062</td>\n",
       "      <td>-0.670598</td>\n",
       "      <td>-0.160341</td>\n",
       "      <td>-0.515137</td>\n",
       "      <td>-0.225687</td>\n",
       "      <td>-0.381578</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>-0.463344</td>\n",
       "      <td>-0.495692</td>\n",
       "      <td>-0.447090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54000 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OVERALL LIKING  TEXTURE LIKING  SWEETNESS INTENSITY  \\\n",
       "0            0.307068        0.250174             0.276647   \n",
       "1            0.307859        0.249023             0.276101   \n",
       "2            0.306348        0.248756             0.277115   \n",
       "3            0.307694        0.248227             0.277607   \n",
       "4            0.308055        0.249070             0.276638   \n",
       "...               ...             ...                  ...   \n",
       "53995        0.237395        0.249785             0.205025   \n",
       "53996        0.240068        0.250256             0.204477   \n",
       "53997        0.238440        0.249851             0.204574   \n",
       "53998        0.238959        0.250376             0.202495   \n",
       "53999        0.237641        0.250909             0.204326   \n",
       "\n",
       "       SOURNESS INTENSITY  STRAWBERRY FLAVOR INTENSITY  6915-15-7   77-92-9  \\\n",
       "0                0.146214                     0.305021  -2.120421  0.171179   \n",
       "1                0.147151                     0.306364  -2.119978  0.171282   \n",
       "2                0.146568                     0.306719  -2.120063  0.171283   \n",
       "3                0.147135                     0.305276  -2.120058  0.171249   \n",
       "4                0.145692                     0.305707  -2.120678  0.171089   \n",
       "...                   ...                          ...        ...       ...   \n",
       "53995            0.225081                     0.258800  -0.222374  0.884429   \n",
       "53996            0.226804                     0.257519  -0.221809  0.884286   \n",
       "53997            0.224211                     0.257632  -0.222039  0.884583   \n",
       "53998            0.225202                     0.258081  -0.222588  0.884425   \n",
       "53999            0.225894                     0.258426  -0.222090  0.884445   \n",
       "\n",
       "        50-99-7   57-48-7   57-50-1  ...  7786-58-5   15111-96-3   706-14-9   \\\n",
       "0      0.759180  0.612070  0.314374  ...   -0.271610    -0.443110  -0.544167   \n",
       "1      0.759195  0.612046  0.314378  ...   -0.276743    -0.334642  -0.545464   \n",
       "2      0.759176  0.612056  0.314430  ...   -0.275587    -0.241669  -0.544872   \n",
       "3      0.759163  0.612060  0.314365  ...   -0.273644    -0.312912  -0.545357   \n",
       "4      0.759154  0.612069  0.314420  ...   -0.273092    -0.211367  -0.545153   \n",
       "...         ...       ...       ...  ...         ...          ...        ...   \n",
       "53995 -0.834332 -0.803436 -0.625512  ...   -0.368228    -0.326611  -0.146165   \n",
       "53996 -0.834295 -0.803437 -0.625499  ...   -0.394705    -0.280702  -0.155369   \n",
       "53997 -0.834322 -0.803455 -0.625512  ...   -0.390017    -0.316657  -0.159865   \n",
       "53998 -0.834312 -0.803431 -0.625546  ...   -0.372647    -0.229130  -0.157428   \n",
       "53999 -0.834318 -0.803431 -0.625532  ...   -0.379062    -0.670598  -0.160341   \n",
       "\n",
       "       10522-34-6   5881-17-4   128-37-0  40716-66-3   4887-30-3   5454-09-1   \\\n",
       "0        -0.463164    1.289539 -0.945803     0.122098   -0.127048    0.804970   \n",
       "1        -0.568225    1.346308 -0.893509     0.117186   -0.129998    0.800679   \n",
       "2        -0.614111    1.283168 -0.997929     0.118824   -0.128236    0.819822   \n",
       "3        -0.562826    1.343249 -0.959232     0.118201   -0.126724    0.811123   \n",
       "4        -0.589017    1.548846 -0.901830     0.120083   -0.126749    0.807910   \n",
       "...            ...         ...       ...          ...         ...         ...   \n",
       "53995    -0.098112    0.032811 -0.408101     0.055410   -0.453151   -0.452553   \n",
       "53996    -0.551456   -0.262542 -0.313059     0.052135   -0.480896   -0.518157   \n",
       "53997    -0.542431   -0.189396 -0.153318     0.055373   -0.470802   -0.472719   \n",
       "53998    -0.625398   -0.242219 -0.471044     0.053125   -0.445828   -0.497520   \n",
       "53999    -0.515137   -0.225687 -0.381578     0.054690   -0.463344   -0.495692   \n",
       "\n",
       "       2305-05-7  \n",
       "0      -0.354773  \n",
       "1      -0.374048  \n",
       "2      -0.357370  \n",
       "3      -0.377366  \n",
       "4      -0.384669  \n",
       "...          ...  \n",
       "53995  -0.445544  \n",
       "53996  -0.462789  \n",
       "53997  -0.478549  \n",
       "53998  -0.467094  \n",
       "53999  -0.447090  \n",
       "\n",
       "[54000 rows x 94 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/strawberry_samples_big.csv', index_col=0)\n",
    "val_data = pd.read_csv('data/strawberry_val_dataset.csv', index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OVERALL LIKING'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_cols = data.columns[[0,1,2,3,4]]\n",
    "# target_cols = data.columns[:5]\n",
    "target_cols = data.columns[[0]]\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['6915-15-7', '77-92-9', '50-99-7', '57-48-7', '57-50-1', 'SSC', 'pH',\n",
       "       'TA', '75-85-4 ', '616-25-1 ', '1629-58-9 ', '96-22-0 ', '110-62-3 ',\n",
       "       '1534-08-3 ', '105-37-3', '109-60-4 ', '623-42-7 ', '591-78-6 ',\n",
       "       '108-10-1 ', '1576-87-0 ', '1576-86-9 ', '623-43-8 ', '71-41-0',\n",
       "       '1576-95-0 ', '556-24-1 ', '589-38-8 ', '105-54-4 ', '66-25-1 ',\n",
       "       '123-86-4 ', '624-24-8 ', '29674-47-3 ', '96-04-8 ', '638-11-9 ',\n",
       "       '116-53-0 ', '7452-79-1 ', '6728-26-3 ', '928-95-0 ', '111-27-3 ',\n",
       "       '123-92-2 ', '624-41-9 ', '110-43-0', '2432-51-1 ', '105-66-8 ',\n",
       "       '539-82-2 ', '111-71-7 ', '628-63-7 ', '1191-16-8 ', '106-70-7 ',\n",
       "       '55514-48-2 ', '110-93-0 ', '109-21-7 ', '123-66-0 ', '124-13-0 ',\n",
       "       '142-92-7 ', '2497-18-9 ', '60415-61-4', '104-76-7 ', ' 2311-46-8 ',\n",
       "       '109-19-3 ', '2548-87-0 ', '540-18-1 ', '4077-47-8 ', '20664-46-4',\n",
       "       '821-55-6 ', '5989-33-3 ', '78-70-6 ', '124-19-6 ', '103-09-3',\n",
       "       '140-11-4 ', '2639-63-6 ', '53398-83-7 ', '106-32-1 ', '112-14-1 ',\n",
       "       '564-94-3 ', '3913-81-3 ', '134-20-3 ', '110-39-4 ', '110-38-3 ',\n",
       "       '29811-50-5 ', '7786-58-5 ', '15111-96-3 ', '706-14-9 ', '10522-34-6 ',\n",
       "       '5881-17-4 ', '128-37-0', '40716-66-3 ', '4887-30-3 ', '5454-09-1 ',\n",
       "       '2305-05-7'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = data.columns[5:]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 89)\n",
      "(54000, 1)\n"
     ]
    }
   ],
   "source": [
    "features = data[feature_cols].values\n",
    "labels = data[target_cols].values\n",
    "val_features = val_data[feature_cols].values\n",
    "val_labels = val_data[target_cols].values\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38000, 89), (38000, 1), (16, 89), (16, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLE_TYPES = 54\n",
    "NUM_SAMPLES_PER_TYPE = len(data) // NUM_SAMPLE_TYPES\n",
    "TEST_SIZE = 0.3\n",
    "train_X = features[int(NUM_SAMPLE_TYPES*TEST_SIZE)*NUM_SAMPLES_PER_TYPE:]\n",
    "train_y = labels[int(NUM_SAMPLE_TYPES*TEST_SIZE)*NUM_SAMPLES_PER_TYPE:]\n",
    "val_X = val_features[:int(NUM_SAMPLE_TYPES*TEST_SIZE)]\n",
    "val_y = val_labels[:int(NUM_SAMPLE_TYPES*TEST_SIZE)]\n",
    "\n",
    "train_X.shape, train_y.shape, val_X.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Hyperbolic </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define your MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.manual_seed(consts.TORCH_MANUAL_SEED)\n",
    "        self.fc_in = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_fcs = nn.ModuleList([nn.Linear(layer_size, layer_size) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define custom PyTorch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Define training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def do_prediction(model, inputs, device):\n",
    "    inputs = inputs.float().to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'num_hidden_layers': [0,1,2,3],\n",
    "              'layer_size': [64,96,128],\n",
    "              'lr': [0.001,0.005,0.01,0.02],\n",
    "              'weight_decay': [0.002],\n",
    "              'batch_size': [1024],\n",
    "              'epochs': [30]}\n",
    "\n",
    "\n",
    "# best_params_overall_liking = {'num_hidden_layers': [16],\n",
    "#             'layer_size': [72],\n",
    "#             'lr': [0.02],\n",
    "#             'weight_decay': [0.001],\n",
    "#             'batch_size': [1024],\n",
    "#             'epochs': [16]}\n",
    "\n",
    "\n",
    "# best_params_sweetness_intensity = {'num_hidden_layers': [14],\n",
    "#               'layer_size': [80],\n",
    "#               'lr': [0.02],\n",
    "#               'weight_decay': [0.002],\n",
    "#               'batch_size': [1024],\n",
    "#               'epochs': [14]}\n",
    "\n",
    "\n",
    "np.prod([len(p) for p in param_grid.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "param_combinations = list(itertools.product(*param_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Combination 0 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 64) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.26333028, 0.25628194, 0.24271828, 0.23054242, 0.21925269, 0.2121547, 0.20680499, 0.20302919, 0.20051013, 0.19845729, 0.19574016, 0.19364208, 0.19106588, 0.18874916, 0.18614066, 0.18368515, 0.18105733, 0.17786232, 0.1748302, 0.170998, 0.16731319, 0.16333514, 0.15950422, 0.15598729, 0.15231456, 0.14921585, 0.14591306, 0.14309736, 0.1400058, 0.1373903]\n",
      "----- Combination 1 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 64) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.25679207, 0.2206147, 0.1975273, 0.18340257, 0.17328727, 0.16349989, 0.1546338, 0.14639282, 0.138039, 0.13034005, 0.12398927, 0.118514284, 0.11330019, 0.10816621, 0.103591934, 0.099286035, 0.09577258, 0.091924764, 0.088433005, 0.085432164, 0.082570605, 0.08030765, 0.078371495, 0.07743876, 0.07657109, 0.07621021, 0.07586413, 0.07554822, 0.07522404, 0.07505883]\n",
      "----- Combination 2 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 64) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.1207191, 0.12609929, 0.12853695, 0.13072158, 0.13036886, 0.12930284, 0.12567401, 0.12053789, 0.11477897, 0.1088896, 0.10384821, 0.09938546, 0.09553218, 0.09176265, 0.08842026, 0.08558644, 0.0828263, 0.08026549, 0.0787962, 0.0774969, 0.07690595, 0.07610483, 0.075136565, 0.07503557, 0.07440965, 0.07467434, 0.074728295, 0.07434583, 0.07396875, 0.07395622]\n",
      "----- Combination 3 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 64) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.14564894, 0.07187734, 0.07425357, 0.08373572, 0.08880477, 0.09354832, 0.094909385, 0.09555284, 0.09486879, 0.092805736, 0.089849964, 0.08677087, 0.08289782, 0.07916295, 0.07775132, 0.07459662, 0.0729127, 0.0713761, 0.06817307, 0.068482816, 0.07145352, 0.07124193, 0.07010062, 0.07038093, 0.051475804, 0.07110534, 0.07126396, 0.071095765, 0.07053106, 0.07128486]\n",
      "----- Combination 4 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 96) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.2722957, 0.2538008, 0.23988861, 0.22757892, 0.21736872, 0.2091757, 0.20268232, 0.1988646, 0.19543219, 0.19213697, 0.189976, 0.18747027, 0.18520708, 0.18358868, 0.18197231, 0.1805023, 0.17860556, 0.17641497, 0.17390803, 0.17115086, 0.16774619, 0.16443267, 0.16105956, 0.15728329, 0.15348136, 0.14980236, 0.14639002, 0.1431633, 0.14006992, 0.13724846]\n",
      "----- Combination 5 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 96) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.25758165, 0.21748254, 0.19546777, 0.18062869, 0.16813555, 0.15486182, 0.14178175, 0.13252588, 0.12451152, 0.11655606, 0.11072778, 0.10651722, 0.10206774, 0.098611854, 0.095765755, 0.092511445, 0.09017122, 0.087065, 0.08480756, 0.08238889, 0.08038367, 0.078584686, 0.07771356, 0.076820455, 0.0757204, 0.07476615, 0.07460998, 0.074123695, 0.07338522, 0.07288471]\n",
      "----- Combination 6 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 96) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.10380246, 0.10300055, 0.11214514, 0.11455144, 0.11027287, 0.106918365, 0.10543983, 0.10112842, 0.09655945, 0.09275526, 0.08919178, 0.08579089, 0.081997946, 0.07965274, 0.07799889, 0.07681956, 0.075631216, 0.07541008, 0.07372224, 0.07346142, 0.07303729, 0.072517455, 0.07323432, 0.07311216, 0.07202847, 0.07229835, 0.0728019, 0.072387144, 0.07281556, 0.07157965]\n",
      "----- Combination 7 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 96) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.23716973, 0.08338296, 0.06670914, 0.08989814, 0.10114484, 0.10549602, 0.107046284, 0.106119275, 0.10295874, 0.09820475, 0.09535164, 0.09376102, 0.087947264, 0.07435258, 0.08349621, 0.07880999, 0.0759627, 0.07519093, 0.072563514, 0.07269339, 0.07323568, 0.07371393, 0.07183951, 0.071657136, 0.059394076, 0.06931381, 0.07052289, 0.07311118, 0.07168563, 0.07030466]\n",
      "----- Combination 8 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.17612997, 0.16996294, 0.17375273, 0.17753029, 0.18335316, 0.1854311, 0.18582302, 0.18369381, 0.1805801, 0.17696631, 0.17346165, 0.16837761, 0.16333103, 0.15837643, 0.1538701, 0.14957172, 0.14528258, 0.14134344, 0.13748966, 0.13372558, 0.12987414, 0.12629205, 0.12271155, 0.118814826, 0.11584167, 0.11258505, 0.109050184, 0.10595654, 0.10254766, 0.09904487]\n",
      "----- Combination 9 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 128) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.14997533, 0.15321158, 0.16926648, 0.17677647, 0.17666796, 0.17383474, 0.16975816, 0.16508679, 0.15873393, 0.15306082, 0.14739312, 0.142344, 0.13787127, 0.13337007, 0.12925568, 0.12514654, 0.12104091, 0.11722563, 0.11289358, 0.10919559, 0.104808405, 0.10009621, 0.09741225, 0.09303916, 0.08958026, 0.08649874, 0.084129654, 0.082145646, 0.08083519, 0.07948816]\n",
      "----- Combination 10 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 128) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.1494444, 0.06013575, 0.051164065, 0.09554711, 0.122229144, 0.13408105, 0.13846464, 0.13753214, 0.13448074, 0.1295464, 0.12363613, 0.12192925, 0.11835626, 0.113810584, 0.11083019, 0.106632434, 0.103062406, 0.09693177, 0.096567124, 0.09060542, 0.09068648, 0.0887104, 0.08309692, 0.08360565, 0.080668755, 0.08305668, 0.08171811, 0.08112663, 0.08075273, 0.08085963]\n",
      "----- Combination 11 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 128) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.7653498, 0.1370146, 0.2530452, 0.25339773, 0.21288627, 0.17868969, 0.1544852, 0.13991685, 0.12810242, 0.14200518, 0.12971617, 0.12827446, 0.121813685, 0.12724529, 0.11711112, 0.111015454, 0.10867995, 0.10412435, 0.0984382, 0.09710492, 0.09164511, 0.08584322, 0.08297808, 0.079866305, 0.07560085, 0.07372214, 0.07359913, 0.07610682, 0.072527446, 0.06309283]\n",
      "----- Combination 12 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 64) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.08145189, 0.09128686, 0.10646554, 0.119413495, 0.13066259, 0.13981444, 0.14606902, 0.14852843, 0.14883652, 0.14727174, 0.14419758, 0.14053738, 0.13595521, 0.13094196, 0.12570104, 0.12064202, 0.11617335, 0.11185468, 0.10801632, 0.10414026, 0.10076201, 0.09751849, 0.09509458, 0.09273149, 0.09155009, 0.0907609, 0.09001464, 0.08961475, 0.088989966, 0.08865623]\n",
      "----- Combination 13 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 64) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.11578648, 0.040459048, 0.075243056, 0.07916369, 0.080291346, 0.08067569, 0.08061716, 0.08102957, 0.079630256, 0.07710309, 0.07587473, 0.0756021, 0.07458696, 0.07528608, 0.0751744, 0.073743194, 0.07405537, 0.07367336, 0.074100405, 0.074418925, 0.07394601, 0.07334919, 0.073519796, 0.0726327, 0.07348387, 0.07385597, 0.07322901, 0.07437615, 0.07321392, 0.07270994]\n",
      "----- Combination 14 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 64) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.22546768, 0.03895892, 0.059459154, 0.0685568, 0.0724521, 0.07230058, 0.07221681, 0.071475774, 0.07170301, 0.07131773, 0.0719496, 0.07096265, 0.07088819, 0.07111132, 0.070336275, 0.06921818, 0.06967898, 0.06969789, 0.07005432, 0.06903461, 0.06943966, 0.06754474, 0.07020232, 0.0684144, 0.06881149, 0.069652066, 0.06856321, 0.07013742, 0.06882136, 0.06846385]\n",
      "----- Combination 15 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 64) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.038388424, 0.040122088, 0.06200808, 0.06230476, 0.06437016, 0.06452569, 0.065552086, 0.0662163, 0.06718199, 0.06280719, 0.06499785, 0.06455217, 0.061895072, 0.06188341, 0.06349255, 0.06315161, 0.061834365, 0.0633633, 0.0605181, 0.061568882, 0.06301433, 0.057771243, 0.061705433, 0.060953125, 0.058505364, 0.061506797, 0.058678616, 0.059322342, 0.058680415, 0.057187144]\n",
      "----- Combination 16 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 96) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.08174744, 0.120384626, 0.13768072, 0.14863476, 0.15461144, 0.1574772, 0.1576724, 0.1553623, 0.15165529, 0.14661357, 0.14115527, 0.13569823, 0.13012624, 0.1242326, 0.118680425, 0.11260092, 0.107184, 0.10230224, 0.09820093, 0.094566226, 0.09196279, 0.09007613, 0.08865985, 0.08791376, 0.08765137, 0.08714005, 0.08627999, 0.08602078, 0.08551099, 0.084731996]\n",
      "----- Combination 17 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 96) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.046503656, 0.08229251, 0.09581846, 0.09187765, 0.088269986, 0.085653834, 0.08487105, 0.08228131, 0.08111326, 0.07967512, 0.07922555, 0.079625115, 0.07895388, 0.07830383, 0.07868731, 0.07794362, 0.07740685, 0.07786947, 0.07871697, 0.077909335, 0.07789344, 0.07853282, 0.07752207, 0.07781332, 0.07763131, 0.077587634, 0.07581158, 0.077601925, 0.07659641, 0.075385034]\n",
      "----- Combination 18 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 96) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.047732193, 0.05873639, 0.07312471, 0.07385663, 0.07571848, 0.07535231, 0.074904636, 0.072980836, 0.075382695, 0.07472679, 0.07463397, 0.07475355, 0.07270602, 0.071478724, 0.07406968, 0.07023696, 0.07087898, 0.07088704, 0.070505634, 0.07076941, 0.067937106, 0.070489995, 0.069156714, 0.06560928, 0.06753791, 0.06735794, 0.065782316, 0.068822935, 0.066058844, 0.06386873]\n",
      "----- Combination 19 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 96) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.07485386, 0.07634899, 0.04452547, 0.04157775, 0.04256642, 0.04847847, 0.057267968, 0.059588484, 0.062337764, 0.062355287, 0.06186051, 0.06188421, 0.058656845, 0.058121648, 0.058033347, 0.05912812, 0.05673667, 0.059498373, 0.055540178, 0.058750313, 0.058208197, 0.058219697, 0.058167446, 0.057186913, 0.057726987, 0.0581457, 0.05584205, 0.058611088, 0.057836592, 0.056471135]\n",
      "----- Combination 20 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.077458136, 0.094350085, 0.10728221, 0.11270203, 0.114087895, 0.11149688, 0.10737358, 0.10163302, 0.096371695, 0.09163852, 0.08818874, 0.08589068, 0.0845756, 0.0836775, 0.082788974, 0.08122605, 0.08014059, 0.079198174, 0.07828212, 0.07825312, 0.07819331, 0.0779847, 0.07816444, 0.0775302, 0.0772392, 0.07715452, 0.076981574, 0.07670968, 0.07661514, 0.075908944]\n",
      "----- Combination 21 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 128) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.07040404, 0.063598774, 0.06277942, 0.06464648, 0.067878194, 0.06845131, 0.06951401, 0.069219455, 0.069249675, 0.06808072, 0.06739949, 0.06857467, 0.06824111, 0.068235084, 0.06836593, 0.06635077, 0.065679386, 0.06639365, 0.06566913, 0.064855486, 0.06425227, 0.06399435, 0.06395759, 0.06373902, 0.0630651, 0.0634169, 0.06337707, 0.06348716, 0.063853905, 0.062302664]\n",
      "----- Combination 22 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 128) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.07573028, 0.042461127, 0.03976006, 0.038839705, 0.051310398, 0.060154315, 0.06575807, 0.06559727, 0.06735143, 0.06534706, 0.0642303, 0.06563594, 0.0640302, 0.06409181, 0.06483969, 0.06261027, 0.06268418, 0.062948644, 0.060224693, 0.06234277, 0.060559247, 0.060211167, 0.060812578, 0.060581267, 0.05941435, 0.06013705, 0.060983494, 0.060949586, 0.06191942, 0.05734541]\n",
      "----- Combination 23 -----\n",
      "('num_hidden_layers', 1) ('layer_size', 128) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.09169144, 0.09089704, 0.06264639, 0.06505256, 0.060988735, 0.051516384, 0.048203304, 0.04692933, 0.04720297, 0.049586073, 0.049474858, 0.05771043, 0.060261916, 0.06357893, 0.0662525, 0.06594594, 0.06546217, 0.0660213, 0.06500366, 0.065798074, 0.062121574, 0.0647571, 0.064627655, 0.0636508, 0.0616396, 0.06140605, 0.05914614, 0.060308874, 0.06056329, 0.057664923]\n",
      "----- Combination 24 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 64) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.055209845, 0.076638, 0.08845541, 0.092549555, 0.088980526, 0.084983885, 0.083799735, 0.083091535, 0.08195038, 0.080841176, 0.07952857, 0.077604815, 0.076596655, 0.07497592, 0.074552126, 0.07432402, 0.07287632, 0.07326609, 0.0724719, 0.07232305, 0.07222799, 0.07203156, 0.07164, 0.07210304, 0.07145496, 0.07144159, 0.071628675, 0.07169828, 0.072249115, 0.07177898]\n",
      "----- Combination 25 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 64) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.05907893, 0.07374479, 0.07209522, 0.071778856, 0.07068208, 0.070055954, 0.07193504, 0.06871356, 0.069525115, 0.07001041, 0.07090232, 0.069949895, 0.069816425, 0.06951988, 0.06869454, 0.06996821, 0.06816998, 0.070313826, 0.06873159, 0.06821374, 0.069468774, 0.0701551, 0.06811319, 0.069796205, 0.06935149, 0.06974752, 0.066455565, 0.07055063, 0.071881056, 0.071113445]\n",
      "----- Combination 26 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 64) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.06457198, 0.069847256, 0.06883419, 0.06674422, 0.067125745, 0.068575926, 0.07017253, 0.06794675, 0.06996325, 0.07000613, 0.07124968, 0.06827661, 0.071536675, 0.067689955, 0.06986581, 0.070310004, 0.068539426, 0.069660366, 0.0674698, 0.06731573, 0.06830289, 0.07156183, 0.068028, 0.06934278, 0.06746505, 0.06713437, 0.06733866, 0.07011259, 0.07128887, 0.07138229]\n",
      "----- Combination 27 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 64) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.050044976, 0.04067375, 0.049664132, 0.059932895, 0.060628243, 0.06216196, 0.06447797, 0.062452476, 0.06479356, 0.06687359, 0.06404403, 0.06253882, 0.06617077, 0.061091892, 0.06394249, 0.0646726, 0.061988275, 0.06321493, 0.060857113, 0.059180688, 0.062172744, 0.06177938, 0.06240414, 0.06186094, 0.061734214, 0.06045369, 0.06132438, 0.064281546, 0.06593172, 0.06442149]\n",
      "----- Combination 28 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 96) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.06121375, 0.068223715, 0.0726552, 0.07271021, 0.07269576, 0.07135067, 0.07058573, 0.06969111, 0.06902334, 0.068381526, 0.06926471, 0.068818785, 0.069400385, 0.06901167, 0.069159515, 0.0687415, 0.06903069, 0.07008017, 0.068762735, 0.068717465, 0.06807469, 0.06899225, 0.06916585, 0.068986185, 0.06853111, 0.068417124, 0.068560645, 0.06860407, 0.06841585, 0.068207294]\n",
      "----- Combination 29 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 96) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.062086746, 0.0691233, 0.06718905, 0.068470694, 0.068712026, 0.06777091, 0.06778857, 0.067488484, 0.06765587, 0.06593458, 0.06699969, 0.06729522, 0.06744559, 0.066894226, 0.06649795, 0.06921807, 0.06703725, 0.06916501, 0.06861277, 0.06711059, 0.06767425, 0.066174, 0.06903513, 0.06693097, 0.06836992, 0.066757604, 0.06818805, 0.066223554, 0.06640619, 0.067034975]\n",
      "----- Combination 30 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 96) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.034284733, 0.046651933, 0.05742294, 0.06296082, 0.06541167, 0.066138364, 0.06580308, 0.06647973, 0.06627731, 0.06484328, 0.06668436, 0.06735024, 0.067163676, 0.06613924, 0.06645444, 0.068350464, 0.06577649, 0.069188766, 0.067352474, 0.06571302, 0.06810331, 0.066345125, 0.068926245, 0.066475466, 0.0682677, 0.06658756, 0.06819101, 0.06608391, 0.06729004, 0.06808324]\n",
      "----- Combination 31 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 96) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.055675883, 0.045507684, 0.03369029, 0.040130764, 0.039365597, 0.040426113, 0.04372872, 0.054440066, 0.059874527, 0.06450546, 0.064760014, 0.06540191, 0.06564529, 0.065484524, 0.06693955, 0.06852512, 0.061997004, 0.06688209, 0.06669912, 0.06801972, 0.0685381, 0.064165756, 0.067781106, 0.06397451, 0.06650884, 0.063908905, 0.066474706, 0.06454639, 0.06473255, 0.0652698]\n",
      "----- Combination 32 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.06519886, 0.082467794, 0.107278466, 0.11456524, 0.1065644, 0.096871465, 0.08947616, 0.08620654, 0.084456295, 0.08384937, 0.08205195, 0.08039348, 0.07815335, 0.077643126, 0.07745284, 0.07656017, 0.07627821, 0.07655458, 0.07610212, 0.07614804, 0.07668626, 0.0763613, 0.07629952, 0.07572262, 0.07593581, 0.07591854, 0.0758427, 0.0753655, 0.07593731, 0.07537669]\n",
      "----- Combination 33 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 128) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.066561185, 0.08027106, 0.07618127, 0.07187165, 0.073467314, 0.07355398, 0.069204636, 0.07209886, 0.07004993, 0.0695654, 0.0687022, 0.06994281, 0.06824218, 0.06913278, 0.067286335, 0.06744906, 0.06830078, 0.06795606, 0.06983569, 0.06761478, 0.070583574, 0.068712994, 0.069040745, 0.06624015, 0.06870424, 0.067525774, 0.0695679, 0.069832504, 0.06860185, 0.069037884]\n",
      "----- Combination 34 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 128) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.03216381, 0.036432236, 0.046453483, 0.05448166, 0.060304463, 0.06256812, 0.06216294, 0.065068215, 0.063531384, 0.06335753, 0.06524937, 0.06734162, 0.06582158, 0.066647924, 0.066796765, 0.06437606, 0.066889815, 0.06596814, 0.067953944, 0.0664374, 0.067964695, 0.066452995, 0.06587057, 0.06395904, 0.06600623, 0.06578328, 0.06713503, 0.06742914, 0.066372946, 0.06721756]\n",
      "----- Combination 35 -----\n",
      "('num_hidden_layers', 2) ('layer_size', 128) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.03827393, 0.033464786, 0.050875723, 0.04057876, 0.04754439, 0.040923152, 0.03851846, 0.044478733, 0.043236427, 0.04214164, 0.042865332, 0.045071926, 0.04443127, 0.04982212, 0.052150138, 0.05746436, 0.061186176, 0.062365614, 0.06375479, 0.062680766, 0.06701123, 0.06534189, 0.06573598, 0.062465996, 0.06483744, 0.063002124, 0.06543806, 0.06592439, 0.06678894, 0.06518492]\n",
      "----- Combination 36 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 64) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.041546877, 0.055537023, 0.06238868, 0.06741835, 0.070567004, 0.071809575, 0.07149753, 0.07107371, 0.07048617, 0.07019086, 0.06981343, 0.06955201, 0.07050187, 0.07002397, 0.07023768, 0.070747614, 0.06991956, 0.07061236, 0.07069713, 0.07130706, 0.07026387, 0.07088891, 0.070351765, 0.070748575, 0.07103349, 0.06993881, 0.07023756, 0.07018597, 0.07013699, 0.07031475]\n",
      "----- Combination 37 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 64) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.05277878, 0.0623821, 0.06756459, 0.06609914, 0.066473536, 0.06961791, 0.06783106, 0.06830867, 0.071781695, 0.06858778, 0.07021029, 0.06857605, 0.071531214, 0.06837036, 0.06998366, 0.070707485, 0.069083914, 0.069491014, 0.07252009, 0.07091067, 0.06985927, 0.07064054, 0.06933425, 0.070017934, 0.07097002, 0.06752576, 0.0705108, 0.06998643, 0.07055153, 0.07179483]\n",
      "----- Combination 38 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 64) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.041085683, 0.057276204, 0.06790104, 0.06812803, 0.067918696, 0.06913273, 0.069998816, 0.068185866, 0.072793946, 0.06938188, 0.0708072, 0.0704998, 0.074343234, 0.069521725, 0.07103269, 0.06965293, 0.06923666, 0.07105667, 0.0735517, 0.0725335, 0.06967117, 0.07051411, 0.07001078, 0.07197505, 0.073474884, 0.06966701, 0.07075926, 0.06985766, 0.071391575, 0.070756294]\n",
      "----- Combination 39 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 64) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.03227654, 0.043522507, 0.04731397, 0.04978838, 0.05280958, 0.059460253, 0.06371614, 0.063173145, 0.07178236, 0.06916939, 0.07252883, 0.06848127, 0.07290895, 0.07125985, 0.07025182, 0.06607832, 0.06698104, 0.06927804, 0.07315668, 0.07178973, 0.06801501, 0.07157524, 0.07066433, 0.06892702, 0.07278535, 0.06763517, 0.06846599, 0.06962949, 0.06979683, 0.07031365]\n",
      "----- Combination 40 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 96) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.05475579, 0.061208576, 0.068437904, 0.071144074, 0.07129616, 0.07011974, 0.07041918, 0.069906905, 0.06986365, 0.06925309, 0.07017866, 0.07046494, 0.070328414, 0.070939586, 0.07002941, 0.06978169, 0.0704177, 0.069548205, 0.06994252, 0.07017158, 0.06990316, 0.070412226, 0.07054639, 0.06964792, 0.07101704, 0.06979054, 0.07057302, 0.07024615, 0.06966765, 0.070035994]\n",
      "----- Combination 41 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 96) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.053572636, 0.066482805, 0.068423025, 0.06773296, 0.06800931, 0.06662497, 0.06756528, 0.06811151, 0.068596765, 0.06733355, 0.06930518, 0.06994057, 0.069361076, 0.072003216, 0.069543675, 0.06744802, 0.06981418, 0.06839523, 0.07009695, 0.07029106, 0.06763807, 0.07061318, 0.070500106, 0.06902152, 0.07172994, 0.06913522, 0.07035838, 0.0685838, 0.069064274, 0.07118729]\n",
      "----- Combination 42 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 96) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.06385937, 0.0695896, 0.07255399, 0.07075684, 0.07203837, 0.07287048, 0.0663556, 0.06734427, 0.06770235, 0.069807544, 0.0698697, 0.07037632, 0.07196091, 0.07013783, 0.06673009, 0.066308245, 0.06961022, 0.06909185, 0.07066704, 0.0685083, 0.06931755, 0.06985973, 0.06894751, 0.06706549, 0.072802864, 0.06888295, 0.0667088, 0.06707867, 0.071607664, 0.07112526]\n",
      "----- Combination 43 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 96) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.03446861, 0.032634705, 0.04748944, 0.06449738, 0.07458748, 0.07203239, 0.06751318, 0.06783682, 0.06974457, 0.06801078, 0.06962312, 0.07372855, 0.069892205, 0.07223913, 0.06808023, 0.064752385, 0.069033906, 0.0715657, 0.07327926, 0.072028965, 0.07202953, 0.064090356, 0.069449544, 0.06973074, 0.07481724, 0.06945275, 0.07114865, 0.06898241, 0.07245241, 0.071186274]\n",
      "----- Combination 44 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.07766203, 0.07951279, 0.09301862, 0.09303967, 0.083708726, 0.08034339, 0.07831216, 0.077937216, 0.07677597, 0.07575485, 0.07505206, 0.07232706, 0.073638156, 0.07289893, 0.07165043, 0.070969775, 0.071067765, 0.07175973, 0.071998686, 0.0706638, 0.07080831, 0.071856216, 0.07246911, 0.07128467, 0.071991846, 0.07121074, 0.071180254, 0.069950745, 0.07061109, 0.07107514]\n",
      "----- Combination 45 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 128) ('lr', 0.005) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.045700245, 0.048233315, 0.054973498, 0.06414028, 0.06582457, 0.06624381, 0.06662675, 0.06683725, 0.06951977, 0.07158339, 0.06898225, 0.06753204, 0.070727125, 0.06930648, 0.06962949, 0.06615528, 0.06738147, 0.06869807, 0.069885865, 0.06770959, 0.0687359, 0.0707279, 0.07029308, 0.070931, 0.06992869, 0.068727225, 0.06733926, 0.06896996, 0.06937797, 0.06890493]\n",
      "----- Combination 46 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 128) ('lr', 0.01) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.03556946, 0.041359555, 0.045499906, 0.05343933, 0.05613155, 0.06498246, 0.06360079, 0.067544274, 0.06873566, 0.06983039, 0.067960404, 0.064714395, 0.0704269, 0.06734089, 0.06781179, 0.06572664, 0.06618648, 0.068622075, 0.06868403, 0.06482227, 0.06826264, 0.06893052, 0.069239475, 0.06983901, 0.07089156, 0.06815783, 0.06755667, 0.06757102, 0.06748059, 0.07048507]\n",
      "----- Combination 47 -----\n",
      "('num_hidden_layers', 3) ('layer_size', 128) ('lr', 0.02) ('weight_decay', 0.002) ('batch_size', 1024) ('epochs', 30)\n",
      "[0.05023192, 0.047642414, 0.04760907, 0.056893438, 0.03857333, 0.035330147, 0.036368042, 0.040947456, 0.041385774, 0.04094298, 0.0372617, 0.041319545, 0.048191346, 0.04678196, 0.054144446, 0.05214207, 0.05939135, 0.06421022, 0.06586404, 0.06045407, 0.063433945, 0.067664966, 0.06810567, 0.06830962, 0.07171799, 0.06887713, 0.063643925, 0.063839495, 0.06351579, 0.06726769]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_eval_stats = []\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f'----- Combination {i} -----')\n",
    "    print(*zip(param_grid.keys(), params))\n",
    "    num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs = params\n",
    "\n",
    "\n",
    "    # Create DataLoader for training and validation\n",
    "    train_dataset = CustomDataset(train_X, train_y)\n",
    "    val_dataset = CustomDataset(val_X, val_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = MLP(input_size=len(feature_cols), output_size=len(target_cols), layer_size=layer_size, num_hidden_layers=num_hidden_layers).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        eval_stats['loss']['train'].append(train_model(model, train_loader, criterion, optimizer, device))\n",
    "        eval_stats['loss']['val'].append(util.evaluate_loss(model, val_loader, criterion, device))\n",
    "\n",
    "        eval_stats['mae']['train'].append(util.evaluate_mae(model, train_loader, device))\n",
    "        eval_stats['mae']['val'].append(util.evaluate_mae(model, val_loader, device))\n",
    "\n",
    "        # print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {eval_stats['loss']['train'][-1]:.4f}, Val Loss: {eval_stats['loss']['val'][-1]:.4f}\")\n",
    "    print(eval_stats['mae']['val'])\n",
    "    param_eval_stats.append(eval_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>overall liking</h3>\n",
    "\n",
    "\n",
    "<h3>sweetness intensity</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Traditional </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define your MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.manual_seed(consts.TORCH_MANUAL_SEED)\n",
    "        self.fc_in = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_fcs = nn.ModuleList([nn.Linear(layer_size, layer_size) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define custom PyTorch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Define training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def do_prediction(model, inputs, device):\n",
    "    inputs = inputs.float().to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'num_hidden_layers': [0,1,2,4,8],\n",
    "              'layer_size': [64,128,256],\n",
    "              'lr': [0.001],\n",
    "              'weight_decay': [0.005],\n",
    "              'batch_size': [64],\n",
    "              'epochs': [10]}\n",
    "\n",
    "np.prod([len(p) for p in param_grid.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "param_combinations = list(itertools.product(*param_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Combination 0 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 64) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
      "[0.15532045, 0.104790434, 0.0716258, 0.064395, 0.061727684, 0.062288485, 0.0613626, 0.061570674, 0.06185454, 0.061552614]\n",
      "----- Combination 1 -----\n",
      "('num_hidden_layers', 0) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_model(model, train_loader, criterion, optimizer, device))\n\u001b[0;32m     32\u001b[0m eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(util\u001b[38;5;241m.\u001b[39mevaluate_loss(model, val_loader, criterion, device))\n\u001b[1;32m---> 34\u001b[0m eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     35\u001b[0m eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(util\u001b[38;5;241m.\u001b[39mevaluate_mae(model, val_loader, device))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {eval_stats['loss']['train'][-1]:.4f}, Val Loss: {eval_stats['loss']['val'][-1]:.4f}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\roryd\\Documents\\master_thesis\\util.py:221\u001b[0m, in \u001b[0;36mevaluate_mae\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m    219\u001b[0m all_targets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 221\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roryd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\roryd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\roryd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roryd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\roryd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\roryd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\roryd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_eval_stats = []\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f'----- Combination {i} -----')\n",
    "    print(*zip(param_grid.keys(), params))\n",
    "    num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs = params\n",
    "\n",
    "    manifold = PoincareBall(c=Curvature(curvature))\n",
    "\n",
    "    # Create DataLoader for training and validation\n",
    "    train_dataset = CustomDataset(train_X, train_y)\n",
    "    val_dataset = CustomDataset(val_X, val_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = MLP(input_size=len(feature_cols), output_size=len(target_cols), layer_size=layer_size, num_hidden_layers=num_hidden_layers).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        eval_stats['loss']['train'].append(train_model(model, train_loader, criterion, optimizer, device))\n",
    "        eval_stats['loss']['val'].append(util.evaluate_loss(model, val_loader, criterion, device))\n",
    "\n",
    "        eval_stats['mae']['train'].append(util.evaluate_mae(model, train_loader, device))\n",
    "        eval_stats['mae']['val'].append(util.evaluate_mae(model, val_loader, device))\n",
    "\n",
    "        # print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {eval_stats['loss']['train'][-1]:.4f}, Val Loss: {eval_stats['loss']['val'][-1]:.4f}\")\n",
    "    print(eval_stats['mae']['val'])\n",
    "    param_eval_stats.append(eval_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('num_hidden_layers', 4) ('layer_size', 128) ('lr', 0.001) ('weight_decay', 0.005) ('batch_size', 64) ('epochs', 10)\n",
    "\n",
    "[0.04040382, 0.039590742, 0.03790839, 0.0377963, 0.03788594, 0.036986552, 0.03724333, 0.038333874, 0.038134657, 0.03877967]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Analysis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_results = []\n",
    "with open('grid_search_results/starwberry_hyp_overall-liking.txt', 'r') as file:\n",
    "    inlist = 0\n",
    "    for line in file.readlines():\n",
    "        if line[0:4] == '----':\n",
    "            grid_search_results.append([])\n",
    "            continue\n",
    "\n",
    "        if line[0] == '(':\n",
    "            fixed_line = '[' + line.strip().replace(') (', '), (') + ']'\n",
    "            params = ast.literal_eval(fixed_line)\n",
    "            nested = [list(zip(key.split(','), np.array(val).flatten())) for (key, val) in params]\n",
    "            unnested = [item for sublist in nested for item in sublist]\n",
    "            grid_search_results[-1].append({key: val for (key, val) in unnested})\n",
    "            grid_search_results[-1].append([])\n",
    "            continue\n",
    "\n",
    "        result = line\n",
    "\n",
    "        if result[0] == '[':\n",
    "            grid_search_results[-1][-1].append(np.array(ast.literal_eval(result)))\n",
    "            continue\n",
    "\n",
    "        grid_search_results[-1][-1].append(float(line.split('\\t ')[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isinstance(grid_search_results[1][0]['layer_size'], float)\n",
    "type(grid_search_results[0][0]['num_hidden_layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_results = {}\n",
    "for d, vs in grid_search_results:\n",
    "    if not (type(d['num_hidden_layers']) == np.int32 or type(d['num_hidden_layers']) == np.int64 or type(d['num_hidden_layers']) == np.float64):\n",
    "        continue\n",
    "\n",
    "    if d['num_hidden_layers'] not in param_results:\n",
    "        param_results[d['num_hidden_layers']] = []\n",
    "\n",
    "    param_results[d['num_hidden_layers']].append(vs[0].argmin())\n",
    "\n",
    "for k in param_results:\n",
    "    param_results[k] = np.array(param_results[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16: array([ 1,  0,  9,  9,  0,  0,  9,  7,  2,  2,  1,  1,  6,  9,  6,  9,  9,\n",
       "         9,  9,  9, 22, 14, 22, 15, 10,  7, 24, 26, 22, 17, 17, 27, 29, 27,\n",
       "        26, 48, 40, 33, 28, 24, 21, 38, 46, 49, 17, 24, 29,  9, 16, 16,  6,\n",
       "        12, 11, 15, 14, 26, 10,  5,  8,  4,  3, 11,  4,  6,  7,  5,  6,  7,\n",
       "         5,  6,  7,  8, 42,  3,  3,  2,  4, 47,  6, 42, 47,  4,  5, 47,  3,\n",
       "         4, 47,  3,  3, 47, 47,  8, 47, 47, 40,  3, 47,  4, 47,  3, 40, 49,\n",
       "        49, 49, 44, 45, 38, 30, 33, 27, 23, 27, 21, 38, 24, 35, 26, 16, 16,\n",
       "         9, 11, 10,  8,  9, 10,  5,  5,  6, 30,  4,  4,  2, 37,  3,  2,  2,\n",
       "         4,  3,  4,  0, 18, 20, 10, 14, 18,  5, 20, 22,  9,  8,  0,  3,  3,\n",
       "         0, 29,  4, 15, 13,  8, 11, 15, 11,  8,  5,  7,  5,  4,  6,  5,  4,\n",
       "         5,  4,  4]),\n",
       " 20: array([1, 1, 4, 4, 2, 2, 9, 9, 9, 9, 1, 9, 3, 8, 8, 9, 6, 9, 4, 6]),\n",
       " 24: array([ 9,  9,  9,  9,  3,  3,  1,  1,  5,  5,  6,  2,  9,  1,  9,  6,  9,\n",
       "         9,  9,  3, 17, 28, 29, 29,  1, 29,  0,  0,  6, 28,  3,  2,  4, 39,\n",
       "         7, 42,  3,  4,  2, 39,  3,  3,  2,  4, 47, 42,  9, 47,  4,  5,  3,\n",
       "        42,  4, 47,  3,  5, 47, 47,  8, 47, 47,  5,  3, 42,  4, 47,  3,  5,\n",
       "         6, 47,  8,  4, 47, 42,  3, 47,  4, 47,  3,  5,  6,  7,  9,  4, 47,\n",
       "        40,  3, 47,  4, 47,  3,  5, 31,  6,  7, 31, 37,  4,  3, 13,  3,  2,\n",
       "         2, 44, 31,  6,  7, 31,  4,  6, 31,  3,  5,  2, 37,  3, 37,  8, 34,\n",
       "        30,  4,  5, 37,  3,  4,  2, 37,  3, 27, 41,  5, 19, 16, 15, 14, 15,\n",
       "        11, 11, 12,  9,  3,  2,  5,  4,  8,  4,  2,  6,  5,  7,  7,  8, 49,\n",
       "        38,  4,  3,  3,  6,  4,  3, 10,  7,  5, 11]),\n",
       " 28: array([1, 9, 9, 9, 7, 6, 1, 1, 3, 4, 1, 1, 9, 9, 8, 9, 6, 9, 2, 2])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.179190751445088\n",
      "14.884146341463415\n"
     ]
    }
   ],
   "source": [
    "# print(param_results[14].mean())\n",
    "print(param_results[16].mean())\n",
    "# print(param_results[20].mean())\n",
    "print(param_results[24].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'num_hidden_layers': [14,16,18,20],\n",
    "              'layer_size': [80],\n",
    "              'lr': [0.02],\n",
    "              'weight_decay': [0.0025],\n",
    "              'batch_size': [1024],\n",
    "              'epochs': [50],\n",
    "              'curvature': [-1]}\n",
    "\n",
    "\n",
    "best_params_sweetness_intensity = {'num_hidden_layers': [14],\n",
    "              'layer_size': [80],\n",
    "              'lr': [0.02],\n",
    "              'weight_decay': [0.002],\n",
    "              'batch_size': [1024],\n",
    "              'epochs': [14],\n",
    "              'curvature': [-1]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
