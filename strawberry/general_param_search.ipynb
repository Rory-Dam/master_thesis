{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from hypll import nn as hnn\n",
    "from hypll.tensors import TangentTensor\n",
    "from hypll.optim import RiemannianAdam\n",
    "from hypll.manifolds.poincare_ball import Curvature, PoincareBall\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall Liking</th>\n",
       "      <th>Texture liking</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Sourness</th>\n",
       "      <th>Salty</th>\n",
       "      <th>Umami</th>\n",
       "      <th>Tomato Flavor Intenstity</th>\n",
       "      <th>glucose</th>\n",
       "      <th>fructose</th>\n",
       "      <th>Soluble solids</th>\n",
       "      <th>...</th>\n",
       "      <th>citric:malic</th>\n",
       "      <th>3-methyl-1-pentanol</th>\n",
       "      <th>2-ethylfuran</th>\n",
       "      <th>isopentyl acetate</th>\n",
       "      <th>cis-3-hexenyl acetate</th>\n",
       "      <th>benzothiazole</th>\n",
       "      <th>benzyl alcohol</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>3-methyl-2-butenal</th>\n",
       "      <th>p-anisaldehyde</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.338914</td>\n",
       "      <td>0.340171</td>\n",
       "      <td>0.256928</td>\n",
       "      <td>0.181355</td>\n",
       "      <td>0.131396</td>\n",
       "      <td>0.130262</td>\n",
       "      <td>0.335056</td>\n",
       "      <td>2.289596</td>\n",
       "      <td>1.929062</td>\n",
       "      <td>2.592386</td>\n",
       "      <td>...</td>\n",
       "      <td>2.520910</td>\n",
       "      <td>1.093684</td>\n",
       "      <td>0.136266</td>\n",
       "      <td>-0.571344</td>\n",
       "      <td>1.210977</td>\n",
       "      <td>1.037262</td>\n",
       "      <td>1.072747</td>\n",
       "      <td>0.980180</td>\n",
       "      <td>-0.146406</td>\n",
       "      <td>-0.119881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.336514</td>\n",
       "      <td>0.340055</td>\n",
       "      <td>0.256104</td>\n",
       "      <td>0.179985</td>\n",
       "      <td>0.131889</td>\n",
       "      <td>0.126163</td>\n",
       "      <td>0.333145</td>\n",
       "      <td>2.290067</td>\n",
       "      <td>1.939249</td>\n",
       "      <td>2.581130</td>\n",
       "      <td>...</td>\n",
       "      <td>2.528238</td>\n",
       "      <td>1.017167</td>\n",
       "      <td>-0.366412</td>\n",
       "      <td>-0.571344</td>\n",
       "      <td>1.290722</td>\n",
       "      <td>0.932396</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.923562</td>\n",
       "      <td>-0.089412</td>\n",
       "      <td>0.212553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.336584</td>\n",
       "      <td>0.339619</td>\n",
       "      <td>0.254152</td>\n",
       "      <td>0.180641</td>\n",
       "      <td>0.127664</td>\n",
       "      <td>0.129144</td>\n",
       "      <td>0.333956</td>\n",
       "      <td>2.294738</td>\n",
       "      <td>1.918264</td>\n",
       "      <td>2.566390</td>\n",
       "      <td>...</td>\n",
       "      <td>2.527202</td>\n",
       "      <td>1.181431</td>\n",
       "      <td>-0.219492</td>\n",
       "      <td>-0.571344</td>\n",
       "      <td>1.270815</td>\n",
       "      <td>-0.680615</td>\n",
       "      <td>-1.743087</td>\n",
       "      <td>0.945401</td>\n",
       "      <td>-0.039227</td>\n",
       "      <td>0.479200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.333920</td>\n",
       "      <td>0.339588</td>\n",
       "      <td>0.256608</td>\n",
       "      <td>0.182085</td>\n",
       "      <td>0.127090</td>\n",
       "      <td>0.128752</td>\n",
       "      <td>0.333464</td>\n",
       "      <td>2.286382</td>\n",
       "      <td>1.930326</td>\n",
       "      <td>2.546053</td>\n",
       "      <td>...</td>\n",
       "      <td>2.515032</td>\n",
       "      <td>1.176457</td>\n",
       "      <td>-0.201208</td>\n",
       "      <td>-0.571344</td>\n",
       "      <td>1.235020</td>\n",
       "      <td>0.632055</td>\n",
       "      <td>-0.813754</td>\n",
       "      <td>0.949416</td>\n",
       "      <td>-0.203380</td>\n",
       "      <td>-0.200393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.337627</td>\n",
       "      <td>0.339244</td>\n",
       "      <td>0.254991</td>\n",
       "      <td>0.182370</td>\n",
       "      <td>0.129525</td>\n",
       "      <td>0.129901</td>\n",
       "      <td>0.335189</td>\n",
       "      <td>2.296162</td>\n",
       "      <td>1.925542</td>\n",
       "      <td>2.543229</td>\n",
       "      <td>...</td>\n",
       "      <td>2.513928</td>\n",
       "      <td>1.056964</td>\n",
       "      <td>-0.232769</td>\n",
       "      <td>-0.571344</td>\n",
       "      <td>1.131131</td>\n",
       "      <td>0.481864</td>\n",
       "      <td>0.064032</td>\n",
       "      <td>0.875690</td>\n",
       "      <td>0.082542</td>\n",
       "      <td>-0.653343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78995</th>\n",
       "      <td>0.037238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135424</td>\n",
       "      <td>0.089726</td>\n",
       "      <td>0.084163</td>\n",
       "      <td>0.091139</td>\n",
       "      <td>0.191412</td>\n",
       "      <td>-1.257119</td>\n",
       "      <td>-1.677233</td>\n",
       "      <td>-0.141795</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.036429</td>\n",
       "      <td>0.475485</td>\n",
       "      <td>0.489022</td>\n",
       "      <td>0.524832</td>\n",
       "      <td>-2.284104</td>\n",
       "      <td>0.213054</td>\n",
       "      <td>0.853422</td>\n",
       "      <td>-0.918645</td>\n",
       "      <td>-0.385336</td>\n",
       "      <td>-0.182016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78996</th>\n",
       "      <td>0.031069</td>\n",
       "      <td>0.015169</td>\n",
       "      <td>0.135655</td>\n",
       "      <td>0.088629</td>\n",
       "      <td>0.081030</td>\n",
       "      <td>0.096139</td>\n",
       "      <td>0.189275</td>\n",
       "      <td>-1.275015</td>\n",
       "      <td>-1.672763</td>\n",
       "      <td>-0.162537</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.037527</td>\n",
       "      <td>0.299981</td>\n",
       "      <td>0.464641</td>\n",
       "      <td>1.805531</td>\n",
       "      <td>-0.768846</td>\n",
       "      <td>0.050725</td>\n",
       "      <td>-0.115696</td>\n",
       "      <td>-0.946242</td>\n",
       "      <td>-0.126320</td>\n",
       "      <td>0.545937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78997</th>\n",
       "      <td>0.046658</td>\n",
       "      <td>0.043162</td>\n",
       "      <td>0.133811</td>\n",
       "      <td>0.090758</td>\n",
       "      <td>0.082977</td>\n",
       "      <td>0.098215</td>\n",
       "      <td>0.193270</td>\n",
       "      <td>-1.282447</td>\n",
       "      <td>-1.667745</td>\n",
       "      <td>-0.202092</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.055045</td>\n",
       "      <td>-0.715392</td>\n",
       "      <td>1.269892</td>\n",
       "      <td>-2.362999</td>\n",
       "      <td>-0.416522</td>\n",
       "      <td>0.351719</td>\n",
       "      <td>-0.257527</td>\n",
       "      <td>-0.990672</td>\n",
       "      <td>0.347381</td>\n",
       "      <td>0.233870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78998</th>\n",
       "      <td>0.043135</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.133449</td>\n",
       "      <td>0.089429</td>\n",
       "      <td>0.083958</td>\n",
       "      <td>0.099179</td>\n",
       "      <td>0.192197</td>\n",
       "      <td>-1.283625</td>\n",
       "      <td>-1.683112</td>\n",
       "      <td>-0.124910</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.047269</td>\n",
       "      <td>1.128747</td>\n",
       "      <td>1.664543</td>\n",
       "      <td>-1.977505</td>\n",
       "      <td>-1.196415</td>\n",
       "      <td>0.026865</td>\n",
       "      <td>0.249716</td>\n",
       "      <td>-1.005686</td>\n",
       "      <td>-0.240351</td>\n",
       "      <td>0.298191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78999</th>\n",
       "      <td>0.031114</td>\n",
       "      <td>0.004309</td>\n",
       "      <td>0.135017</td>\n",
       "      <td>0.088864</td>\n",
       "      <td>0.081878</td>\n",
       "      <td>0.095770</td>\n",
       "      <td>0.190444</td>\n",
       "      <td>-1.283820</td>\n",
       "      <td>-1.686030</td>\n",
       "      <td>-0.136360</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.030924</td>\n",
       "      <td>-0.094489</td>\n",
       "      <td>-0.315281</td>\n",
       "      <td>-2.898434</td>\n",
       "      <td>-1.110234</td>\n",
       "      <td>-0.085172</td>\n",
       "      <td>0.704038</td>\n",
       "      <td>-0.981728</td>\n",
       "      <td>-1.037517</td>\n",
       "      <td>0.213064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79000 rows Ã— 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Overall Liking  Texture liking  Sweetness  Sourness     Salty  \\\n",
       "0            0.338914        0.340171   0.256928  0.181355  0.131396   \n",
       "1            0.336514        0.340055   0.256104  0.179985  0.131889   \n",
       "2            0.336584        0.339619   0.254152  0.180641  0.127664   \n",
       "3            0.333920        0.339588   0.256608  0.182085  0.127090   \n",
       "4            0.337627        0.339244   0.254991  0.182370  0.129525   \n",
       "...               ...             ...        ...       ...       ...   \n",
       "78995        0.037238        0.000000   0.135424  0.089726  0.084163   \n",
       "78996        0.031069        0.015169   0.135655  0.088629  0.081030   \n",
       "78997        0.046658        0.043162   0.133811  0.090758  0.082977   \n",
       "78998        0.043135        0.005880   0.133449  0.089429  0.083958   \n",
       "78999        0.031114        0.004309   0.135017  0.088864  0.081878   \n",
       "\n",
       "          Umami  Tomato Flavor Intenstity   glucose  fructose  Soluble solids  \\\n",
       "0      0.130262                  0.335056  2.289596  1.929062        2.592386   \n",
       "1      0.126163                  0.333145  2.290067  1.939249        2.581130   \n",
       "2      0.129144                  0.333956  2.294738  1.918264        2.566390   \n",
       "3      0.128752                  0.333464  2.286382  1.930326        2.546053   \n",
       "4      0.129901                  0.335189  2.296162  1.925542        2.543229   \n",
       "...         ...                       ...       ...       ...             ...   \n",
       "78995  0.091139                  0.191412 -1.257119 -1.677233       -0.141795   \n",
       "78996  0.096139                  0.189275 -1.275015 -1.672763       -0.162537   \n",
       "78997  0.098215                  0.193270 -1.282447 -1.667745       -0.202092   \n",
       "78998  0.099179                  0.192197 -1.283625 -1.683112       -0.124910   \n",
       "78999  0.095770                  0.190444 -1.283820 -1.686030       -0.136360   \n",
       "\n",
       "       ...  citric:malic  3-methyl-1-pentanol  2-ethylfuran  \\\n",
       "0      ...      2.520910             1.093684      0.136266   \n",
       "1      ...      2.528238             1.017167     -0.366412   \n",
       "2      ...      2.527202             1.181431     -0.219492   \n",
       "3      ...      2.515032             1.176457     -0.201208   \n",
       "4      ...      2.513928             1.056964     -0.232769   \n",
       "...    ...           ...                  ...           ...   \n",
       "78995  ...     -1.036429             0.475485      0.489022   \n",
       "78996  ...     -1.037527             0.299981      0.464641   \n",
       "78997  ...     -1.055045            -0.715392      1.269892   \n",
       "78998  ...     -1.047269             1.128747      1.664543   \n",
       "78999  ...     -1.030924            -0.094489     -0.315281   \n",
       "\n",
       "       isopentyl acetate  cis-3-hexenyl acetate  benzothiazole  \\\n",
       "0              -0.571344               1.210977       1.037262   \n",
       "1              -0.571344               1.290722       0.932396   \n",
       "2              -0.571344               1.270815      -0.680615   \n",
       "3              -0.571344               1.235020       0.632055   \n",
       "4              -0.571344               1.131131       0.481864   \n",
       "...                  ...                    ...            ...   \n",
       "78995           0.524832              -2.284104       0.213054   \n",
       "78996           1.805531              -0.768846       0.050725   \n",
       "78997          -2.362999              -0.416522       0.351719   \n",
       "78998          -1.977505              -1.196415       0.026865   \n",
       "78999          -2.898434              -1.110234      -0.085172   \n",
       "\n",
       "       benzyl alcohol  citric acid  3-methyl-2-butenal  p-anisaldehyde  \n",
       "0            1.072747     0.980180           -0.146406       -0.119881  \n",
       "1            0.002128     0.923562           -0.089412        0.212553  \n",
       "2           -1.743087     0.945401           -0.039227        0.479200  \n",
       "3           -0.813754     0.949416           -0.203380       -0.200393  \n",
       "4            0.064032     0.875690            0.082542       -0.653343  \n",
       "...               ...          ...                 ...             ...  \n",
       "78995        0.853422    -0.918645           -0.385336       -0.182016  \n",
       "78996       -0.115696    -0.946242           -0.126320        0.545937  \n",
       "78997       -0.257527    -0.990672            0.347381        0.233870  \n",
       "78998        0.249716    -1.005686           -0.240351        0.298191  \n",
       "78999        0.704038    -0.981728           -1.037517        0.213064  \n",
       "\n",
       "[79000 rows x 77 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_FILE = '../data/tomato_samples_big.csv'\n",
    "VAL_FILE = '../data/tomato_val_dataset.csv'\n",
    "\n",
    "data = pd.read_csv(TRAIN_FILE, index_col=0)\n",
    "val_data = pd.read_csv(VAL_FILE, index_col=0)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['glucose', 'fructose', 'Soluble solids', '1-penten-3-one',\n",
      "       'isovaleronitrile', 'trans-2-pentenal', 'trans-2-heptenal',\n",
      "       'trans-3-hexen-1-ol', '6-methyl-5-hepten-2-ol', 'nonyl aldehyde',\n",
      "       'cis-4-decenal', 'sugar:acid', 'isovaleraldehyde', '3-methyl-1-butanol',\n",
      "       'methional', '2,5-dimethyl-4-hydroxy-3(2H)-furanone', '3-pentanone',\n",
      "       '1-pentanol', 'benzyl cyanide', 'isovaleric acid', '2-isobutylthiazole',\n",
      "       '1-nitro-3-methylbutane', 'benzaldehyde', '6-methyl-5-hepten-2-one',\n",
      "       'b-ionone', 'b-cyclocitral', 'geranial', 'phenylacetaldehyde',\n",
      "       'eugenol', 'geranylacetone', '2-phenyl ethanol', 'neral',\n",
      "       'salicylaldehyde', 'isobutyl acetate', 'butyl acetate',\n",
      "       'cis-3-hexen-1-ol', '1-nitro-2-phenylethane', '1-penten-3-ol',\n",
      "       '2-methylbutyl acetate', 'heptaldehyde', 'trans,trans-2,4-decadienal',\n",
      "       'malic acid', '2-methylbutanal', '4-carene', 'hexyl alcohol',\n",
      "       'guaiacol', 'propyl acetate', 'hexanal', 'cis-2-penten-1-ol',\n",
      "       'glutamatic acid', '2-butylacetate', '1-octen-3-one', 'cis-3-hexenal',\n",
      "       'methylsalicylate', 'trans-2-hexenal', 'b-damascenone',\n",
      "       '2-methyl-1-butanol', '2-methyl-2-butenal', 'prenyl acetate',\n",
      "       'hexyl acetate', 'citric:malic', '3-methyl-1-pentanol', '2-ethylfuran',\n",
      "       'isopentyl acetate', 'cis-3-hexenyl acetate', 'benzothiazole',\n",
      "       'benzyl alcohol', 'citric acid', '3-methyl-2-butenal',\n",
      "       'p-anisaldehyde'],\n",
      "      dtype='object')\n",
      "Index(['Overall Liking'], dtype='object')\n",
      "[(0, 20), (20, 40), (40, 60), (60, 79)]\n"
     ]
    }
   ],
   "source": [
    "NUM_LABEL_COLS = 7\n",
    "FEATURE_COLS = data.columns[7:]\n",
    "LABEL_COLS = data.columns[[0]]\n",
    "print(FEATURE_COLS)\n",
    "print(LABEL_COLS)\n",
    "\n",
    "FOLDS = 4\n",
    "NUM_SAMPLE_TYPES = len(val_data)\n",
    "NUM_SAMPLES_PER_TYPE = len(data) // NUM_SAMPLE_TYPES\n",
    "\n",
    "fold_nums = list(range(FOLDS))\n",
    "[num*NUM_SAMPLE_TYPES for num in fold_nums]\n",
    "[(num+1)*NUM_SAMPLE_TYPES for num in fold_nums]\n",
    "\n",
    "FOLD_INDICIES = util.get_fold_indices(NUM_SAMPLE_TYPES, FOLDS)\n",
    "\n",
    "# FOLD_INDICIES = list(zip([num*NUM_SAMPLE_TYPES//FOLDS for num in fold_nums],\n",
    "#                          [(num+1)*NUM_SAMPLE_TYPES//FOLDS for num in fold_nums]))\n",
    "\n",
    "print(FOLD_INDICIES)\n",
    "\n",
    "ALL_TRAIN_FEATURES = data[FEATURE_COLS].values\n",
    "ALL_TRAIN_LABELS = data[LABEL_COLS].values\n",
    "ALL_VAL_FEATURES = val_data[FEATURE_COLS].values\n",
    "ALL_VAL_LABELS = val_data[LABEL_COLS].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom PyTorch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Hyperbolic </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP model\n",
    "class HYP_MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers, manifold):\n",
    "        super(HYP_MLP, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.fc_in = hnn.HLinear(input_size, layer_size, manifold=manifold)\n",
    "        self.relu = hnn.HReLU(manifold=manifold)\n",
    "        self.hidden_fcs = nn.ModuleList([hnn.HLinear(layer_size, layer_size, manifold=manifold) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = hnn.HLinear(layer_size, output_size, manifold=manifold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define training function\n",
    "def hyp_train_model(model, train_loader, criterion, optimizer, manifold, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tangents = TangentTensor(data=inputs, man_dim=-1, manifold=manifold)\n",
    "        manifold_inputs = manifold.expmap(tangents)\n",
    "\n",
    "        outputs = model(manifold_inputs)\n",
    "\n",
    "        loss = criterion(outputs.tensor, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> EUCLIDEAN </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP model\n",
    "class EUC_MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size, num_hidden_layers):\n",
    "        super(EUC_MLP, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.fc_in = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_fcs = nn.ModuleList([nn.Linear(layer_size, layer_size) for _ in range(num_hidden_layers)])\n",
    "        self.fc_out = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.relu(x)\n",
    "        for fc in self.hidden_fcs:\n",
    "            x = fc(x)\n",
    "            x = self.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define training function\n",
    "def euc_train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# param_grid = {\n",
    "#     'model_type': ['hyp'],\n",
    "#     'num_hidden_layers': [0,2,8,12,14,16,18,20],\n",
    "#     'layer_size': [2,8,16,32,48,64,72,80,96,128,256,512],\n",
    "#     'lr': [0.018,0.02,0.022],\n",
    "#     'weight_decay': [0.001],\n",
    "#     'batch_size': [1024],\n",
    "#     'epochs': [50],\n",
    "#     'curvature': [-1]\n",
    "# }\n",
    "\n",
    "# param_grid = {\n",
    "#     'model_type': ['euc'],\n",
    "#     'num_hidden_layers': [0,1,2,3,4,5,8,12],\n",
    "#     'layer_size': [2,8,16,64,128,192,256,320,448,480,512,544,576],\n",
    "#     'lr': [0.003,0.004,0.005],\n",
    "#     'weight_decay': [0.001],\n",
    "#     'batch_size': [1024],\n",
    "#     'epochs': [50],\n",
    "#     'curvature': [-1]\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# param_grid = {\n",
    "#     'model_type': ['euc'],\n",
    "#     'num_hidden_layers': [0,1,2,4],\n",
    "#     'layer_size': [2,4,8,16,32,64,128],\n",
    "#     'lr': [0.003],\n",
    "#     'weight_decay': [0.0005],\n",
    "#     'batch_size': [1024],\n",
    "#     'epochs': [400],\n",
    "#     'curvature': [-1]\n",
    "# }\n",
    "\n",
    "# param_grid = {\n",
    "#     'model_type': ['hyp'],\n",
    "#     'num_hidden_layers': [0,1,2,4],\n",
    "#     'layer_size': [2,4,8,16,32,64,128],\n",
    "#     'lr': [0.005],\n",
    "#     'weight_decay': [0.001],\n",
    "#     'batch_size': [1024],\n",
    "#     'epochs': [400],\n",
    "#     'curvature': [-1]\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'model_type': ['hyp'],\n",
    "    'num_hidden_layers': [0,1,2,4],\n",
    "    'layer_size': [2,4,8,16,32,64,128],\n",
    "    'lr': [0.0075],\n",
    "    'weight_decay': [0.0005],\n",
    "    'batch_size': [1024],\n",
    "    'epochs': [50],\n",
    "    'curvature': [-1]\n",
    "}\n",
    "\n",
    "\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "len(param_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Combination 0 -----\n",
      "('model_type', 'hyp') ('num_hidden_layers', 0) ('layer_size', 2) ('lr', 0.0075) ('weight_decay', 0.0005) ('batch_size', 1024) ('epochs', 50) ('curvature', -1)\n",
      "Fold 0\n",
      "[0.26345295, 0.18447974, 0.14252834, 0.11374255, 0.107388124, 0.10438355, 0.10366936, 0.10454792, 0.10484234, 0.10505575, 0.10556717, 0.10576926, 0.10599293, 0.10496837, 0.10505166, 0.104684934, 0.104787484, 0.1042135, 0.10460315, 0.10398506, 0.10432235, 0.10411217, 0.10344906, 0.10406717, 0.10423674, 0.10400115, 0.10365184, 0.103905715, 0.10359254, 0.10437362, 0.10447736, 0.10419512, 0.10366311, 0.10330506, 0.103970684, 0.103147045, 0.10294956, 0.102151774, 0.10216719, 0.10189507, 0.101192355, 0.101909794, 0.10248524, 0.10358639, 0.10483327, 0.10535413, 0.10665093, 0.10691263, 0.10675744, 0.1084554]\n",
      "Fold 1\n",
      "[0.19381538, 0.12971227, 0.09394763, 0.07634361, 0.07090715, 0.06942582, 0.0688538, 0.06898098, 0.06947188, 0.07025341, 0.07214711, 0.07286824, 0.07555146, 0.0771644, 0.07992125, 0.08085718, 0.080772206, 0.07941316, 0.077498905, 0.076900735, 0.075126305, 0.07241331, 0.071120076, 0.06851633, 0.06606311, 0.06413402, 0.062057465, 0.060555667, 0.059105653, 0.057795525, 0.058683623, 0.05954942, 0.059850268, 0.05964751, 0.06022317, 0.060233254, 0.060366623, 0.06080813, 0.061165463, 0.061336208, 0.061212223, 0.06132247, 0.061501484, 0.062076084, 0.06267007, 0.06284539, 0.06327073, 0.06311099, 0.061931957, 0.062466167]\n",
      "Fold 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(hyp_train_model(model, train_loader, criterion, optimizer, manifold, device))\n\u001b[1;32m     55\u001b[0m     eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(util\u001b[38;5;241m.\u001b[39mh_evaluate_loss(model, val_loader, criterion, manifold, device))\n\u001b[0;32m---> 57\u001b[0m     eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh_evaluate_mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanifold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     58\u001b[0m     eval_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(util\u001b[38;5;241m.\u001b[39mh_evaluate_mae(model, val_loader, manifold, device))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/SynologyDrive/Drive/UVA/Year_4/thesis/master_thesis/strawberry/../util.py:90\u001b[0m, in \u001b[0;36mh_evaluate_mae\u001b[0;34m(model, dataloader, manifold, device)\u001b[0m\n\u001b[1;32m     88\u001b[0m all_targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     91\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     93\u001b[0m         tangents \u001b[38;5;241m=\u001b[39m TangentTensor(data\u001b[38;5;241m=\u001b[39minputs, man_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, manifold\u001b[38;5;241m=\u001b[39mmanifold)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_eval_stats = []\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f'----- Combination {i} -----')\n",
    "    print(*zip(param_grid.keys(), params))\n",
    "    model_type, num_hidden_layers, layer_size, lr, weight_decay, batch_size, epochs, curvature = params\n",
    "    for fold, (fold_start, fold_stop) in enumerate(FOLD_INDICIES):\n",
    "        print(f'Fold {fold}')\n",
    "\n",
    "        # fold_train_X = train_X[[*list(range(fold_start*NUM_SAMPLES_PER_TYPE)), *list(range(fold_stop*NUM_SAMPLES_PER_TYPE, len(train_X)))]]\n",
    "        # fold_train_y   =   train_y[[*list(range(fold_start*NUM_SAMPLES_PER_TYPE)), *list(range(fold_stop*NUM_SAMPLES_PER_TYPE, len(train_X)))]]\n",
    "        # fold_val_X   = val_X[list(range(fold_start, fold_stop))]\n",
    "        # fold_val_y     =   val_y[list(range(fold_start, fold_stop))]\n",
    "        train_features = ALL_TRAIN_FEATURES[[*list(range(fold_start*NUM_SAMPLES_PER_TYPE)), *list(range(fold_stop*NUM_SAMPLES_PER_TYPE, len(ALL_TRAIN_FEATURES)))]]\n",
    "        train_labels   =   ALL_TRAIN_LABELS[[*list(range(fold_start*NUM_SAMPLES_PER_TYPE)), *list(range(fold_stop*NUM_SAMPLES_PER_TYPE, len(ALL_TRAIN_FEATURES)))]]\n",
    "        val_features   = ALL_VAL_FEATURES[list(range(fold_start, fold_stop))]\n",
    "        val_labels     =   ALL_VAL_LABELS[list(range(fold_start, fold_stop))]\n",
    "\n",
    "        train_dataset = CustomDataset(train_features, train_labels)\n",
    "        val_dataset = CustomDataset(val_features, val_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            manifold = PoincareBall(c=Curvature(curvature))\n",
    "        elif model_type == 'euc':\n",
    "            manifold = None\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            model = HYP_MLP(input_size=len(FEATURE_COLS),\n",
    "                            output_size=len(LABEL_COLS),\n",
    "                            layer_size=layer_size,\n",
    "                            num_hidden_layers=num_hidden_layers,\n",
    "                            manifold=manifold).to(device)\n",
    "        elif model_type == 'euc':\n",
    "            model = EUC_MLP(input_size=len(FEATURE_COLS),\n",
    "                            output_size=len(LABEL_COLS),\n",
    "                            layer_size=layer_size,\n",
    "                            num_hidden_layers=num_hidden_layers).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        if model_type == 'hyp':\n",
    "            optimizer = RiemannianAdam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif model_type == 'euc':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        eval_stats = {'loss': {'train': [], 'val': []}, 'mae': {'train': [], 'val': []}}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if model_type == 'hyp':\n",
    "                eval_stats['loss']['train'].append(hyp_train_model(model, train_loader, criterion, optimizer, manifold, device))\n",
    "                eval_stats['loss']['val'].append(util.h_evaluate_loss(model, val_loader, criterion, manifold, device))\n",
    "\n",
    "                eval_stats['mae']['train'].append(util.h_evaluate_mae(model, train_loader, manifold, device))\n",
    "                eval_stats['mae']['val'].append(util.h_evaluate_mae(model, val_loader, manifold, device))\n",
    "            elif model_type == 'euc':\n",
    "                eval_stats['loss']['train'].append(euc_train_model(model, train_loader, criterion, optimizer, device))\n",
    "                eval_stats['loss']['val'].append(util.evaluate_loss(model, val_loader, criterion, device))\n",
    "\n",
    "                eval_stats['mae']['train'].append(util.evaluate_mae(model, train_loader, device))\n",
    "                eval_stats['mae']['val'].append(util.evaluate_mae(model, val_loader, device))\n",
    "\n",
    "        print(eval_stats['mae']['val'])\n",
    "        param_eval_stats.append(eval_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
